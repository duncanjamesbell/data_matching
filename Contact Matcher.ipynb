{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np #need?\n",
    "import math #need?\n",
    "from jellyfish import jaro_winkler\n",
    "from py_common_subseq import find_common_subsequences\n",
    "import numbers\n",
    "import time\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#Pre-processor to normalize things like state acronym, nicknames, abbreviations\n",
    "#email match gets counted as a considered name match right off the bat, need method for doing this\n",
    "#name rarity gets considered, if name is very unique then it should be counted as a match regardless of org tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import shutil\n",
    "\n",
    "with io.open('cupola contacts sample.txt', encoding='utf-8', errors='ignore') as source:\n",
    "    with io.open('cupola contacts sample_utf.txt', mode='w', encoding='utf-8') as target:\n",
    "        shutil.copyfileobj(source,target)\n",
    "        \n",
    "#with io.open('ASAE Tech.csv', encoding='utf-8', errors='ignore') as source:\n",
    "#    with io.open('ASAE Tech_utf.csv', mode='w', encoding='utf-8') as target:\n",
    "#        shutil.copyfileobj(source,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_df = pd.read_table('cupola contacts sample_utf.txt',keep_default_na=False)\n",
    "right_df = pd.read_csv('ASAE Tech_utf.csv',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_person_id</th>\n",
       "      <th>organization_id</th>\n",
       "      <th>status</th>\n",
       "      <th>person_id</th>\n",
       "      <th>org_name</th>\n",
       "      <th>acronym</th>\n",
       "      <th>first_name</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>position</th>\n",
       "      <th>email</th>\n",
       "      <th>address_id</th>\n",
       "      <th>address1</th>\n",
       "      <th>address2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>alt_name</th>\n",
       "      <th>phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245871</td>\n",
       "      <td>79319</td>\n",
       "      <td>2</td>\n",
       "      <td>253411</td>\n",
       "      <td>New York Graduate Admissions Professionals</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Tonya</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Guzman</td>\n",
       "      <td>President and Website</td>\n",
       "      <td>NULL</td>\n",
       "      <td>165507</td>\n",
       "      <td>P.O. Box 14605</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Lenexa</td>\n",
       "      <td>KS</td>\n",
       "      <td>66285</td>\n",
       "      <td>NULL</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>377275</td>\n",
       "      <td>54864</td>\n",
       "      <td>2</td>\n",
       "      <td>380239</td>\n",
       "      <td>Alabama State Bar</td>\n",
       "      <td>ASB</td>\n",
       "      <td>Sherry</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Langley</td>\n",
       "      <td>Receptionist</td>\n",
       "      <td>sherry.langley@alabar.org</td>\n",
       "      <td>192053</td>\n",
       "      <td>P.O. Box 671</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Montgomery</td>\n",
       "      <td>AL</td>\n",
       "      <td>36101</td>\n",
       "      <td>NULL</td>\n",
       "      <td>334-269-1515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287731</td>\n",
       "      <td>92462</td>\n",
       "      <td>2</td>\n",
       "      <td>294754</td>\n",
       "      <td>Safeguard Properties</td>\n",
       "      <td></td>\n",
       "      <td>Kellie</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Chambers</td>\n",
       "      <td>Assistant Vice President, Investor Relations</td>\n",
       "      <td>kellie.chambers@safeguardproperties.com</td>\n",
       "      <td>178242</td>\n",
       "      <td>7887 Safeguard Cir.</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Valley View</td>\n",
       "      <td>OH</td>\n",
       "      <td>44125</td>\n",
       "      <td>NULL</td>\n",
       "      <td>216-520-1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>279710</td>\n",
       "      <td>90629</td>\n",
       "      <td>2</td>\n",
       "      <td>286839</td>\n",
       "      <td>National Council on Geocosmic Research</td>\n",
       "      <td>NCGR</td>\n",
       "      <td>Ken</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Irving</td>\n",
       "      <td>Webmaster</td>\n",
       "      <td>ncgrweb@gmail.com</td>\n",
       "      <td>175831</td>\n",
       "      <td>1351 Maryland Ave. NE</td>\n",
       "      <td>Apt. B</td>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>20002-4439</td>\n",
       "      <td>NULL</td>\n",
       "      <td>212-838-6247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263389</td>\n",
       "      <td>86595</td>\n",
       "      <td>2</td>\n",
       "      <td>270769</td>\n",
       "      <td>CSSI Inc</td>\n",
       "      <td>CSSI</td>\n",
       "      <td>Cynthia</td>\n",
       "      <td>Anne</td>\n",
       "      <td>Castillo</td>\n",
       "      <td>Chief Executive Officer</td>\n",
       "      <td>NULL</td>\n",
       "      <td>170722</td>\n",
       "      <td>425 Third St. SW</td>\n",
       "      <td>Suite 700</td>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>20024</td>\n",
       "      <td>NULL</td>\n",
       "      <td>202-863-2175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   org_person_id  organization_id status  person_id  \\\n",
       "0         245871            79319      2     253411   \n",
       "1         377275            54864      2     380239   \n",
       "2         287731            92462      2     294754   \n",
       "3         279710            90629      2     286839   \n",
       "4         263389            86595      2     270769   \n",
       "\n",
       "                                     org_name acronym first_name middle_name  \\\n",
       "0  New York Graduate Admissions Professionals    NULL      Tonya        NULL   \n",
       "1                           Alabama State Bar     ASB     Sherry        NULL   \n",
       "2                        Safeguard Properties             Kellie        NULL   \n",
       "3      National Council on Geocosmic Research    NCGR        Ken        NULL   \n",
       "4                                    CSSI Inc    CSSI    Cynthia        Anne   \n",
       "\n",
       "  last_name                                      position  \\\n",
       "0    Guzman                         President and Website   \n",
       "1   Langley                                  Receptionist   \n",
       "2  Chambers  Assistant Vice President, Investor Relations   \n",
       "3    Irving                                     Webmaster   \n",
       "4  Castillo                       Chief Executive Officer   \n",
       "\n",
       "                                     email  address_id               address1  \\\n",
       "0                                     NULL      165507         P.O. Box 14605   \n",
       "1                sherry.langley@alabar.org      192053           P.O. Box 671   \n",
       "2  kellie.chambers@safeguardproperties.com      178242    7887 Safeguard Cir.   \n",
       "3                        ncgrweb@gmail.com      175831  1351 Maryland Ave. NE   \n",
       "4                                     NULL      170722       425 Third St. SW   \n",
       "\n",
       "    address2         city state postal_code alt_name         phone  \n",
       "0       NULL       Lenexa    KS       66285     NULL          #N/A  \n",
       "1       NULL   Montgomery    AL       36101     NULL  334-269-1515  \n",
       "2       NULL  Valley View    OH       44125     NULL  216-520-1334  \n",
       "3     Apt. B   Washington    DC  20002-4439     NULL  212-838-6247  \n",
       "4  Suite 700   Washington    DC       20024     NULL  202-863-2175  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4099, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                                                                                                         int64\n",
       "SALES_DATE                                                                                                                                object\n",
       "FIRST_NAME                                                                                                                                object\n",
       "LAST_NAME                                                                                                                                 object\n",
       "ATTENDEE_CREDENTIALS                                                                                                                      object\n",
       "ATTENDEE_TITLE                                                                                                                            object\n",
       "COMPANY_NAME                                                                                                                              object\n",
       "ADDRESS1                                                                                                                                  object\n",
       "ADDRESS2                                                                                                                                  object\n",
       "CITY                                                                                                                                      object\n",
       "STATE                                                                                                                                     object\n",
       "ZIP_CODE                                                                                                                                  object\n",
       "COUNTRY                                                                                                                                   object\n",
       "Email                                                                                                                                     object\n",
       "WHAT IS YOUR COMPANY TYPE?                                                                                                                object\n",
       "WHAT IS YOUR CURRENT POSITION LEVEL?                                                                                                      object\n",
       "WHAT IS YOUR PRIMARY JOB RESPONSIBILITY?                                                                                                  object\n",
       "HOW MANY YEARS HAVE YOU ATTENDED THE TECHNOLOGY CONFERENCE  & EXPO?                                                                       object\n",
       "HOW MANY YEARS OF EXPERIENCE DO YOU HAVE IN THE TECHNOLOGY FIELD?                                                                         object\n",
       "WHAT IS THE MAIN REASON YOU ARE ATTENDING THIS YEAR'S TECHNOLOGY CONFERENCE?                                                              object\n",
       "WHAT IS THE STAFF SIZE OF YOUR ORGANIZATION?                                                                                              object\n",
       "WHAT IS YOUR APPROXIMATE TIMELINE FOR PURCHASING YOUR NEXT TECHNOLOGY PRODUCT/SERVICE?                                                    object\n",
       "WHAT IS YOUR ORGANIZATION'S ANNUAL TECHNOLOGY BUDGET?                                                                                     object\n",
       "WHAT IS YOUR ROLE IN THE PURCHASING OF PRODUCTS AND SERVICES FOR YOUR EMPLOYER?                                                           object\n",
       "OVER THE NEXT YEAR WILL YOUR COMPANY BE LOOKING TO PURCHASE/UPGRADE TECHNOLOGIES IN ANY OF THE FOLLOWING AREAS? (CHECK ALL THAT APPLY)    object\n",
       "FOR THIS EVENT, HOW WOULD YOU LIKE TO HEAR FROM OUR SPONSORS AND/OR EXHIBITORS?                                                           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATAFRAMES INTO MEMORY...\n",
      "dataframes loaded --- 0.0490000247955 seconds ---\n",
      "\n",
      "CREATING DICTIONARY OF ALL UNIQUE TOKENS W INCLUDE FLAG...\n",
      "token dictionaries created --- 0.206999778748 seconds ---\n",
      "\n",
      "TOKENIZING LEFT NAMES...\n",
      "left names tokenized --- 19.5590000153 seconds ---\n",
      "\n",
      "TOKENIZING LEFT ORGS...\n",
      "left orgs tokenized --- 91.4609999657 seconds ---\n",
      "\n",
      "TOKENIZING RIGHT NAMES...\n",
      "right names tokenized --- 0.583999872208 seconds ---\n",
      "\n",
      "TOKENIZING RIGHT ORGS...\n",
      "right orgs tokenized --- 2.95700001717 seconds ---\n",
      "\n",
      "JOINING LEFT & RIGHT TOKEN KEYS...\n",
      "left & right token keys joined --- 0.0149998664856 seconds ---\n",
      "\n",
      "GROUPING BY UNIQUE LEFT & RIGHT IDS & GETTING COUNT OF MATCHED TOKENS...\n",
      "keys grouped & counted --- 0.058000087738 seconds ---\n",
      "\n",
      "JOINING NAME & ORG TOKEN MATCHES...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id_l'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5f60cddd8cf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"JOINING NAME & ORG TOKEN MATCHES...\"\u001b[0m \u001b[1;31m#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m \u001b[0mname_org_joined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname_matched_records\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morg_matched_records\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id_l'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'id_r'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#,lsuffix='_l',rsuffix='_r')\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;31m#here we are reducing down to those match candidates which meet the minimum threshold of matched tokens (name + org tokens)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   4667\u001b[0m         \u001b[1;31m# For SparseDataFrame's benefit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4668\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[1;32m-> 4669\u001b[1;33m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[0;32m   4670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4671\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   4682\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[0;32m   4683\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4684\u001b[1;33m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[0;32m   4685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4686\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\reshape\\merge.pyc\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[0;32m     51\u001b[0m                          \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                          copy=copy, indicator=indicator)\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\reshape\\merge.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[0;32m    556\u001b[0m         (self.left_join_keys,\n\u001b[0;32m    557\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\reshape\\merge.pyc\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    833\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                     \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1964\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2442\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2444\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id_l'"
     ]
    }
   ],
   "source": [
    "#I think I need to revisit the approach\n",
    "#maybe, do a token match on person name only, and seperately do a token match on organization related things\n",
    "#then join matched keys and only consider matches which exist in BOTH\n",
    "#use position as a means to score?\n",
    "#email treated same as phone?  or, maybe ignore altogether since email matches are generally reliable and straightforward?\n",
    "\n",
    "start_time = time.time()\n",
    "print \"LOADING DATAFRAMES INTO MEMORY...\"\n",
    "\n",
    "left_df = pd.read_table('cupola contacts sample_utf.txt',keep_default_na=False)\n",
    "right_df = pd.read_csv('ASAE Tech_utf.csv',keep_default_na=False)\n",
    "\n",
    "#rename dataframe columns for processing:\n",
    "left_df.rename(columns={'org_person_id':'id',\n",
    "                        'org_name':'l_org_name',\n",
    "                        'acronym':'l_acronym',\n",
    "                        'alt_name':'l_alt_name',\n",
    "                        'first_name':'l_first_name',\n",
    "                        'middle_name':'l_middle_name',\n",
    "                        'last_name':'l_last_name',\n",
    "                        'position':'l_position',\n",
    "                        'address1':'l_address1',\n",
    "                        'address2':'l_address2',\n",
    "                        'city':'l_city',\n",
    "                        'state':'l_state',\n",
    "                        'postal_code':'l_postal_code',\n",
    "                        'email':'l_email'}, inplace=True)\n",
    "right_df.rename(columns={'ID':'id',\n",
    "                         'COMPANY_NAME':'r_org_name',\n",
    "                         'FIRST_NAME':'r_first_name',\n",
    "                         'LAST_NAME':'r_last_name',\n",
    "                         'ATTENDEE_TITLE':'r_position',\n",
    "                         'ADDRESS1':'r_address1',\n",
    "                         'ADDRESS2':'r_address2',\n",
    "                         'CITY':'r_city',\n",
    "                         'STATE':'r_state',\n",
    "                         'ZIP_CODE':'r_postal_code',\n",
    "                         'Email':'r_email'}, inplace=True)\n",
    "\n",
    "#set parameters\n",
    "name_token_limiter = 1 #percent of non-single name tokens to tokenize, where common tokens are excluded before rare\n",
    "org_token_limiter = .995 # percent of non-single org tokens to tokenize, where common tokens are excluded before rare\n",
    "name_rarity_thresh = .85 \n",
    "name_token_match_min = 1 # minimum number of matched tokens to be considered a match\n",
    "org_token_match_min = 1 # minimum number of matched tokens to be considered a match\n",
    "overall_token_match_min = name_token_match_min + org_token_match_min + 0 #optionally can increase to further limit matches; if +0 this is ignored\n",
    "\n",
    "#scoring weights\n",
    "first_name_weight = 1\n",
    "last_name_weight = 1.25\n",
    "org_name_weight = .75\n",
    "zip_weight = 1\n",
    "\n",
    "print(\"dataframes loaded --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CREATING DICTIONARY OF ALL UNIQUE TOKENS W INCLUDE FLAG...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# lowercase the name and split on spaces, remove non-alphanumeric chars\n",
    "def tokenize_name(name):\n",
    "    if isinstance(name, basestring) is True:\n",
    "        clean_name = ''.join(c if c.isalnum() else ' ' for c in name)\n",
    "        return clean_name.lower().split()\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "# same tokenizers as for names, meh, good enough\n",
    "def tokenize_address(address):\n",
    "    if isinstance(address, basestring) is True:\n",
    "        clean_name = ''.join(c if c.isalnum() else ' ' for c in address)\n",
    "        return clean_name.lower().split()\n",
    "    else:\n",
    "        return address\n",
    "\n",
    "# same tokenizers as for names, meh, good enough\n",
    "def tokenize_web(web):\n",
    "    if isinstance(web, basestring) is True:\n",
    "        clean_name = ''.join(c if c.isalnum() else ' ' for c in web)\n",
    "        return clean_name.lower().split()\n",
    "    else:\n",
    "        return web\n",
    "    \n",
    "# for the left dataset\n",
    "\n",
    "#should we tokenize position?  email?  \n",
    "#I think email may be better used to exclude from tokenizing process, ie no need to attempt to match if it exists and non generic\n",
    "left_name_tokenizers = [\n",
    "    ('l_first_name', tokenize_name),\n",
    "    ('l_middle_name', tokenize_name),\n",
    "    ('l_last_name', tokenize_name)\n",
    "]\n",
    "\n",
    "left_org_tokenizers = [\n",
    "    ('l_org_name', tokenize_name),\n",
    "    ('l_acronym', tokenize_name),\n",
    "    ('l_alt_name', tokenize_name),\n",
    "    ('l_city', tokenize_address),\n",
    "    ('l_state', tokenize_address),\n",
    "    ('l_postal_code', tokenize_address),\n",
    "    ('l_position', tokenize_name)\n",
    "]\n",
    "\n",
    "# and right\n",
    "right_name_tokenizers = [\n",
    "    ('r_first_name', tokenize_name),\n",
    "    ('r_last_name', tokenize_name)\n",
    "]\n",
    "\n",
    "right_org_tokenizers = [\n",
    "    ('r_org_name', tokenize_name),\n",
    "    ('r_city', tokenize_address),\n",
    "    ('r_state', tokenize_address),\n",
    "    ('r_postal_code', tokenize_address),\n",
    "    ('r_position', tokenize_name)\n",
    "]\n",
    "\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< creating dictionary for name words\n",
    "\n",
    "name_words = [] #creating list of all words used in name columns for counting to determine rarity\n",
    "\n",
    "for word in left_df['l_first_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        name_words.append(tokenize_name(str(word)))\n",
    "        \n",
    "for word in left_df['l_middle_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        name_words.append(tokenize_name(str(word)))\n",
    "        \n",
    "for word in left_df['l_last_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        name_words.append(tokenize_name(str(word)))\n",
    "        \n",
    "for word in right_df['r_first_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        name_words.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in right_df['r_last_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        name_words.append(tokenize_name(str(word)))\n",
    "\n",
    "flat_name_words = [item for sublist in name_words for item in sublist] #flatten list so it can be counted\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "cnt = Counter()\n",
    "for token in flat_name_words:\n",
    "    cnt[token] += 1\n",
    "    \n",
    "name_cnt_dict = dict(cnt) #convert to dictionary\n",
    "\n",
    "name_tokens_df = pd.DataFrame(name_cnt_dict.items(), columns=['token', 'count'])\n",
    "name_tokens_df = name_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "name_token_flag = []\n",
    "for index, value in enumerate(name_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        name_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    elif index < int(name_tokens_df.shape[0] * name_token_limiter): #important line, we are cutting the top x% of frequently occuring tokens\n",
    "        name_token_flag.append(1)\n",
    "    else:\n",
    "        name_token_flag.append(0)  #for the most common tokens, we exclude\n",
    "\n",
    "name_tokens_df['flag'] = name_token_flag\n",
    "\n",
    "name_tokens_df.drop('count',axis=1,inplace=True)\n",
    "name_tokens_df['flag'] = name_tokens_df.flag.astype(int) #converting flags to int\n",
    "name_tokens_dct = name_tokens_df.to_dict('split') #converting tokens_df to dictionary\n",
    "name_tokens_dct=dict(name_tokens_dct['data']) #honestly can't remember why this works, something to do with conversion to dictionary\n",
    "\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< creating dictoinary for org words\n",
    "\n",
    "org_words = [] #creating a list of all words used in org-related columns, for counting to determine rarity\n",
    "\n",
    "for word in left_df['l_org_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_words.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in left_df['l_alt_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_words.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in left_df['l_city']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_words.append(tokenize_name(str(word)))\n",
    "        \n",
    "for word in left_df['l_position']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_words.append(tokenize_name(str(word)))\n",
    "\n",
    "#for word in left_df['l_email']:\n",
    "#    if isinstance(word, float) is False:\n",
    "#        org_words.append(tokenize_name(str(word)))\n",
    "    \n",
    "for word in right_df['r_org_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_words.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in right_df['r_city']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_words.append(tokenize_name(str(word)))\n",
    "        \n",
    "for word in right_df['r_position']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_words.append(tokenize_name(str(word)))\n",
    "        \n",
    "#for word in right_df['r_email']:\n",
    "#    if isinstance(word, float) is False:\n",
    "#        org_words.append(tokenize_name(str(word)))\n",
    "\n",
    "flat_org_words1 = [item for sublist in org_words for item in sublist] #flatten list so it can be counted\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "cnt = Counter()\n",
    "for token in flat_org_words1:\n",
    "    cnt[token] += 1\n",
    "    \n",
    "org_cnt_dict = dict(cnt) #convert to dictionary\n",
    "\n",
    "org_tokens_df = pd.DataFrame(org_cnt_dict.items(), columns=['token', 'count'])\n",
    "org_tokens_df = org_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "org_token_flag = []\n",
    "for index, value in enumerate(org_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        org_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    elif index < int(org_tokens_df.shape[0] * org_token_limiter): #important line, we are cutting the top x% of frequently occuring tokens\n",
    "        org_token_flag.append(1)\n",
    "    else:\n",
    "        org_token_flag.append(0)  #for the most common tokens, we exclude\n",
    "\n",
    "org_tokens_df['flag'] = org_token_flag\n",
    "\n",
    "unique_org_tokens = [] #we treat state and zips differently because we want to include ALl state and zip tokens as these are unique\n",
    "\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< add chosen unique columns here from each df\n",
    "for word in left_df['l_state']:\n",
    "    if isinstance(word, float) is False:\n",
    "        unique_org_tokens.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in left_df['l_postal_code']:\n",
    "    if isinstance(word, float) is False:\n",
    "        unique_org_tokens.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in right_df['r_state']:\n",
    "    if isinstance(word, float) is False:\n",
    "        unique_org_tokens.append(tokenize_name(str(word)))\n",
    "        \n",
    "for word in right_df['r_postal_code']:\n",
    "    if isinstance(word, float) is False:\n",
    "        unique_org_tokens.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in left_df['l_acronym']:\n",
    "    if isinstance(word, float) is False:\n",
    "        unique_org_tokens.append(tokenize_name(str(word)))\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        \n",
    "flat_org_words2 = [item for sublist in unique_org_tokens for item in sublist]\n",
    "new_unique_tokens = list(set(flat_org_words2) - set(flat_org_words1)) #getting a list of tokens which are NOT included in the first flat_list\n",
    "\n",
    "org_tokens_df.drop('count',axis=1,inplace=True)\n",
    "org_tokens_df['flag'] = org_tokens_df.flag.astype(int) #converting flags to int\n",
    "org_tokens_dct = org_tokens_df.to_dict('split') #converting tokens_df to dictionary\n",
    "org_tokens_dct=dict(org_tokens_dct['data']) #honestly can't remember why this works, something to do with conversion to dictionary\n",
    "\n",
    "for token in new_unique_tokens: #assigning flag of 1 for all newly added unique tokens\n",
    "    org_tokens_dct[token] = 1  #note that we are NOT limiting to tokens occuring more than once, as above.  something that could be enhanced\n",
    "    \n",
    "print(\"token dictionaries created --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "#this is the generator function we use to create token/ID pairs\n",
    "def prepare_name_join_keys(df, tokenizers):\n",
    "    for source_column, tokenizer in tokenizers:\n",
    "        if source_column in df.columns:\n",
    "            for index, record in enumerate(df[source_column]):\n",
    "                if isinstance(record, numbers.Integral) is False: #control for longs\n",
    "                    if isinstance(record, float) is False: #control for nans\n",
    "                        for token in tokenizer(record):\n",
    "                            if name_tokens_dct[token] == 1: #tokenize only for tokens present in dictionary with value 1\n",
    "                                yield (token, df.iloc[index]['id'])\n",
    "\n",
    "def prepare_org_join_keys(df, tokenizers):\n",
    "    for source_column, tokenizer in tokenizers:\n",
    "        if source_column in df.columns:\n",
    "            for index, record in enumerate(df[source_column]):\n",
    "                if isinstance(record, numbers.Integral) is False: #control for longs\n",
    "                    if isinstance(record, float) is False: #control for nans\n",
    "                        for token in tokenizer(record):\n",
    "                            if org_tokens_dct[token] == 1: #tokenize only for tokens present in dictionary with value 1\n",
    "                                yield (token, df.iloc[index]['id'])\n",
    "                            \n",
    "start_time = time.time()\n",
    "print \"TOKENIZING LEFT NAMES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#NOTE: tokenizing the dataframes is the most computationally expensive part of this script.  \n",
    "\n",
    "#tokenize left dataframe\n",
    "left_names_keyed = pd.DataFrame(columns=('token', 'id'))\n",
    "for item in prepare_name_join_keys(left_df, left_name_tokenizers):\n",
    "    left_names_keyed.loc[len(left_names_keyed)] = item\n",
    "    \n",
    "print(\"left names tokenized --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"TOKENIZING LEFT ORGS...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#tokenize left dataframe\n",
    "left_orgs_keyed = pd.DataFrame(columns=('token', 'id'))\n",
    "for item in prepare_org_join_keys(left_df, left_org_tokenizers):\n",
    "    left_orgs_keyed.loc[len(left_orgs_keyed)] = item\n",
    "    \n",
    "print(\"left orgs tokenized --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"TOKENIZING RIGHT NAMES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#tokenize right dataframe\n",
    "right_names_keyed = pd.DataFrame(columns=('token', 'id'))\n",
    "for item in prepare_name_join_keys(right_df, right_name_tokenizers):\n",
    "    right_names_keyed.loc[len(right_names_keyed)] = item\n",
    "    \n",
    "print(\"right names tokenized --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"TOKENIZING RIGHT ORGS...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "right_orgs_keyed = pd.DataFrame(columns=('token', 'id'))\n",
    "for item in prepare_org_join_keys(right_df, right_org_tokenizers):\n",
    "    right_orgs_keyed.loc[len(right_orgs_keyed)] = item\n",
    "    \n",
    "print(\"right orgs tokenized --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"JOINING LEFT & RIGHT TOKEN KEYS...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#join left and right token keys\n",
    "name_joined = left_names_keyed.join(right_names_keyed.set_index('token'), on='token', how='inner',lsuffix='_l',rsuffix='_r')\n",
    "org_joined = left_orgs_keyed.join(right_orgs_keyed.set_index('token'), on='token', how='inner',lsuffix='_l',rsuffix='_r')\n",
    "\n",
    "print(\"left & right token keys joined --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"GROUPING BY UNIQUE LEFT & RIGHT IDS & GETTING COUNT OF MATCHED TOKENS...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#aggregate to get a count of unique id_l and id_r pairs based on joined tokens, which is used to assess match strength\n",
    "aggregations = {\n",
    "    'id_l': 'count'\n",
    "}\n",
    "\n",
    "name_keys_grouped = name_joined.groupby(by=['id_l', 'id_r']).agg(aggregations)\n",
    "name_keys_grouped.rename(columns={'id_l':'name id_l count'}, inplace=True)\n",
    "name_matched_records = name_keys_grouped[name_keys_grouped['name id_l count'] >= name_token_match_min]\n",
    "\n",
    "org_keys_grouped = org_joined.groupby(by=['id_l', 'id_r']).agg(aggregations)\n",
    "org_keys_grouped.rename(columns={'id_l':'org id_l count'}, inplace=True)\n",
    "org_matched_records = org_keys_grouped[org_keys_grouped['org id_l count'] >= org_token_match_min]\n",
    "\n",
    "print(\"keys grouped & counted --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"JOINING NAME & ORG TOKEN MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "name_org_joined = name_matched_records.join(org_matched_records, on=['id_l', 'id_r'], how='inner')#,lsuffix='_l',rsuffix='_r')\n",
    "\n",
    "#here we are reducing down to those match candidates which meet the minimum threshold of matched tokens (name + org tokens)\n",
    "name_org_joined['sum all token matches'] = name_org_joined['name id_l count'] + name_org_joined['org id_l count']\n",
    "name_org_matched_records = name_org_joined[name_org_joined['sum all token matches'] >= overall_token_match_min]\n",
    "\n",
    "#adding columns for the id values to extract them from multiindex\n",
    "left_ids = name_org_matched_records.index.get_level_values(0)\n",
    "right_ids = name_org_matched_records.index.get_level_values(1)\n",
    "name_org_matched_records['id_l'] = left_ids\n",
    "name_org_matched_records['id_r'] = right_ids\n",
    "#reset index?\n",
    "\n",
    "print(\"name & org token matches joined and reduced --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CONCATENATING MATCH IDS WITH ORIGINAL DATA...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#renaming ids to be linked to source.  this happens here and not the beginning so as to play nice with the prepare join keys generator\n",
    "left_df.rename(columns={'id':'id_l'}, inplace=True)\n",
    "right_df.rename(columns={'id':'id_r'}, inplace=True)\n",
    "\n",
    "#creating left/right dataframes which contain only the most relevant details for reviewing the match strengths\n",
    "left_match_data = left_df[['id_l','l_first_name','l_middle_name','l_last_name','l_org_name','l_position','l_city','l_state','l_postal_code','l_email']].copy()\n",
    "right_match_data = right_df[['id_r','r_first_name','r_last_name','r_org_name','r_position','r_city','r_state','r_postal_code','r_email']].copy()\n",
    "\n",
    "#merging matched_records df with original record data for ease of review\n",
    "l_conc = pd.merge(name_org_matched_records, left_match_data, on='id_l')\n",
    "full_conc = pd.merge(l_conc, right_match_data, on='id_r')\n",
    "\n",
    "print(\"original data concatenated with matches --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING PERSON NAME SIMULARITY...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on edit distance of org names\n",
    "def jaro_simularity(left_record, right_record):\n",
    "    if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "        return jaro_winkler(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '')\n",
    "def fuzz_partial(left_record, right_record):\n",
    "    if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "        return fuzz.partial_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_sort(left_record, right_record):\n",
    "    if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "        return fuzz.token_sort_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_set(left_record, right_record):\n",
    "    if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "        return fuzz.token_set_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "\n",
    "full_conc['l_first_name'] = full_conc['l_first_name'].astype('str')\n",
    "#full_conc['l_middle_name'] = full_conc['l_middle_name'].astype('str')\n",
    "full_conc['l_last_name'] = full_conc['l_last_name'].astype('str')\n",
    "\n",
    "full_conc['r_first_name'] = full_conc['r_first_name'].astype('str')\n",
    "#full_conc['r_middle_name'] = full_conc['r_middle_name'].astype('str')\n",
    "full_conc['r_last_name'] = full_conc['r_last_name'].astype('str')\n",
    "    \n",
    "full_conc['first_jaro_score'] = full_conc.apply(lambda x: jaro_simularity(x.l_first_name, x.r_first_name), axis=1)\n",
    "full_conc['first_fuzz_partial_score'] = full_conc.apply(lambda x: fuzz_partial(x.l_first_name, x.r_first_name), axis=1)\n",
    "full_conc['first_fuzz_sort_score'] = full_conc.apply(lambda x: fuzz_sort(x.l_first_name, x.r_first_name), axis=1)\n",
    "full_conc['first_fuzz_set_score'] = full_conc.apply(lambda x: fuzz_set(x.l_first_name, x.r_first_name), axis=1)    \n",
    "   \n",
    "full_conc['last_jaro_score'] = full_conc.apply(lambda x: jaro_simularity(x.l_last_name, x.r_last_name), axis=1)\n",
    "full_conc['last_fuzz_partial_score'] = full_conc.apply(lambda x: fuzz_partial(x.l_last_name, x.r_last_name), axis=1)\n",
    "full_conc['last_fuzz_sort_score'] = full_conc.apply(lambda x: fuzz_sort(x.l_last_name, x.r_last_name), axis=1)\n",
    "full_conc['last_fuzz_set_score'] = full_conc.apply(lambda x: fuzz_set(x.l_last_name, x.r_last_name), axis=1)\n",
    "    \n",
    "print(\"person name simularities scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "    \n",
    "start_time = time.time()\n",
    "print \"SCORING ORG NAME SIMULARITY...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<    \n",
    "    \n",
    "full_conc['l_org_name'] = full_conc['l_org_name'].astype('str')\n",
    "full_conc['r_org_name'] = full_conc['r_org_name'].astype('str')\n",
    "\n",
    "full_conc['org_jaro_score'] = full_conc.apply(lambda x: jaro_simularity(x.l_org_name, x.r_org_name), axis=1)\n",
    "full_conc['org_fuzz_partial_score'] = full_conc.apply(lambda x: fuzz_partial(x.l_org_name, x.r_org_name), axis=1)\n",
    "full_conc['org_fuzz_sort_score'] = full_conc.apply(lambda x: fuzz_sort(x.l_org_name, x.r_org_name), axis=1)\n",
    "full_conc['org_fuzz_set_score'] = full_conc.apply(lambda x: fuzz_set(x.l_org_name, x.r_org_name), axis=1)\n",
    "\n",
    "print(\"org name simularity scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR POSTAL CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching postal code\n",
    "\n",
    "def sanitize_postal(postal):\n",
    "    if isinstance(postal, basestring) is True:\n",
    "        return ''.join(c for c in (postal or '') if c in '1234567890')\n",
    "    if isinstance(postal, float) is False:\n",
    "        return postal\n",
    "\n",
    "def postal_simularity(postal_a, postal_b):\n",
    "    sanitized_postal_a = str(sanitize_postal(postal_a))\n",
    "    sanitized_postal_b = str(sanitize_postal(postal_b))\n",
    "\n",
    "    # if the number is too short, means it's fubar\n",
    "    if len(sanitized_postal_a) < 5 or len(sanitized_postal_b) < 5:\n",
    "        return 0\n",
    "    if float(max(len(sub) for sub in find_common_subsequences(sanitized_postal_a, sanitized_postal_b))) / 5 >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "full_conc['zip_match'] = full_conc.apply(lambda x: postal_simularity(x.l_postal_code, x.r_postal_code), axis=1)\n",
    "    \n",
    "print(\"postal codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "#calculate composite match score based on component scores and weights\n",
    "full_conc['composite_person_name_match_score'] = full_conc.first_jaro_score * first_name_weight \\\n",
    "+ full_conc.first_fuzz_partial_score * first_name_weight \\\n",
    "+ full_conc.first_fuzz_sort_score * first_name_weight \\\n",
    "+ full_conc.first_fuzz_set_score * first_name_weight \\\n",
    "+ full_conc.last_jaro_score * last_name_weight \\\n",
    "+ full_conc.last_fuzz_partial_score * last_name_weight \\\n",
    "+ full_conc.last_fuzz_sort_score * last_name_weight \\\n",
    "+ full_conc.last_fuzz_set_score * last_name_weight \n",
    "\n",
    "full_conc['composite_org_match_score'] = full_conc.org_jaro_score * org_name_weight \\\n",
    "+ full_conc.org_fuzz_partial_score * org_name_weight \\\n",
    "+ full_conc.org_fuzz_sort_score * org_name_weight \\\n",
    "+ full_conc.org_fuzz_set_score * org_name_weight \\\n",
    "+ full_conc.zip_match * zip_weight\n",
    "\n",
    "full_conc['overall_composite_match_score'] = full_conc['composite_person_name_match_score'] + full_conc['composite_org_match_score']\n",
    "\n",
    "reduced_conc = full_conc[['id_l','id_r','l_first_name','l_middle_name','l_last_name','l_org_name','l_position','r_first_name','r_last_name','r_org_name','r_position','composite_person_name_match_score','composite_org_match_score','overall_composite_match_score']]\n",
    "reduced_conc.sort_values(by='overall_composite_match_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_conc.to_csv('reduced_conc first attempt contact match output test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_org_joined.to_csv('name_org_joined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3547\n",
       "1     694\n",
       "2     387\n",
       "Name: flag, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_tokens_df.flag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>id_l</th>\n",
       "      <th>id_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>melissa</td>\n",
       "      <td>417271</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>thompson</td>\n",
       "      <td>417271</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token    id_l id_r\n",
       "368    melissa  417271  126\n",
       "3909  thompson  417271  126"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_joined[(name_joined.id_r == 126) & (name_joined.id_l == 417271)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>id_l</th>\n",
       "      <th>id_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25940</th>\n",
       "      <td>digital</td>\n",
       "      <td>417271</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25941</th>\n",
       "      <td>strategy</td>\n",
       "      <td>417271</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>neuroscience</td>\n",
       "      <td>417271</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18027</th>\n",
       "      <td>dc</td>\n",
       "      <td>417271</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              token    id_l id_r\n",
       "25940       digital  417271  126\n",
       "25941      strategy  417271  126\n",
       "1145   neuroscience  417271  126\n",
       "18027            dc  417271  126"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_joined[(org_joined.id_r == 126) & (org_joined.id_l == 417271)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[417271    126] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-c631815c74ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mname_org_joined\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname_org_joined\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m417271\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m126\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2680\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2682\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2683\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2724\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2726\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2727\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\pandas\\core\\indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1327\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[417271    126] not in index'"
     ]
    }
   ],
   "source": [
    "name_org_joined[name_org_joined[[417271,126]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>id_l</th>\n",
       "      <th>id_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>dc</td>\n",
       "      <td>31681</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11968</th>\n",
       "      <td>20004</td>\n",
       "      <td>31681</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       token   id_l id_r\n",
       "8888      dc  31681  100\n",
       "8888      dc  31681    4\n",
       "8888      dc  31681    5\n",
       "8888      dc  31681    8\n",
       "8888      dc  31681   10\n",
       "8888      dc  31681   13\n",
       "8888      dc  31681   15\n",
       "8888      dc  31681   16\n",
       "8888      dc  31681   21\n",
       "8888      dc  31681   22\n",
       "8888      dc  31681   25\n",
       "8888      dc  31681   33\n",
       "8888      dc  31681   46\n",
       "8888      dc  31681   48\n",
       "8888      dc  31681   53\n",
       "8888      dc  31681   56\n",
       "8888      dc  31681   57\n",
       "8888      dc  31681   66\n",
       "8888      dc  31681   68\n",
       "8888      dc  31681   77\n",
       "8888      dc  31681   80\n",
       "8888      dc  31681   81\n",
       "8888      dc  31681   92\n",
       "8888      dc  31681   93\n",
       "8888      dc  31681   98\n",
       "8888      dc  31681  100\n",
       "8888      dc  31681  106\n",
       "8888      dc  31681  110\n",
       "8888      dc  31681  113\n",
       "8888      dc  31681  114\n",
       "...      ...    ...  ...\n",
       "8888      dc  31681  205\n",
       "8888      dc  31681  208\n",
       "8888      dc  31681  209\n",
       "8888      dc  31681  217\n",
       "8888      dc  31681  219\n",
       "8888      dc  31681  221\n",
       "8888      dc  31681  223\n",
       "8888      dc  31681  224\n",
       "8888      dc  31681  225\n",
       "8888      dc  31681  226\n",
       "8888      dc  31681  227\n",
       "8888      dc  31681  228\n",
       "8888      dc  31681  229\n",
       "8888      dc  31681  230\n",
       "8888      dc  31681  231\n",
       "8888      dc  31681  233\n",
       "8888      dc  31681  244\n",
       "8888      dc  31681  256\n",
       "8888      dc  31681  257\n",
       "8888      dc  31681  260\n",
       "8888      dc  31681  262\n",
       "8888      dc  31681  271\n",
       "8888      dc  31681  272\n",
       "8888      dc  31681  279\n",
       "8888      dc  31681  280\n",
       "8888      dc  31681  286\n",
       "8888      dc  31681  287\n",
       "8888      dc  31681  294\n",
       "8888      dc  31681  298\n",
       "11968  20004  31681   10\n",
       "\n",
       "[81 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_joined[org_joined.id_l == 31681]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SALES_DATE</th>\n",
       "      <th>r_first_name</th>\n",
       "      <th>r_last_name</th>\n",
       "      <th>ATTENDEE_CREDENTIALS</th>\n",
       "      <th>r_position</th>\n",
       "      <th>r_org_name</th>\n",
       "      <th>r_address1</th>\n",
       "      <th>r_address2</th>\n",
       "      <th>r_city</th>\n",
       "      <th>...</th>\n",
       "      <th>WHAT IS YOUR PRIMARY JOB RESPONSIBILITY?</th>\n",
       "      <th>HOW MANY YEARS HAVE YOU ATTENDED THE TECHNOLOGY CONFERENCE  &amp; EXPO?</th>\n",
       "      <th>HOW MANY YEARS OF EXPERIENCE DO YOU HAVE IN THE TECHNOLOGY FIELD?</th>\n",
       "      <th>WHAT IS THE MAIN REASON YOU ARE ATTENDING THIS YEAR'S TECHNOLOGY CONFERENCE?</th>\n",
       "      <th>WHAT IS THE STAFF SIZE OF YOUR ORGANIZATION?</th>\n",
       "      <th>WHAT IS YOUR APPROXIMATE TIMELINE FOR PURCHASING YOUR NEXT TECHNOLOGY PRODUCT/SERVICE?</th>\n",
       "      <th>WHAT IS YOUR ORGANIZATION'S ANNUAL TECHNOLOGY BUDGET?</th>\n",
       "      <th>WHAT IS YOUR ROLE IN THE PURCHASING OF PRODUCTS AND SERVICES FOR YOUR EMPLOYER?</th>\n",
       "      <th>OVER THE NEXT YEAR WILL YOUR COMPANY BE LOOKING TO PURCHASE/UPGRADE TECHNOLOGIES IN ANY OF THE FOLLOWING AREAS? (CHECK ALL THAT APPLY)</th>\n",
       "      <th>FOR THIS EVENT, HOW WOULD YOU LIKE TO HEAR FROM OUR SPONSORS AND/OR EXHIBITORS?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>11/28/2018 11:10 AM</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>McCourt</td>\n",
       "      <td></td>\n",
       "      <td>Senior Director, Operations</td>\n",
       "      <td>Association of Clinical Research Professionals</td>\n",
       "      <td>99 Canal Center Plaza #150</td>\n",
       "      <td></td>\n",
       "      <td>Alexandria</td>\n",
       "      <td>...</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1 - 2</td>\n",
       "      <td>10 -12</td>\n",
       "      <td>See Groundbreaking Technology</td>\n",
       "      <td>75 - 99</td>\n",
       "      <td>1 month - 3 months</td>\n",
       "      <td>$100,001 - $499,999</td>\n",
       "      <td>Final decision maker</td>\n",
       "      <td>Testing/Research and Survey Srvcs.;Email Marke...</td>\n",
       "      <td>Mail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id           SALES_DATE r_first_name r_last_name ATTENDEE_CREDENTIALS  \\\n",
       "10  11  11/28/2018 11:10 AM        Kevin     McCourt                        \n",
       "\n",
       "                     r_position  \\\n",
       "10  Senior Director, Operations   \n",
       "\n",
       "                                        r_org_name  \\\n",
       "10  Association of Clinical Research Professionals   \n",
       "\n",
       "                    r_address1 r_address2      r_city  \\\n",
       "10  99 Canal Center Plaza #150             Alexandria   \n",
       "\n",
       "                                         ...                                        \\\n",
       "10                                       ...                                         \n",
       "\n",
       "   WHAT IS YOUR PRIMARY JOB RESPONSIBILITY?  \\\n",
       "10                                    Sales   \n",
       "\n",
       "   HOW MANY YEARS HAVE YOU ATTENDED THE TECHNOLOGY CONFERENCE  & EXPO?  \\\n",
       "10                                              1 - 2                    \n",
       "\n",
       "   HOW MANY YEARS OF EXPERIENCE DO YOU HAVE IN THE TECHNOLOGY FIELD?  \\\n",
       "10                                             10 -12                  \n",
       "\n",
       "   WHAT IS THE MAIN REASON YOU ARE ATTENDING THIS YEAR'S TECHNOLOGY CONFERENCE?  \\\n",
       "10                      See Groundbreaking Technology                             \n",
       "\n",
       "   WHAT IS THE STAFF SIZE OF YOUR ORGANIZATION?  \\\n",
       "10                                      75 - 99   \n",
       "\n",
       "   WHAT IS YOUR APPROXIMATE TIMELINE FOR PURCHASING YOUR NEXT TECHNOLOGY PRODUCT/SERVICE?  \\\n",
       "10                                 1 month - 3 months                                       \n",
       "\n",
       "   WHAT IS YOUR ORGANIZATION'S ANNUAL TECHNOLOGY BUDGET?  \\\n",
       "10                                $100,001 - $499,999      \n",
       "\n",
       "   WHAT IS YOUR ROLE IN THE PURCHASING OF PRODUCTS AND SERVICES FOR YOUR EMPLOYER?  \\\n",
       "10                               Final decision maker                                \n",
       "\n",
       "   OVER THE NEXT YEAR WILL YOUR COMPANY BE LOOKING TO PURCHASE/UPGRADE TECHNOLOGIES IN ANY OF THE FOLLOWING AREAS? (CHECK ALL THAT APPLY)  \\\n",
       "10  Testing/Research and Survey Srvcs.;Email Marke...                                                                                       \n",
       "\n",
       "   FOR THIS EVENT, HOW WOULD YOU LIKE TO HEAR FROM OUR SPONSORS AND/OR EXHIBITORS?  \n",
       "10                                               Mail                               \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_df[right_df.id == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "org_keys_grouped.to_csv('org_keys_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_keys_grouped.to_csv('name_keys_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       3547\n",
       "2        475\n",
       "3        170\n",
       "4        103\n",
       "5         74\n",
       "7         33\n",
       "6         31\n",
       "8         27\n",
       "12        20\n",
       "10        19\n",
       "9         16\n",
       "11        15\n",
       "13         9\n",
       "17         9\n",
       "14         8\n",
       "22         6\n",
       "18         5\n",
       "16         5\n",
       "20         4\n",
       "27         4\n",
       "15         4\n",
       "21         4\n",
       "33         4\n",
       "28         3\n",
       "32         3\n",
       "19         3\n",
       "39         3\n",
       "42         2\n",
       "25         2\n",
       "38         2\n",
       "30         2\n",
       "23         2\n",
       "52         2\n",
       "24         1\n",
       "36         1\n",
       "67         1\n",
       "64         1\n",
       "29         1\n",
       "61         1\n",
       "69         1\n",
       "51         1\n",
       "3352       1\n",
       "26         1\n",
       "66         1\n",
       "97         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_tokens_df['count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mi = pd.MultiIndex.from_arrays((list('abc'), list('def')))\n",
    "mi.names = ['level_1', 'level_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex(levels=[[u'a', u'b', u'c'], [u'd', u'e', u'f']],\n",
       "           labels=[[0, 1, 2], [0, 1, 2]],\n",
       "           names=[u'level_1', u'level_2'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'a', u'b', u'c'], dtype='object', name=u'level_1')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name id_l count</th>\n",
       "      <th>org id_l count</th>\n",
       "      <th>sum all token matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>487591</th>\n",
       "      <th>218</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375510</th>\n",
       "      <th>299</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417271</th>\n",
       "      <th>126</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446026</th>\n",
       "      <th>158</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463722</th>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41253</th>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476343</th>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473224</th>\n",
       "      <th>143</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469598</th>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438638</th>\n",
       "      <th>232</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435213</th>\n",
       "      <th>282</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422144</th>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410506</th>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391253</th>\n",
       "      <th>269</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40679</th>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name id_l count  org id_l count  sum all token matches\n",
       "487591 218                2               7                      9\n",
       "375510 299                2               5                      7\n",
       "417271 126                2               4                      6\n",
       "446026 158                2               3                      5\n",
       "463722 149                1               3                      4\n",
       "41253  22                 1               2                      3\n",
       "476343 159                1               2                      3\n",
       "473224 143                1               2                      3\n",
       "469598 62                 1               2                      3\n",
       "438638 232                1               2                      3\n",
       "435213 282                1               2                      3\n",
       "422144 46                 1               2                      3\n",
       "410506 152                1               2                      3\n",
       "391253 269                1               2                      3\n",
       "40679  14                 1               2                      3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_org_matched_records.sort_values(by='sum all token matches',ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_org_matched_records.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 14,  22, 116, 152,  77,  24, 195,  79, 205,   9,  14, 151, 169,\n",
       "            169,  60, 299, 269, 152, 126,  46, 282, 232, 158, 149,  62, 143,\n",
       "            159, 218],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_org_matched_records.index.get_level_values(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
