{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from jellyfish import jaro_winkler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from py_common_subseq import find_common_subsequences\n",
    "import numbers\n",
    "import time\n",
    "from collections import Counter \n",
    "from fuzzywuzzy import fuzz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NOTES\n",
    "#removing duplicate token-ID keys inplace caused some strange ID issue, so I am not removing duplicates anymore.  could be improved.\n",
    "#unique_token_freq_max should really be derived dynamically by the number of tokens, some math\n",
    "#jaro score threshold is too low.  Try correlation matrix to identify the best score to use, not sure it is jaro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import shutil\n",
    "\n",
    "with io.open('mint orgs.txt', encoding='utf-8', errors='ignore') as source:\n",
    "    with io.open('mint orgs_utf.txt', mode='w', encoding='utf-8') as target:\n",
    "        shutil.copyfileobj(source,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 943: expected 10 fields, saw 11\n",
      "Skipping line 1897: expected 10 fields, saw 11\n",
      "Skipping line 2024: expected 10 fields, saw 11\n",
      "Skipping line 2379: expected 10 fields, saw 11\n",
      "Skipping line 9308: expected 10 fields, saw 11\n",
      "Skipping line 12943: expected 10 fields, saw 11\n",
      "Skipping line 20599: expected 10 fields, saw 11\n",
      "Skipping line 22142: expected 10 fields, saw 11\n",
      "Skipping line 22434: expected 10 fields, saw 11\n",
      "Skipping line 23297: expected 10 fields, saw 11\n",
      "Skipping line 24235: expected 10 fields, saw 11\n",
      "Skipping line 24955: expected 10 fields, saw 11\n",
      "Skipping line 25932: expected 10 fields, saw 11\n",
      "Skipping line 26535: expected 10 fields, saw 11\n",
      "Skipping line 26581: expected 10 fields, saw 11\n",
      "Skipping line 29338: expected 10 fields, saw 11\n",
      "Skipping line 31345: expected 10 fields, saw 11\n",
      "Skipping line 32432: expected 10 fields, saw 11\n",
      "Skipping line 35982: expected 10 fields, saw 11\n",
      "Skipping line 37408: expected 10 fields, saw 11\n",
      "Skipping line 40387: expected 10 fields, saw 11\n",
      "Skipping line 41439: expected 10 fields, saw 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>cbi_orgid</th>\n",
       "      <th>NAME</th>\n",
       "      <th>STREET1</th>\n",
       "      <th>STREET2</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STPROV</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>PHONE</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1ST ARMORED DIVISION ASSOCIATION</td>\n",
       "      <td>1815 LAKEWOOD DR</td>\n",
       "      <td></td>\n",
       "      <td>ELIZABETHTOWN</td>\n",
       "      <td>KY</td>\n",
       "      <td>42702</td>\n",
       "      <td>502-737-0901</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1ST CANADIAN PARACHUTE BATTALION ASSOCIATION</td>\n",
       "      <td>360 BLOOR ST E</td>\n",
       "      <td>APT 610</td>\n",
       "      <td>TORONTO</td>\n",
       "      <td>ON</td>\n",
       "      <td>M4W 3M3</td>\n",
       "      <td>416-927-0118</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>96749</td>\n",
       "      <td>1st Cavalry Division Association</td>\n",
       "      <td>302 North Main Street</td>\n",
       "      <td></td>\n",
       "      <td>Copperas Cove</td>\n",
       "      <td>TX</td>\n",
       "      <td>76522</td>\n",
       "      <td>(254) 547-6537</td>\n",
       "      <td>http://www.1cda.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>83774</td>\n",
       "      <td>1st Fighter Group Association</td>\n",
       "      <td>11512 Henegan Pl.</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Spotsylvania</td>\n",
       "      <td>VA</td>\n",
       "      <td>22551</td>\n",
       "      <td>308-632-3946</td>\n",
       "      <td>1stfighterassociation.weebly.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>85835</td>\n",
       "      <td>1st Marine Division Association</td>\n",
       "      <td>1132 E. St.</td>\n",
       "      <td>Camp Pendleton</td>\n",
       "      <td>North</td>\n",
       "      <td>CA</td>\n",
       "      <td>92055</td>\n",
       "      <td>760-763-3268</td>\n",
       "      <td>1stmarinedivisionassociation.org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rec_id cbi_orgid                                          NAME  \\\n",
       "0       1      NULL              1ST ARMORED DIVISION ASSOCIATION   \n",
       "1       2      NULL  1ST CANADIAN PARACHUTE BATTALION ASSOCIATION   \n",
       "2       3     96749              1st Cavalry Division Association   \n",
       "3       4     83774                 1st Fighter Group Association   \n",
       "4       5     85835               1st Marine Division Association   \n",
       "\n",
       "                 STREET1         STREET2           CITY STPROV      ZIP  \\\n",
       "0       1815 LAKEWOOD DR                  ELIZABETHTOWN     KY    42702   \n",
       "1         360 BLOOR ST E         APT 610        TORONTO     ON  M4W 3M3   \n",
       "2  302 North Main Street                  Copperas Cove     TX    76522   \n",
       "3      11512 Henegan Pl.            NULL   Spotsylvania     VA    22551   \n",
       "4            1132 E. St.  Camp Pendleton          North     CA    92055   \n",
       "\n",
       "            PHONE                                URL  \n",
       "0    502-737-0901                            unknown  \n",
       "1    416-927-0118                               NULL  \n",
       "2  (254) 547-6537                http://www.1cda.org  \n",
       "3    308-632-3946  1stfighterassociation.weebly.com/  \n",
       "4    760-763-3268   1stmarinedivisionassociation.org  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_df = pd.read_table('mint orgs_utf.txt',keep_default_na=False,error_bad_lines=False)\n",
    "right_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING INITIAL DATAFRAMES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 943: expected 10 fields, saw 11\n",
      "Skipping line 1897: expected 10 fields, saw 11\n",
      "Skipping line 2024: expected 10 fields, saw 11\n",
      "Skipping line 2379: expected 10 fields, saw 11\n",
      "Skipping line 9308: expected 10 fields, saw 11\n",
      "Skipping line 12943: expected 10 fields, saw 11\n",
      "Skipping line 20599: expected 10 fields, saw 11\n",
      "Skipping line 22142: expected 10 fields, saw 11\n",
      "Skipping line 22434: expected 10 fields, saw 11\n",
      "Skipping line 23297: expected 10 fields, saw 11\n",
      "Skipping line 24235: expected 10 fields, saw 11\n",
      "Skipping line 24955: expected 10 fields, saw 11\n",
      "Skipping line 25932: expected 10 fields, saw 11\n",
      "Skipping line 26535: expected 10 fields, saw 11\n",
      "Skipping line 26581: expected 10 fields, saw 11\n",
      "Skipping line 29338: expected 10 fields, saw 11\n",
      "Skipping line 31345: expected 10 fields, saw 11\n",
      "Skipping line 32432: expected 10 fields, saw 11\n",
      "Skipping line 35982: expected 10 fields, saw 11\n",
      "Skipping line 37408: expected 10 fields, saw 11\n",
      "Skipping line 40387: expected 10 fields, saw 11\n",
      "Skipping line 41439: expected 10 fields, saw 11\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes loaded --- 0.723000049591 seconds ---\n",
      "\n",
      "PRE-PROCESSING...\n",
      "states, phones, domains normalized --- 1.34100008011 seconds ---\n",
      "\n",
      "TOKENIZING, IDENTIFYING CANDIDATE MATCH PAIRS...\n",
      "match candidates identified --- 267.397000074 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\pandas\\core\\generic.py:4405: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data concatenated with matches --- 38.5650000572 seconds ---\n",
      "\n",
      "SCORING ORG NAME SIMULARITY...\n",
      "jaro scores done --- 1284.34299994 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuzz partial scores done --- 3155.148 seconds ---\n"
     ]
    }
   ],
   "source": [
    "overall_time = time.time()\n",
    "\n",
    "#define column names\n",
    "l_id = 'organization_id'\n",
    "l_name = 'org_name'\n",
    "l_address1 = 'address1'\n",
    "l_city = 'city'\n",
    "l_state = 'state'\n",
    "l_zip = 'postal_code'\n",
    "l_phone = 'phone'\n",
    "l_web = 'website'\n",
    "l_acronym = 'acronym'\n",
    "l_alt_name = 'alt_name'\n",
    "\n",
    "r_id = 'rec_id'\n",
    "r_name = 'NAME'\n",
    "r_address1 = 'STREET1'\n",
    "r_city = 'CITY'\n",
    "r_state = 'STPROV'\n",
    "r_zip = 'ZIP'\n",
    "r_phone = 'PHONE'\n",
    "r_web = 'URL'\n",
    "#r_acronym = 'CBI_Acronym'\n",
    "\n",
    "#set parameters\n",
    "token_match_min = 2 # minimum number of matched tokens to be considered a match\n",
    "token_limiter = .995 # percent of non-single tokens to tokenize, where rare tokens are at the bottom and common at the top\n",
    "unique_token_freq_max = 5 #threshold <= to a token is considered \"unique\" and links to these tokens are counted double\n",
    "name_weight = .75 #note that this is really .75 * 4 because there are 4 org name simularity metrics\n",
    "name_uniqueness_weight = 1.5\n",
    "state_weight = 1\n",
    "zip_weight = 1\n",
    "phone_weight = 2\n",
    "domain_weight = 2\n",
    "name_score_min = 3\n",
    "composite_score_min = 4 #minimum composite match score to be considered a match\n",
    "\n",
    "start_time = time.time()\n",
    "print \"LOADING INITIAL DATAFRAMES...\"\n",
    "\n",
    "left_df = pd.read_csv('all cupola orgs w phone_db_utf.csv',keep_default_na=False)\n",
    "right_df = pd.read_table('mint orgs_utf.txt',keep_default_na=False,error_bad_lines=False)\n",
    "\n",
    "left_df.rename(columns={l_id:'id',l_name:'l_org_name',l_address1:'l_address1',l_city:'l_city',l_state:'l_state',l_zip:'l_postal_code',l_web:'l_web',l_phone:'l_phone',l_acronym:'l_acronym',l_alt_name:'l_alt_name'}, inplace=True)\n",
    "right_df.rename(columns={r_id:'id',r_name:'r_org_name',r_address1:'r_address1',r_city:'r_city',r_state:'r_state',r_zip:'r_postal_code',r_web:'r_web',r_phone:'r_phone'}, inplace=True)\n",
    "\n",
    "print(\"Dataframes loaded --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"PRE-PROCESSING...\"\n",
    "#normalize state codes\n",
    "state_lkup = pd.read_csv('state_lkup.csv',keep_default_na=False)\n",
    "\n",
    "from collections import defaultdict\n",
    "state_dict = defaultdict(list)\n",
    "for state, acronym in zip(state_lkup.state.values,state_lkup.acronym.values):\n",
    "    state_dict[state].append(acronym)\n",
    "\n",
    "left_df.l_state = left_df.l_state.str.lower()\n",
    "left_df.l_state = left_df.l_state.replace(state_dict)\n",
    "right_df.r_state = right_df.r_state.str.lower()\n",
    "right_df.r_state = right_df.r_state.replace(state_dict)\n",
    "\n",
    "#clean up non numeric characters in phones\n",
    "l_clean_phones = []\n",
    "for phone in left_df.l_phone:\n",
    "    l_clean_phones.append(re.sub('[^0-9]','', phone))\n",
    "    \n",
    "left_df['l_clean_phone'] = l_clean_phones\n",
    "\n",
    "r_clean_phones = []\n",
    "for phone in right_df.r_phone:\n",
    "    r_clean_phones.append(re.sub('[^0-9]','', phone))\n",
    "\n",
    "right_df['r_clean_phone'] = r_clean_phones\n",
    "\n",
    "#isolate domains from web URLs\n",
    "l_domains = []\n",
    "for web in left_df.l_web:\n",
    "    l_domains.append(web.split('//')[-1].split('/')[0].strip('www.'))\n",
    "left_df['l_domain'] = l_domains\n",
    "    \n",
    "r_domains = []\n",
    "for web in right_df.r_web:\n",
    "    r_domains.append(web.split('//')[-1].split('/')[0].strip('www.'))\n",
    "right_df['r_domain'] = r_domains\n",
    "\n",
    "print(\"states, phones, domains normalized --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "print \"TOKENIZING, IDENTIFYING CANDIDATE MATCH PAIRS...\"\n",
    "\n",
    "left_unique_token_columns = [ #tokens here circumvent the token_limiter which is cutting off the most commonly occuring tokens\n",
    "    'l_state', \n",
    "    'l_postal_code',\n",
    "    'l_clean_phone'\n",
    "\n",
    "]\n",
    "\n",
    "left_delta_token_columns = [\n",
    "    'l_acronym',\n",
    "    'l_org_name',\n",
    "    'l_alt_name',\n",
    "    #'l_address1',\n",
    "    #'l_address2',\n",
    "    'l_city',\n",
    "    'l_domain'\n",
    "]\n",
    "\n",
    "right_unique_token_columns = [\n",
    "    'r_state', \n",
    "    'r_postal_code',\n",
    "    'r_clean_phone'\n",
    "]\n",
    "\n",
    "right_delta_token_columns = [\n",
    "    #'r_acronym',\n",
    "    'r_org_name',\n",
    "    #'r_alt_name',\n",
    "    #'r_address1',\n",
    "    #'r_address2',\n",
    "    'r_city',\n",
    "    'r_domain'\n",
    "]\n",
    "\n",
    "# lowercase the name and split on spaces, remove non-alphanumeric chars\n",
    "def tokenize_name(name):\n",
    "    if isinstance(name, basestring) is True:\n",
    "        clean_name = ''.join(c if c.isalnum() else ' ' for c in name)\n",
    "        return clean_name.lower().split()\n",
    "    else:\n",
    "        return name\n",
    "    \n",
    "unique_tokens = []    \n",
    "for col in left_unique_token_columns:\n",
    "    for word in left_df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            unique_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "for col in right_unique_token_columns:\n",
    "    for word in right_df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            unique_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "unique_flat_list = [item for sublist in unique_tokens for item in sublist]\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "u_cnt = Counter()\n",
    "for token in unique_flat_list:\n",
    "    u_cnt[token] += 1\n",
    "\n",
    "u_cnt_dict = dict(u_cnt) #convert to dictionary\n",
    "\n",
    "unique_tokens_df = pd.DataFrame(u_cnt_dict.items(), columns=['token', 'count'])\n",
    "unique_tokens_df = unique_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "#consider waiting to do the count flag thing later, instead use some type of \"token type\" code\n",
    "unique_token_flag = []\n",
    "for index, value in enumerate(unique_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        unique_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    else:\n",
    "        unique_token_flag.append(1)\n",
    "\n",
    "unique_tokens_df['flag'] = unique_token_flag        \n",
    "\n",
    "all_other_words = []\n",
    "for col in left_delta_token_columns:\n",
    "    for word in left_df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            all_other_words.append(tokenize_name(str(word)))\n",
    "            \n",
    "for col in right_delta_token_columns:\n",
    "    for word in right_df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            all_other_words.append(tokenize_name(str(word)))\n",
    "            \n",
    "flat_list = [item for sublist in all_other_words for item in sublist] #flatten list so it can be counted\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "cnt = Counter()\n",
    "for token in flat_list:\n",
    "    cnt[token] += 1\n",
    "\n",
    "cnt_dict = dict(cnt) #convert to dictionary\n",
    "\n",
    "main_tokens_df = pd.DataFrame(cnt_dict.items(), columns=['token', 'count'])\n",
    "main_tokens_df = main_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "#wait to do count until joined with unique tokens?\n",
    "main_token_flag = []\n",
    "for index, value in enumerate(main_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        main_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    elif index < int(main_tokens_df.shape[0] * token_limiter): #important line, we are cutting the top x% of frequently occuring tokens\n",
    "        main_token_flag.append(1)\n",
    "    else:\n",
    "        main_token_flag.append(0)  #for the most common tokens, we exclude\n",
    "\n",
    "main_tokens_df['flag'] = main_token_flag\n",
    "\n",
    "all_tokens = pd.concat([unique_tokens_df, main_tokens_df])\n",
    "\n",
    "all_tokens.drop('count',axis=1,inplace=True)\n",
    "all_tokens['flag'] = all_tokens.flag.astype(int) #converting flags to int\n",
    "tokens_dct = all_tokens.to_dict('split') #converting tokens_df to dictionary\n",
    "tokens_dct=dict(tokens_dct['data']) #honestly can't remember why this works, something to do with conversion to dictionary\n",
    "\n",
    "#preparing token_ids which will be used for joining left and right dfs\n",
    "all_tokens.sort_values(by='flag',ascending=False,inplace=True)\n",
    "all_tokens.sort_values(by='token',inplace=True)\n",
    "all_tokens.drop_duplicates(subset='token',keep='first',inplace=True)\n",
    "token_ids = all_tokens.index.get_level_values(0)\n",
    "all_tokens['token_id'] = token_ids\n",
    "\n",
    "all_tokens.drop('flag',axis=1,inplace=True)\n",
    "all_tokens['token_id'] = all_tokens.token_id.astype(int)\n",
    "token_id_dct = all_tokens.to_dict('split')\n",
    "tokens_id_dct=dict(token_id_dct['data'])\n",
    "\n",
    "vocabulary = np.array([w for w, c in tokens_dct.items() if c ==1]) #this works even without the ==1 and I don't know why\n",
    "cv = CountVectorizer( vocabulary=vocabulary)\n",
    "\n",
    "#now we are ready to tokenize left and right dataframes\n",
    "all_left_cols = left_unique_token_columns + left_delta_token_columns\n",
    "\n",
    "left_frame_list = []\n",
    "for colname in all_left_cols:\n",
    "    tokenmapping = cv.fit_transform(left_df[colname])\n",
    "    df_row, token_id = tokenmapping.nonzero()\n",
    "\n",
    "    left_frame_list.append(pd.DataFrame(np.vstack([vocabulary[token_id], left_df['id'].values[df_row]]).T, columns = ['token', 'id_l']))\n",
    "\n",
    "left_keyed = pd.concat(left_frame_list)\n",
    "left_keyed.drop_duplicates()#inplace=True)\n",
    "#removing duplicates inplace was giving me a very strange issue where a small percentage of token_ids would be excluded from the left_keyed index\n",
    "\n",
    "#append token_id to token as this will be more efficient to join with\n",
    "left_token_ids = []\n",
    "for token in left_keyed.token:\n",
    "    left_token_ids.append(tokens_id_dct[token])\n",
    "\n",
    "left_keyed['token_id'] = left_token_ids\n",
    "left_keyed.sort_values(by='token_id',inplace=True)\n",
    "left_keyed.set_index('token_id',inplace=True)\n",
    "left_keyed.drop('token',axis=1,inplace=True)\n",
    "\n",
    "all_right_cols = right_unique_token_columns + right_delta_token_columns\n",
    "\n",
    "right_frame_list = []\n",
    "for colname in all_right_cols:\n",
    "    tokenmapping = cv.fit_transform(right_df[colname])\n",
    "    df_row, token_id = tokenmapping.nonzero()\n",
    "\n",
    "    right_frame_list.append(pd.DataFrame(np.vstack([vocabulary[token_id], right_df['id'].values[df_row]]).T, columns = ['token', 'id_r']))\n",
    "\n",
    "right_keyed = pd.concat(right_frame_list)\n",
    "right_keyed.drop_duplicates()#inplace=True)\n",
    "\n",
    "#append token_id to token as this will be more efficient to join with\n",
    "right_token_ids = []\n",
    "for token in right_keyed.token:\n",
    "    right_token_ids.append(tokens_id_dct[token])\n",
    "\n",
    "right_keyed['token_id'] = right_token_ids\n",
    "right_keyed.sort_values(by='token_id',inplace=True)\n",
    "right_keyed.set_index('token_id',inplace=True)\n",
    "right_keyed.drop('token',axis=1,inplace=True)\n",
    "\n",
    "aggregations = {\n",
    "    'id_l': 'count'\n",
    "}\n",
    "\n",
    "joined = left_keyed.join(right_keyed, how='inner',lsuffix='_l',rsuffix='_r')\n",
    "#double-counting unique token matches\n",
    "bonus_point_tokens = []\n",
    "for token in main_tokens_df[(main_tokens_df['count'] > 1) & (main_tokens_df['count']<=unique_token_freq_max)].token:\n",
    "    bonus_point_tokens.append(tokens_id_dct[token])\n",
    "\n",
    "intersection_bonus_tokens = set(bonus_point_tokens).intersection(set(list(joined.index)))    \n",
    "    \n",
    "bonus_token_joins = []\n",
    "for token_id in intersection_bonus_tokens:\n",
    "    bonus_token_joins.append(joined.loc[token_id])\n",
    "    \n",
    "bonus_joins = pd.concat(bonus_token_joins)\n",
    "bonus_joins_cols = bonus_joins[['id_l','id_r']].copy()\n",
    "bonus_joins_cols.dropna(inplace=True)\n",
    "joined_bonus = pd.concat([joined,bonus_joins_cols])\n",
    "\n",
    "keys_grouped = joined_bonus.groupby(by=['id_l', 'id_r']).agg(aggregations)\n",
    "keys_grouped.rename(columns={'id_l':'id_l count'}, inplace=True)\n",
    "matched_records = keys_grouped[keys_grouped['id_l count'] >= token_match_min]\n",
    "matched_records.reset_index(inplace=True)\n",
    "\n",
    "print(\"match candidates identified --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "left_df.rename(columns={'id':'id_l'},inplace=True)\n",
    "right_df.rename(columns={'id':'id_r'},inplace=True)\n",
    "\n",
    "left_match_data = left_df[['id_l','l_org_name','l_city','l_state','l_postal_code','l_domain','l_clean_phone']].copy()\n",
    "right_match_data = right_df[['id_r','r_org_name','r_city','r_state','r_postal_code','r_domain','r_clean_phone']].copy()\n",
    "\n",
    "#making sure keys are str, results in blank df otherwise\n",
    "left_match_data.id_l = left_match_data.id_l.astype('str')\n",
    "right_match_data.id_r = right_match_data.id_r.astype('str')\n",
    "matched_records.id_l = matched_records.id_l.astype('str')\n",
    "matched_records.id_r = matched_records.id_r.astype('str')\n",
    "\n",
    "#merging matched_records df with original record data for ease of review\n",
    "l_conc = pd.merge(matched_records, left_match_data, on='id_l')\n",
    "full_conc = pd.merge(l_conc, right_match_data, on='id_r')\n",
    "\n",
    "print(\"original data concatenated with matches --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING ORG NAME SIMULARITY...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on edit distance of org names\n",
    "def jaro_simularity(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return jaro_winkler(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '')\n",
    "def fuzz_partial(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.partial_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_sort(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_sort_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_set(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_set_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "\n",
    "full_conc['l_org_name'] = full_conc['l_org_name'].astype('str')\n",
    "full_conc['r_org_name'] = full_conc['r_org_name'].astype('str')\n",
    "\n",
    "jaro_time = time.time()\n",
    "full_conc['jaro_score'] = full_conc.apply(lambda x: jaro_simularity(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"jaro scores done --- %s seconds ---\" % (time.time() - jaro_time))\n",
    "\n",
    "jaro_reduced = full_conc[full_conc.jaro_score > .25]\n",
    "\n",
    "partial_time = time.time()\n",
    "jaro_reduced['fuzz_partial_score'] = jaro_reduced.apply(lambda x: fuzz_partial(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz partial scores done --- %s seconds ---\" % (time.time() - partial_time))\n",
    "sort_time = time.time()\n",
    "jaro_reduced['fuzz_sort_score'] = jaro_reduced.apply(lambda x: fuzz_sort(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz sort scores done --- %s seconds ---\" % (time.time() - sort_time))\n",
    "set_time = time.time()\n",
    "jaro_reduced['fuzz_set_score'] = jaro_reduced.apply(lambda x: fuzz_set(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz set scores done --- %s seconds ---\" % (time.time() - set_time))\n",
    "print \"\"\n",
    "\n",
    "print(\"name simularity scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING NAME SEQUENCE UNIQUENESS...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "org_tokens = []    \n",
    "for word in left_df['l_org_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_tokens.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in right_df['r_org_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "org_flat_list = [item for sublist in org_tokens for item in sublist]\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "org_cnt = Counter()\n",
    "for token in org_flat_list:\n",
    "    org_cnt[token] += 1\n",
    "\n",
    "org_cnt_dict = dict(org_cnt) #convert to dictionary\n",
    "\n",
    "def sequence_uniqueness(seq):\n",
    "    return sum(1/org_cnt_dict[str.lower(t)]**0.5 for t in seq)\n",
    "\n",
    "def name_similarity(a, b):\n",
    "    if isinstance(a,basestring) is True and isinstance(b,basestring) is True:\n",
    "        if len(a) > 0 and len(b) > 0:\n",
    "            a_tokens = set(tokenize_name(a))\n",
    "            b_tokens = set(tokenize_name(b))\n",
    "            a_uniq = sequence_uniqueness(a_tokens)\n",
    "            b_uniq = sequence_uniqueness(b_tokens)\n",
    "\n",
    "            return sequence_uniqueness(a_tokens.intersection(b_tokens))/(a_uniq * b_uniq) ** 0.5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "jaro_reduced['uniq'] = jaro_reduced.apply(lambda x: name_similarity(x.l_org_name, x.r_org_name), axis=1)\n",
    "\n",
    "print(\"name uniqueness scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR STATE CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def sanitize_state(state):\n",
    "    if isinstance(state,basestring) is True:\n",
    "        return ''.join(c for c in (state or '') if c in 'abcdefghijklmnopqrstuvwxyz')\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def state_match(state_a, state_b):\n",
    "    sanitized_state_a = str(sanitize_state(state_a))\n",
    "    sanitized_state_b = str(sanitize_state(state_b))\n",
    "\n",
    "    # if the value is too short, means it's fubar\n",
    "    if len(sanitized_state_a) < 2 or len(sanitized_state_b) < 2:\n",
    "        return 0\n",
    "    if state_a == state_b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "jaro_reduced['state_match'] = jaro_reduced.apply(lambda x: state_match(x.l_state, x.r_state), axis=1)\n",
    "\n",
    "print(\"state codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR POSTAL CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching postal code\n",
    "\n",
    "def sanitize_postal(postal):\n",
    "    if isinstance(postal, basestring) is True:\n",
    "        return ''.join(c for c in (postal or '') if c in '1234567890')\n",
    "    if isinstance(postal, float) is False:\n",
    "        return postal\n",
    "\n",
    "def postal_simularity(postal_a, postal_b):\n",
    "    sanitized_postal_a = str(sanitize_postal(postal_a))\n",
    "    sanitized_postal_b = str(sanitize_postal(postal_b))\n",
    "\n",
    "    # if the number is too short, means it's fubar\n",
    "    if len(sanitized_postal_a) < 5 or len(sanitized_postal_b) < 5:\n",
    "        return 0\n",
    "    if float(max(len(sub) for sub in find_common_subsequences(sanitized_postal_a, sanitized_postal_b))) / 5 >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "jaro_reduced['zip_match'] = jaro_reduced.apply(lambda x: postal_simularity(x.l_postal_code, x.r_postal_code), axis=1)\n",
    "    \n",
    "print(\"postal codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR WEB DOMAIN MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def domain_match(domain_a, domain_b):\n",
    "    if isinstance(domain_a, basestring) is True and isinstance(domain_b, basestring) is True:\n",
    "        if domain_a == domain_b:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "jaro_reduced['domain_match'] = jaro_reduced.apply(lambda x: domain_match(x.l_domain, x.r_domain), axis=1)\n",
    "\n",
    "print(\"web domains checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR PHONE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching phone\n",
    "def phone_simularity(phone_a, phone_b):\n",
    "\n",
    "    if len(phone_a) < 10 or len(phone_b) < 10:\n",
    "        return 0\n",
    "    if float(max(len(sub) for sub in find_common_subsequences(phone_a, phone_b))) / 10 >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "jaro_reduced['phone_match'] = jaro_reduced.apply(lambda x: phone_simularity(x.l_clean_phone, x.r_clean_phone), axis=1)\n",
    "    \n",
    "print(\"phones checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"DISTILLING STRONG ORG MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "jaro_reduced['overall_name_score'] = jaro_reduced.jaro_score * name_weight \\\n",
    "+ jaro_reduced.fuzz_partial_score * name_weight \\\n",
    "+ jaro_reduced.fuzz_sort_score * name_weight \\\n",
    "+ jaro_reduced.fuzz_set_score * name_weight \\\n",
    "+ jaro_reduced.uniq * name_uniqueness_weight\n",
    "\n",
    "#calculate composite match score based on component scores and weights\n",
    "jaro_reduced['composite_match_score'] = jaro_reduced.overall_name_score \\\n",
    "+ jaro_reduced.zip_match * zip_weight \\\n",
    "+ jaro_reduced.state_match * state_weight \\\n",
    "+ jaro_reduced.domain_match * domain_weight \\\n",
    "+ jaro_reduced.phone_match * phone_weight\n",
    "\n",
    "#we take any matches meeting either name match threshold or composite match threshold as matches for review\n",
    "org_matches = jaro_reduced[(jaro_reduced.overall_name_score >= name_score_min) | (jaro_reduced.composite_match_score >= composite_score_min)]\n",
    "\n",
    "print(\"final matches isolated --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "print(\"TOTAL COMPUTE TIME --- %s seconds ---\" % (time.time() - overall_time))\n",
    "\n",
    "org_matches.sort_values(by='composite_match_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_conc.loc[276996].r_org_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_conc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_uniqueness(seq):\n",
    "    return sum(1/org_cnt_dict[str.lower(t)]**0.5 for t in seq)\n",
    "\n",
    "def name_similarity(a, b):\n",
    "    if isinstance(a,basestring) is True and isinstance(b,basestring) is True:\n",
    "        if len(a) > 0 and len(b) > 0:\n",
    "            a_tokens = set(tokenize_name(a))\n",
    "            b_tokens = set(tokenize_name(b))\n",
    "            a_uniq = sequence_uniqueness(a_tokens)\n",
    "            b_uniq = sequence_uniqueness(b_tokens)\n",
    "\n",
    "            return sequence_uniqueness(a_tokens.intersection(b_tokens))/(a_uniq * b_uniq) ** 0.5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "full_conc['uniq'] = full_conc.apply(lambda x: name_similarity(x.l_org_name, x.r_org_name), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
