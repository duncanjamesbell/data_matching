{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from jellyfish import jaro_winkler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from py_common_subseq import find_common_subsequences\n",
    "import numbers\n",
    "import time\n",
    "from collections import Counter \n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import shutil\n",
    "\n",
    "with io.open('990 match data.csv', encoding='utf-8', errors='ignore') as source:\n",
    "    with io.open('990 match data_utf.csv', mode='w', encoding='utf-8') as target:\n",
    "        shutil.copyfileobj(source,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_time = time.time()\n",
    "\n",
    "#define column names\n",
    "org_id = 'organization_id'\n",
    "org_name = 'org_name'\n",
    "org_address1 = 'address1'\n",
    "org_city = 'city'\n",
    "org_state = 'state'\n",
    "org_zip = 'postal_code'\n",
    "org_phone = 'phone'\n",
    "org_web = 'website'\n",
    "org_acronym = 'acronym'\n",
    "org_alt_name = 'alt_name'\n",
    "\n",
    "#set parameters\n",
    "token_match_min = 2 # minimum number of matched tokens to be considered a match\n",
    "token_limiter = .995 # percent of non-single tokens to tokenize, where rare tokens are at the bottom and common at the top\n",
    "unique_token_freq_max = 5 #threshold <= to a token is considered \"unique\" and links to these tokens are counted double\n",
    "name_weight = .75 #note that this is really .75 * 4 because there are 4 org name simularity metrics\n",
    "name_uniqueness_weight = 1.5\n",
    "state_weight = 1\n",
    "zip_weight = 1\n",
    "phone_weight = 2\n",
    "domain_weight = 2\n",
    "name_score_min = 3\n",
    "composite_score_min = 4 #minimum composite match score to be considered a match\n",
    "\n",
    "start_time = time.time()\n",
    "print \"LOADING INITIAL DATAFRAMES...\"\n",
    "\n",
    "df = pd.read_csv('all cupola orgs w phone_db_utf.csv',keep_default_na=True)\n",
    "\n",
    "df.rename(columns={org_id:'id',org_name:'org_name',org_address1:'org_address1',org_city:'org_city',org_state:'org_state',org_zip:'org_zip',l_web:'l_web',org_phone:'org_phone',org_acronym:'org_acronym',org_alt_name:'org_alt_name'}, inplace=True)\n",
    "\n",
    "df = df.replace(np.nan, '', regex=True)\n",
    "\n",
    "#right_df.r_org_name = right_df.r_org_name.astype('str')\n",
    "#right_df.r_address1 = right_df.r_address1.astype('str')\n",
    "#right_df.r_city = right_df.r_city.astype('str')\n",
    "#right_df.r_state = right_df.r_state.astype('str')\n",
    "#right_df.r_postal_code = right_df.r_postal_code.astype('str')\n",
    "#right_df.r_phone = right_df.r_phone.astype('str')\n",
    "#right_df.r_web = right_df.r_web.astype('str')\n",
    "\n",
    "print(\"Dataframes loaded --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"PRE-PROCESSING...\"\n",
    "#normalize state codes\n",
    "state_lkup = pd.read_csv('state_lkup.csv',keep_default_na=False)\n",
    "\n",
    "from collections import defaultdict\n",
    "state_dict = defaultdict(list)\n",
    "for state, acronym in zip(state_lkup.state.values,state_lkup.acronym.values):\n",
    "    state_dict[state].append(acronym)\n",
    "\n",
    "df.org_state = df.org_state.replace(np.nan, '', regex=True).str.lower()\n",
    "df.org_state = df.org_state.replace(state_dict)\n",
    "\n",
    "#clean up non numeric characters in phones\n",
    "clean_phones = []\n",
    "df.org_phone = df.org_phone.astype('str')\n",
    "df.org_phone = df.org_phone.replace(np.nan, '', regex=True)\n",
    "for phone in df.org_phone:\n",
    "    clean_phones.append(re.sub('[^0-9]','', phone))\n",
    "    \n",
    "df['clean_phone'] = clean_phones\n",
    "\n",
    "#isolate domains from web URLs\n",
    "domains = []\n",
    "df.org_web = df.org_web.replace(np.nan, '', regex=True).str.lower()\n",
    "for web in df.org_web:\n",
    "    domains.append(web.split('//')[-1].split('/')[0].strip('www.'))\n",
    "df['domain'] = domains\n",
    "\n",
    "print(\"states, phones, domains normalized --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"TOKENIZING, IDENTIFYING CANDIDATE MATCH PAIRS...\"\n",
    "\n",
    "unique_token_columns = [ #tokens here circumvent the token_limiter which is cutting off the most commonly occuring tokens\n",
    "    'org_state', \n",
    "    'org_zip',\n",
    "    'clean_phone'\n",
    "\n",
    "]\n",
    "\n",
    "delta_token_columns = [\n",
    "    'org_acronym',\n",
    "    'org_name',\n",
    "    'org_alt_name',\n",
    "    #'org_address1',\n",
    "    'org_city',\n",
    "    'domain'\n",
    "]\n",
    "\n",
    "# lowercase the name and split on spaces, remove non-alphanumeric chars\n",
    "def tokenize_name(name):\n",
    "    if isinstance(name, basestring) is True:\n",
    "        clean_name = ''.join(c if c.isalnum() else ' ' for c in name)\n",
    "        return clean_name.lower().split()\n",
    "    else:\n",
    "        return name\n",
    "    \n",
    "unique_tokens = []    \n",
    "for col in unique_token_columns:\n",
    "    for word in df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            unique_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "unique_flat_list = [item for sublist in unique_tokens for item in sublist]\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "u_cnt = Counter()\n",
    "for token in unique_flat_list:\n",
    "    u_cnt[token] += 1\n",
    "\n",
    "u_cnt_dict = dict(u_cnt) #convert to dictionary\n",
    "\n",
    "unique_tokens_df = pd.DataFrame(u_cnt_dict.items(), columns=['token', 'count'])\n",
    "unique_tokens_df = unique_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "#consider waiting to do the count flag thing later, instead use some type of \"token type\" code\n",
    "unique_token_flag = []\n",
    "for index, value in enumerate(unique_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        unique_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    else:\n",
    "        unique_token_flag.append(1)\n",
    "\n",
    "unique_tokens_df['flag'] = unique_token_flag        \n",
    "\n",
    "all_other_words = []\n",
    "for col in delta_token_columns:\n",
    "    for word in df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            all_other_words.append(tokenize_name(str(word)))\n",
    "            \n",
    "flat_list = [item for sublist in all_other_words for item in sublist] #flatten list so it can be counted\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "cnt = Counter()\n",
    "for token in flat_list:\n",
    "    cnt[token] += 1\n",
    "\n",
    "cnt_dict = dict(cnt) #convert to dictionary\n",
    "\n",
    "main_tokens_df = pd.DataFrame(cnt_dict.items(), columns=['token', 'count'])\n",
    "main_tokens_df = main_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "#wait to do count until joined with unique tokens?\n",
    "main_token_flag = []\n",
    "for index, value in enumerate(main_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        main_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    elif index < int(main_tokens_df.shape[0] * token_limiter): #important line, we are cutting the top x% of frequently occuring tokens\n",
    "        main_token_flag.append(1)\n",
    "    else:\n",
    "        main_token_flag.append(0)  #for the most common tokens, we exclude\n",
    "\n",
    "main_tokens_df['flag'] = main_token_flag\n",
    "\n",
    "all_tokens = pd.concat([unique_tokens_df, main_tokens_df])\n",
    "\n",
    "all_tokens.drop('count',axis=1,inplace=True)\n",
    "all_tokens['flag'] = all_tokens.flag.astype(int) #converting flags to int\n",
    "tokens_dct = all_tokens.to_dict('split') #converting tokens_df to dictionary\n",
    "tokens_dct=dict(tokens_dct['data']) #honestly can't remember why this works, something to do with conversion to dictionary\n",
    "\n",
    "#preparing token_ids which will be used for joining left and right dfs\n",
    "all_tokens.sort_values(by='flag',ascending=False,inplace=True)\n",
    "all_tokens.sort_values(by='token',inplace=True)\n",
    "all_tokens.drop_duplicates(subset='token',keep='first',inplace=True)\n",
    "token_ids = all_tokens.index.get_level_values(0)\n",
    "all_tokens['token_id'] = token_ids\n",
    "\n",
    "all_tokens.drop('flag',axis=1,inplace=True)\n",
    "all_tokens['token_id'] = all_tokens.token_id.astype(int)\n",
    "token_id_dct = all_tokens.to_dict('split')\n",
    "tokens_id_dct=dict(token_id_dct['data'])\n",
    "\n",
    "vocabulary = np.array([w for w, c in tokens_dct.items() if c ==1]) #this works even without the ==1 and I don't know why\n",
    "cv = CountVectorizer( vocabulary=vocabulary)\n",
    "\n",
    "#now we are ready to tokenize left and right dataframes\n",
    "all_cols = unique_token_columns + delta_token_columns\n",
    "\n",
    "left_frame_list = []\n",
    "for colname in all_left_cols:\n",
    "    tokenmapping = cv.fit_transform(df[colname])\n",
    "    df_row, token_id = tokenmapping.nonzero()\n",
    "\n",
    "    left_frame_list.append(pd.DataFrame(np.vstack([vocabulary[token_id], df['id'].values[df_row]]).T, columns = ['token', 'id_l']))\n",
    "\n",
    "left_keyed = pd.concat(left_frame_list)\n",
    "left_keyed.drop_duplicates()#inplace=True)\n",
    "#removing duplicates inplace was giving me a very strange issue where a small percentage of token_ids would be excluded from the left_keyed index\n",
    "\n",
    "#append token_id to token as this will be more efficient to join with\n",
    "left_token_ids = []\n",
    "for token in left_keyed.token:\n",
    "    left_token_ids.append(tokens_id_dct[token])\n",
    "\n",
    "left_keyed['token_id'] = left_token_ids\n",
    "left_keyed.sort_values(by='token_id',inplace=True)\n",
    "left_keyed.set_index('token_id',inplace=True)\n",
    "left_keyed.drop('token',axis=1,inplace=True)\n",
    "\n",
    "left_keyed.sort_values(by='id_l',inplace=True)\n",
    "\n",
    "#right is just a copy of left\n",
    "right_keyed = left_keyed.copy()\n",
    "\n",
    "aggregations = {\n",
    "    'id_l': 'count'\n",
    "}\n",
    "\n",
    "left_keyed.to_csv('left_keyed.csv')\n",
    "bonus_point_tokens = []\n",
    "for token in main_tokens_df[(main_tokens_df['count'] > 1) & (main_tokens_df['count']<=unique_token_freq_max)].token:\n",
    "    bonus_point_tokens.append(tokens_id_dct[token])\n",
    "\n",
    "match_dfs = []\n",
    "for chunk in pd.read_csv('left_keyed.csv',keep_default_na=False,chunksize=50000,index_col='token_id'):\n",
    "    \n",
    "    joined = chunk.join(right_keyed, how='inner',lsuffix='_l',rsuffix='_r')\n",
    "    joined['id_l']=joined.id_l.astype('str')\n",
    "    #double-counting unique token matches\n",
    "\n",
    "    intersection_bonus_tokens = set(bonus_point_tokens).intersection(set(list(joined.index)))    \n",
    "\n",
    "    bonus_token_joins = []\n",
    "    for token_id in intersection_bonus_tokens:\n",
    "        bonus_token_joins.append(joined.loc[token_id])\n",
    "\n",
    "    bonus_joins = pd.concat(bonus_token_joins)\n",
    "    bonus_joins_cols = bonus_joins[['id_l','id_r']].copy()\n",
    "    bonus_joins_cols.dropna(inplace=True)\n",
    "    joined_bonus = pd.concat([joined,bonus_joins_cols])\n",
    "    \n",
    "    keys_grouped = joined_bonus.groupby(by=['id_l', 'id_r']).agg(aggregations)\n",
    "    keys_grouped.rename(columns={'id_l':'id_l count'}, inplace=True)\n",
    "    matched_records = keys_grouped[keys_grouped['id_l count'] >= token_match_min]\n",
    "    matched_records.reset_index(inplace=True)\n",
    "    duplicate_candidates = matched_records[matched_records['id_l'] <> matched_records['id_r']]\n",
    "    match_dfs.append(duplicate_candidates)\n",
    "    \n",
    "all_match_candidates = pd.concat(match_dfs)\n",
    "all_match_candidates.reset_index(inplace=True)\n",
    "all_match_candidates.drop(labels='index',axis=1)\n",
    "all_match_candidates.id_l.astype('str')\n",
    "\n",
    "print(\"match candidates identified --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "print \"REMOVING REDUNDANT DUPLICATE ID PAIRS...\"\n",
    "\n",
    "match_tuples = list(zip(all_match_candidates['id_l'], all_match_candidates['id_r']))\n",
    "\n",
    "sorted_match_tuples = []\n",
    "for tup in match_tuples:\n",
    "    s = tuple(sorted(tup))\n",
    "    sorted_match_tuples.append(s)\n",
    "\n",
    "all_match_candidates.drop(['id_l','id_r'],axis=1,inplace=True)\n",
    "all_match_candidates['id_tuples'] = sorted_match_tuples\n",
    "\n",
    "new_col_list = ['id_l','id_r']\n",
    "for n,col in enumerate(new_col_list):\n",
    "    all_match_candidates[col] = all_match_candidates['id_tuples'].apply(lambda location: location[n])\n",
    "    \n",
    "all_match_candidates.drop('id_tuples',axis=1,inplace=True)\n",
    "all_match_candidates.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"redundant id pairs removed --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"MERGING MATCH CANDIDATES WITH ORIGINAL DATA...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "right_df = df.copy()\n",
    "df.rename(columns={'id':'id_l','org_name':'l_org_name','org_address1':'l_address1','org_city':'l_city','org_state':'l_state','org_zip':'l_postal_code','clean_phone':'l_clean_phone','domain':'l_domain'}, inplace=True)\n",
    "right_df.rename(columns={'id':'id_r','org_name':'r_org_name','org_address1':'r_address1','org_city':'r_city','org_state':'r_state','org_zip':'r_postal_code','clean_phone':'r_clean_phone','domain':'r_domain'}, inplace=True)\n",
    "\n",
    "left_match_data = df[['id_l','l_org_name','l_city','l_state','l_postal_code','l_domain','l_clean_phone']].copy()\n",
    "right_match_data = right_df[['id_r','r_org_name','r_city','r_state','r_postal_code','r_domain','r_clean_phone']].copy()\n",
    "\n",
    "#making sure keys are str, results in blank df otherwise\n",
    "left_match_data.id_l = left_match_data.id_l.astype('str')\n",
    "right_match_data.id_r = right_match_data.id_r.astype('str')\n",
    "matched_records.id_l = matched_records.id_l.astype('str')\n",
    "matched_records.id_r = matched_records.id_r.astype('str')\n",
    "\n",
    "#merging matched_records df with original record data for ease of review\n",
    "l_conc = pd.merge(all_match_candidates, left_match_data, on='id_l')\n",
    "full_conc = pd.merge(l_conc, right_match_data, on='id_r')\n",
    "\n",
    "print(\"original data merged with matches --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING ORG NAME SIMULARITY...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on edit distance of org names\n",
    "def jaro_simularity(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return jaro_winkler(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '')\n",
    "def fuzz_partial(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.partial_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_sort(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_sort_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_set(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_set_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "\n",
    "full_conc['l_org_name'] = full_conc['l_org_name'].replace('', 'none', regex=True).astype('str')\n",
    "full_conc['r_org_name'] = full_conc['r_org_name'].replace('', 'none', regex=True).astype('str')\n",
    "\n",
    "jaro_time = time.time()\n",
    "full_conc['jaro_score'] = full_conc.apply(lambda x: jaro_simularity(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"jaro scores done --- %s seconds ---\" % (time.time() - jaro_time))\n",
    "partial_time = time.time()\n",
    "full_conc['fuzz_partial_score'] = full_conc.apply(lambda x: fuzz_partial(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz partial scores done --- %s seconds ---\" % (time.time() - partial_time))\n",
    "sort_time = time.time()\n",
    "full_conc['fuzz_sort_score'] = full_conc.apply(lambda x: fuzz_sort(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz sort scores done --- %s seconds ---\" % (time.time() - sort_time))\n",
    "set_time = time.time()\n",
    "full_conc['fuzz_set_score'] = full_conc.apply(lambda x: fuzz_set(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz set scores done --- %s seconds ---\" % (time.time() - set_time))\n",
    "print \"\"\n",
    "\n",
    "print(\"name simularity scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING NAME SEQUENCE UNIQUENESS...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "org_tokens = []    \n",
    "for word in df['l_org_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_tokens.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in right_df['r_org_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "org_flat_list = [item for sublist in org_tokens for item in sublist]\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "org_cnt = Counter()\n",
    "for token in org_flat_list:\n",
    "    org_cnt[token] += 1\n",
    "\n",
    "org_cnt_dict = dict(org_cnt) #convert to dictionary\n",
    "\n",
    "def sequence_uniqueness(seq):\n",
    "    return sum(1/org_cnt_dict[str.lower(t)]**0.5 for t in seq)\n",
    "\n",
    "def name_similarity(a, b):\n",
    "    if isinstance(a,basestring) is True and isinstance(b,basestring) is True:\n",
    "        if len(a) > 0 and len(b) > 0:\n",
    "            a_tokens = set(tokenize_name(a))\n",
    "            b_tokens = set(tokenize_name(b))\n",
    "            a_uniq = sequence_uniqueness(a_tokens)\n",
    "            b_uniq = sequence_uniqueness(b_tokens)\n",
    "\n",
    "            return sequence_uniqueness(a_tokens.intersection(b_tokens))/(a_uniq * b_uniq) ** 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "full_conc['uniq'] = full_conc.apply(lambda x: name_similarity(x.l_org_name, x.r_org_name), axis=1)\n",
    "\n",
    "print(\"name uniqueness scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR STATE CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def sanitize_state(state):\n",
    "    if isinstance(state,basestring) is True:\n",
    "        return ''.join(c for c in (state or '') if c in 'abcdefghijklmnopqrstuvwxyz')\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def state_match(state_a, state_b):\n",
    "    sanitized_state_a = str(sanitize_state(state_a))\n",
    "    sanitized_state_b = str(sanitize_state(state_b))\n",
    "\n",
    "    # if the value is too short, means it's fubar\n",
    "    if len(sanitized_state_a) < 2 or len(sanitized_state_b) < 2:\n",
    "        return 0\n",
    "    if state_a == state_b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "full_conc['state_match'] = full_conc.apply(lambda x: state_match(x.l_state, x.r_state), axis=1)\n",
    "\n",
    "print(\"state codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR POSTAL CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching postal code\n",
    "\n",
    "def sanitize_postal(postal):\n",
    "    if isinstance(postal, basestring) is True:\n",
    "        return ''.join(c for c in (postal or '') if c in '1234567890')\n",
    "    if isinstance(postal, float) is False:\n",
    "        return postal\n",
    "\n",
    "def postal_simularity(postal_a, postal_b):\n",
    "    sanitized_postal_a = str(sanitize_postal(postal_a))\n",
    "    sanitized_postal_b = str(sanitize_postal(postal_b))\n",
    "\n",
    "    # if the number is too short, means it's fubar\n",
    "    if len(sanitized_postal_a) < 5 or len(sanitized_postal_b) < 5:\n",
    "        return 0\n",
    "    if float(max(len(sub) for sub in find_common_subsequences(sanitized_postal_a, sanitized_postal_b))) / 5 >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "full_conc['zip_match'] = full_conc.apply(lambda x: postal_simularity(x.l_postal_code, x.r_postal_code), axis=1)\n",
    "    \n",
    "print(\"postal codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR WEB DOMAIN MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def domain_match(domain_a, domain_b):\n",
    "    if isinstance(domain_a, basestring) is True and isinstance(domain_b, basestring) is True:\n",
    "        if len(domain_a) > 0 and len(domain_b) > 0:\n",
    "            if domain_a == domain_b:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "full_conc['domain_match'] = full_conc.apply(lambda x: domain_match(x.domain, x.r_domain), axis=1)\n",
    "\n",
    "print(\"web domains checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR PHONE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching phone\n",
    "def phone_simularity(phone_a, phone_b):\n",
    "\n",
    "    if len(phone_a) < 10 or len(phone_b) < 10:\n",
    "        return 0\n",
    "    elif float(max(len(sub) for sub in find_common_subsequences(phone_a, phone_b))) / 10 >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "full_conc['phone_match'] = full_conc.apply(lambda x: phone_simularity(x.clean_phone, x.r_clean_phone), axis=1)\n",
    "    \n",
    "print(\"phones checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"COMPOSITE SCORING, PREDICTING MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#full_conc['overall_name_score'] = full_conc.jaro_score * name_weight \\\n",
    "#+ full_conc.fuzz_partial_score * name_weight \\\n",
    "#+ full_conc.fuzz_sort_score * name_weight \\\n",
    "#+ full_conc.fuzz_set_score * name_weight \\\n",
    "#+ full_conc.uniq * name_uniqueness_weight\n",
    "\n",
    "#calculate composite match score based on component scores and weights\n",
    "#full_conc['composite_match_score'] = full_conc.overall_name_score \\\n",
    "#+ full_conc.zip_match * zip_weight \\\n",
    "#+ full_conc.state_match * state_weight \\\n",
    "#+ full_conc.domain_match * domain_weight \\\n",
    "#+ full_conc.phone_match * phone_weight\n",
    "\n",
    "#use labeled matches to train logistic regression to predict matches\n",
    "training_data = pd.read_table('training_data3.txt')\n",
    "\n",
    "feature_cols = ['jaro_score',\n",
    "                'fuzz_partial_score',\n",
    "                'fuzz_sort_score',\n",
    "                'fuzz_set_score',\n",
    "                'uniq',\n",
    "                'state_match',\n",
    "                'zip_match'],\n",
    "                'domain_match',\n",
    "                'phone_match']\n",
    "\n",
    "# define X and y\n",
    "X = training_data[feature_cols]\n",
    "y = training_data['is_match']\n",
    "\n",
    "# logistic regression\n",
    "log = LogisticRegression()\n",
    "log.fit(X, y)\n",
    "y_pred_class = log.predict(full_conc[feature_cols])\n",
    "y_pred_proba = log.predict_proba(full_conc[feature_cols])[:,1]\n",
    "\n",
    "full_conc['match_pred'] = y_pred_class\n",
    "full_conc['pred_proba'] = y_pred_proba\n",
    "\n",
    "#we take any matches meeting either name match threshold or composite match threshold as matches for review\n",
    "#org_matches = full_conc[(full_conc.overall_name_score >= name_score_min) | (full_conc.composite_match_score >= composite_score_min)]\n",
    "org_matches = full_conc[full_conc.match_pred == 1]\n",
    "failed_matches_for_review = full_conc[(full_conc.match_pred == 0) & (full_conc.pred_proba > .4)]\n",
    "\n",
    "print(\"final matches isolated --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "print(\"TOTAL COMPUTE TIME --- %s seconds ---\" % (time.time() - overall_time))\n",
    "\n",
    "org_matches.sort_values(by='pred_proba', ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
