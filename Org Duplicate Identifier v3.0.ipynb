{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from jellyfish import jaro_winkler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from py_common_subseq import find_common_subsequences\n",
    "import numbers\n",
    "import time\n",
    "from collections import Counter \n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import shutil\n",
    "\n",
    "with io.open('all_recipients.txt', encoding='utf-8', errors='ignore') as source:\n",
    "    with io.open('all_recipients_utf.txt', mode='w', encoding='utf-8') as target:\n",
    "        shutil.copyfileobj(source,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING INITIAL DATAFRAMES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 1329580: expected 7 fields, saw 12\n",
      "Skipping line 1385100: expected 7 fields, saw 8\n",
      "\n",
      "C:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2718: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes loaded --- 20.8509998322 seconds ---\n",
      "\n",
      "PRE-PROCESSING...\n",
      "states, phones, domains normalized --- 15.4089999199 seconds ---\n",
      "\n",
      "TOKENIZING, IDENTIFYING CANDIDATE MATCH PAIRS...\n",
      "token dictionary created --- 93.5170001984 seconds ---\n",
      "tokens keyed to org records, now beginning chunking process to join left/right dfs --- 134.623999834 seconds ---\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "match candidates identified --- 26569.747 seconds ---\n",
      "REMOVING REDUNDANT DUPLICATE ID PAIRS...\n",
      "redundant id pairs removed --- 416.603999853 seconds ---\n",
      "\n",
      "MERGING MATCH CANDIDATES WITH ORIGINAL DATA...\n",
      "original data merged with matches --- 77.0720000267 seconds ---\n",
      "\n",
      "SCORING ORG NAME SIMULARITY...\n"
     ]
    }
   ],
   "source": [
    "overall_time = time.time()\n",
    "\n",
    "#define column names\n",
    "org_id = 'RecipientID'\n",
    "org_name = 'RecipientName'\n",
    "org_address1 = 'addressline1txt'\n",
    "org_city = 'citynm'\n",
    "org_state = 'stateabbreviationcd'\n",
    "org_zip = 'zip'\n",
    "#org_phone = 'phone'\n",
    "org_web = 'websiteaddresstxt'\n",
    "#org_acronym = 'acronym'\n",
    "#org_alt_name = 'alt_name'\n",
    "\n",
    "#set parameters\n",
    "chunk_size = 5000\n",
    "token_match_min = 3 # minimum number of matched tokens to be considered a match\n",
    "token_limiter = .995 # percent of non-single tokens to tokenize, where rare tokens are at the bottom and common at the top\n",
    "unique_token_freq_max = 5 #threshold <= to a token is considered \"unique\" and links to these tokens are counted double\n",
    "name_weight = .75 #note that this is really .75 * 4 because there are 4 org name simularity metrics\n",
    "name_uniqueness_weight = 1.5\n",
    "state_weight = 1\n",
    "zip_weight = 1\n",
    "phone_weight = 2\n",
    "domain_weight = 2\n",
    "name_score_min = 3\n",
    "composite_score_min = 4 #minimum composite match score to be considered a match\n",
    "\n",
    "start_time = time.time()\n",
    "print \"LOADING INITIAL DATAFRAMES...\"\n",
    "\n",
    "df = pd.read_table('all_recipients_utf.txt',keep_default_na=True,error_bad_lines=False)\n",
    "df.sort_values(['websiteaddresstxt','addressline1txt','citynm','stateabbreviationcd','zip'],inplace=True)\n",
    "df.drop_duplicates(subset='RecipientID',inplace=True)\n",
    "\n",
    "df.rename(columns={org_id:'id',org_name:'org_name',org_address1:'org_address1',org_city:'org_city',org_state:'org_state',org_zip:'org_zip',org_web:'org_web'\n",
    "                   #,org_phone:'org_phone',org_acronym:'org_acronym',org_alt_name:'org_alt_name'\n",
    "                  }, inplace=True)\n",
    "\n",
    "df = df.replace(np.nan, '', regex=True)\n",
    "\n",
    "df.org_name = df.org_name.astype('str')\n",
    "df.org_address1 = df.org_address1.astype('str')\n",
    "df.org_city = df.org_city.astype('str')\n",
    "df.org_state = df.org_state.astype('str')\n",
    "df.org_zip = df.org_zip.astype('str')\n",
    "#right_df.r_phone = right_df.r_phone.astype('str')\n",
    "df.org_web = df.org_web.astype('str')\n",
    "\n",
    "print(\"Dataframes loaded --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"PRE-PROCESSING...\"\n",
    "#normalize state codes\n",
    "state_lkup = pd.read_csv('state_lkup.csv',keep_default_na=False)\n",
    "\n",
    "from collections import defaultdict\n",
    "state_dict = defaultdict(list)\n",
    "for state, acronym in zip(state_lkup.state.values,state_lkup.acronym.values):\n",
    "    state_dict[state].append(acronym)\n",
    "\n",
    "df.org_state = df.org_state.replace(np.nan, '', regex=True).str.lower()\n",
    "df.org_state = df.org_state.replace(state_dict)\n",
    "\n",
    "#clean up non numeric characters in phones\n",
    "#clean_phones = []\n",
    "#df.org_phone = df.org_phone.astype('str')\n",
    "#df.org_phone = df.org_phone.replace(np.nan, '', regex=True)\n",
    "#for phone in df.org_phone:\n",
    "#    clean_phones.append(re.sub('[^0-9]','', phone))\n",
    "    \n",
    "#df['clean_phone'] = clean_phones\n",
    "\n",
    "#isolate domains from web URLs\n",
    "domains = []\n",
    "df.org_web = df.org_web.replace(np.nan, '', regex=True).str.lower()\n",
    "for web in df.org_web:\n",
    "    domains.append(web.split('//')[-1].split('/')[0].strip('www.'))\n",
    "df['domain'] = domains\n",
    "\n",
    "print(\"states, phones, domains normalized --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"TOKENIZING, IDENTIFYING CANDIDATE MATCH PAIRS...\"\n",
    "\n",
    "unique_token_columns = [ #tokens here circumvent the token_limiter which is cutting off the most commonly occuring tokens\n",
    "    'org_state', \n",
    "    'org_zip'#,\n",
    "    #'clean_phone'\n",
    "\n",
    "]\n",
    "\n",
    "delta_token_columns = [\n",
    "    #'org_acronym',\n",
    "    'org_name',\n",
    "    #'org_alt_name',\n",
    "    'org_address1',\n",
    "    'org_city',\n",
    "    'domain'\n",
    "]\n",
    "\n",
    "# lowercase the name and split on spaces, remove non-alphanumeric chars\n",
    "def tokenize_name(name):\n",
    "    if isinstance(name, basestring) is True:\n",
    "        clean_name = ''.join(c if c.isalnum() else ' ' for c in name)\n",
    "        return clean_name.lower().split()\n",
    "    else:\n",
    "        return name\n",
    "    \n",
    "unique_tokens = []    \n",
    "for col in unique_token_columns:\n",
    "    for word in df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            unique_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "unique_flat_list = [item for sublist in unique_tokens for item in sublist]\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "u_cnt = Counter()\n",
    "for token in unique_flat_list:\n",
    "    u_cnt[token] += 1\n",
    "\n",
    "u_cnt_dict = dict(u_cnt) #convert to dictionary\n",
    "\n",
    "unique_tokens_df = pd.DataFrame(u_cnt_dict.items(), columns=['token', 'count'])\n",
    "unique_tokens_df = unique_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "#consider waiting to do the count flag thing later, instead use some type of \"token type\" code\n",
    "unique_token_flag = []\n",
    "for index, value in enumerate(unique_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        unique_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    else:\n",
    "        unique_token_flag.append(1)\n",
    "\n",
    "unique_tokens_df['flag'] = unique_token_flag        \n",
    "\n",
    "all_other_words = []\n",
    "for col in delta_token_columns:\n",
    "    for word in df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            all_other_words.append(tokenize_name(str(word)))\n",
    "            \n",
    "flat_list = [item for sublist in all_other_words for item in sublist] #flatten list so it can be counted\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "cnt = Counter()\n",
    "for token in flat_list:\n",
    "    cnt[token] += 1\n",
    "\n",
    "cnt_dict = dict(cnt) #convert to dictionary\n",
    "\n",
    "main_tokens_df = pd.DataFrame(cnt_dict.items(), columns=['token', 'count'])\n",
    "main_tokens_df = main_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "#wait to do count until joined with unique tokens?\n",
    "main_token_flag = []\n",
    "for index, value in enumerate(main_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        main_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    elif index < int(main_tokens_df.shape[0] * token_limiter): #important line, we are cutting the top x% of frequently occuring tokens\n",
    "        main_token_flag.append(1)\n",
    "    else:\n",
    "        main_token_flag.append(0)  #for the most common tokens, we exclude\n",
    "\n",
    "main_tokens_df['flag'] = main_token_flag\n",
    "\n",
    "all_tokens = pd.concat([unique_tokens_df, main_tokens_df])\n",
    "\n",
    "all_tokens.drop('count',axis=1,inplace=True)\n",
    "all_tokens['flag'] = all_tokens.flag.astype(int) #converting flags to int\n",
    "tokens_dct = all_tokens.to_dict('split') #converting tokens_df to dictionary\n",
    "tokens_dct=dict(tokens_dct['data']) #honestly can't remember why this works, something to do with conversion to dictionary\n",
    "\n",
    "#preparing token_ids which will be used for joining left and right dfs\n",
    "all_tokens.sort_values(by='flag',ascending=False,inplace=True)\n",
    "all_tokens.sort_values(by='token',inplace=True)\n",
    "all_tokens.drop_duplicates(subset='token',keep='first',inplace=True)\n",
    "token_ids = all_tokens.index.get_level_values(0)\n",
    "all_tokens['token_id'] = token_ids\n",
    "\n",
    "all_tokens.drop('flag',axis=1,inplace=True)\n",
    "all_tokens['token_id'] = all_tokens.token_id.astype(int)\n",
    "token_id_dct = all_tokens.to_dict('split')\n",
    "tokens_id_dct=dict(token_id_dct['data'])\n",
    "\n",
    "print(\"token dictionary created --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "vocabulary = np.array([w for w, c in tokens_dct.items() if c ==1]) #this works even without the ==1 and I don't know why\n",
    "cv = CountVectorizer( vocabulary=vocabulary)\n",
    "\n",
    "#now we are ready to tokenize left and right dataframes\n",
    "all_cols = unique_token_columns + delta_token_columns\n",
    "\n",
    "left_frame_list = []\n",
    "for colname in all_cols:\n",
    "    tokenmapping = cv.fit_transform(df[colname])\n",
    "    df_row, token_id = tokenmapping.nonzero()\n",
    "\n",
    "    left_frame_list.append(pd.DataFrame(np.vstack([vocabulary[token_id], df['id'].values[df_row]]).T, columns = ['token', 'id']))\n",
    "\n",
    "left_keyed = pd.concat(left_frame_list)\n",
    "left_keyed.drop_duplicates()#inplace=True)\n",
    "#removing duplicates inplace was giving me a very strange issue where a small percentage of token_ids would be excluded from the left_keyed index\n",
    "\n",
    "#append token_id to token as this will be more efficient to join with\n",
    "left_token_ids = []\n",
    "for token in left_keyed.token:\n",
    "    left_token_ids.append(tokens_id_dct[token])\n",
    "\n",
    "left_keyed['token_id'] = left_token_ids\n",
    "left_keyed.sort_values(by='token_id',inplace=True)\n",
    "left_keyed.set_index('token_id',inplace=True)\n",
    "left_keyed.drop('token',axis=1,inplace=True)\n",
    "\n",
    "left_keyed.sort_values(by='id',inplace=True)\n",
    "\n",
    "#right is just a copy of left\n",
    "right_keyed = left_keyed.copy()\n",
    "\n",
    "aggregations = {\n",
    "    'id_l': 'count'\n",
    "}\n",
    "\n",
    "left_keyed.to_csv('left_keyed.csv')\n",
    "\n",
    "del left_keyed\n",
    "gc.collect()\n",
    "left_keyed = pd.DataFrame()\n",
    "\n",
    "print(\"tokens keyed to org records, now beginning chunking process to join left/right dfs --- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "\n",
    "bonus_point_tokens = []\n",
    "for token in main_tokens_df[(main_tokens_df['count'] > 1) & (main_tokens_df['count']<=unique_token_freq_max)].token:\n",
    "    bonus_point_tokens.append(tokens_id_dct[token])\n",
    "\n",
    "n=0\n",
    "match_dfs = []\n",
    "for chunk in pd.read_csv('left_keyed.csv',keep_default_na=False,chunksize=chunk_size,index_col='token_id'):\n",
    "    \n",
    "    n=n+1\n",
    "    print n\n",
    "    joined = chunk.join(right_keyed, how='inner',lsuffix='_l',rsuffix='_r')\n",
    "    joined['id_l']=joined.id_l.astype('str')\n",
    "    #double-counting unique token matches\n",
    "\n",
    "    intersection_bonus_tokens = set(bonus_point_tokens).intersection(set(list(joined.index)))    \n",
    "\n",
    "    bonus_token_joins = []\n",
    "    for token_id in intersection_bonus_tokens:\n",
    "        bonus_token_joins.append(joined.loc[token_id])\n",
    "\n",
    "    bonus_joins = pd.concat(bonus_token_joins)\n",
    "    bonus_joins_cols = bonus_joins[['id_l','id_r']].copy()\n",
    "    bonus_joins_cols.dropna(inplace=True)\n",
    "    joined_bonus = pd.concat([joined,bonus_joins_cols])\n",
    "    \n",
    "    keys_grouped = joined_bonus.groupby(by=['id_l', 'id_r']).agg(aggregations)\n",
    "    keys_grouped.rename(columns={'id_l':'id_l count'}, inplace=True)\n",
    "    matched_records = keys_grouped[keys_grouped['id_l count'] >= token_match_min]\n",
    "    matched_records.reset_index(inplace=True)\n",
    "    duplicate_candidates = matched_records[matched_records['id_l'] <> matched_records['id_r']]\n",
    "    match_dfs.append(duplicate_candidates)\n",
    "    del [[joined,bonus_joins,joined_bonus,keys_grouped,matched_records,duplicate_candidates]]\n",
    "    gc.collect()\n",
    "    joined = pd.DataFrame() #trying to preserve memory by returning variables to 0 mem usage\n",
    "    bonus_joins = pd.DataFrame()\n",
    "    joined_bonus = pd.DataFrame()\n",
    "    keys_grouped = pd.DataFrame()\n",
    "    matched_records = pd.DataFrame()\n",
    "    duplicate_candidates = pd.DataFrame()\n",
    "    \n",
    "all_match_candidates = pd.concat(match_dfs)\n",
    "del match_dfs\n",
    "gc.collect()\n",
    "match_dfs = pd.DataFrame()\n",
    "\n",
    "all_match_candidates.reset_index(inplace=True)\n",
    "all_match_candidates.drop(labels='index',axis=1,inplace=True)\n",
    "all_match_candidates.id_l.astype('str')\n",
    "\n",
    "print(\"match candidates identified --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "print \"REMOVING REDUNDANT DUPLICATE ID PAIRS...\"\n",
    "\n",
    "match_tuples = list(zip(all_match_candidates['id_l'], all_match_candidates['id_r']))\n",
    "\n",
    "sorted_match_tuples = []\n",
    "for tup in match_tuples:\n",
    "    s = tuple(sorted(tup))\n",
    "    sorted_match_tuples.append(s)\n",
    "\n",
    "all_match_candidates.drop(['id_l','id_r'],axis=1,inplace=True)\n",
    "all_match_candidates['id_tuples'] = sorted_match_tuples\n",
    "\n",
    "new_col_list = ['id_l','id_r']\n",
    "for n,col in enumerate(new_col_list):\n",
    "    all_match_candidates[col] = all_match_candidates['id_tuples'].apply(lambda location: location[n])\n",
    "    \n",
    "all_match_candidates.drop('id_tuples',axis=1,inplace=True)\n",
    "all_match_candidates.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"redundant id pairs removed --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"MERGING MATCH CANDIDATES WITH ORIGINAL DATA...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "right_df = df.copy()\n",
    "df.rename(columns={'id':'id_l','org_name':'l_org_name','org_address1':'l_address1','org_city':'l_city','org_state':'l_state','org_zip':'l_postal_code'\n",
    "                   #,'clean_phone':'l_clean_phone'\n",
    "                   ,'domain':'l_domain'}, inplace=True)\n",
    "right_df.rename(columns={'id':'id_r','org_name':'r_org_name','org_address1':'r_address1','org_city':'r_city','org_state':'r_state','org_zip':'r_postal_code'\n",
    "                         #,'clean_phone':'r_clean_phone'\n",
    "                         ,'domain':'r_domain'}, inplace=True)\n",
    "\n",
    "left_match_data = df[['id_l','l_org_name','l_city','l_state','l_postal_code','l_domain'\n",
    "                      #,'l_clean_phone'\n",
    "                     ]].copy()\n",
    "right_match_data = right_df[['id_r','r_org_name','r_city','r_state','r_postal_code','r_domain'\n",
    "                             #,'r_clean_phone'\n",
    "                            ]].copy()\n",
    "\n",
    "#making sure keys are str, results in blank df otherwise\n",
    "left_match_data.id_l = left_match_data.id_l.astype('str')\n",
    "right_match_data.id_r = right_match_data.id_r.astype('str')\n",
    "all_match_candidates.id_l = all_match_candidates.id_l.astype('str')\n",
    "all_match_candidates.id_r = all_match_candidates.id_r.astype('str')\n",
    "\n",
    "#merging matched_records df with original record data for ease of review\n",
    "l_conc = pd.merge(all_match_candidates, left_match_data, on='id_l')\n",
    "full_conc = pd.merge(l_conc, right_match_data, on='id_r')\n",
    "\n",
    "print(\"original data merged with matches --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING ORG NAME SIMULARITY...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on edit distance of org names\n",
    "def jaro_simularity(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return jaro_winkler(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '')\n",
    "def fuzz_partial(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.partial_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_sort(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_sort_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_set(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_set_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "\n",
    "full_conc['l_org_name'] = full_conc['l_org_name'].replace('', 'none', regex=True).astype('str')\n",
    "full_conc['r_org_name'] = full_conc['r_org_name'].replace('', 'none', regex=True).astype('str')\n",
    "\n",
    "jaro_time = time.time()\n",
    "full_conc['jaro_score'] = full_conc.apply(lambda x: jaro_simularity(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"jaro scores done --- %s seconds ---\" % (time.time() - jaro_time))\n",
    "partial_time = time.time()\n",
    "full_conc['fuzz_partial_score'] = full_conc.apply(lambda x: fuzz_partial(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz partial scores done --- %s seconds ---\" % (time.time() - partial_time))\n",
    "sort_time = time.time()\n",
    "full_conc['fuzz_sort_score'] = full_conc.apply(lambda x: fuzz_sort(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz sort scores done --- %s seconds ---\" % (time.time() - sort_time))\n",
    "set_time = time.time()\n",
    "full_conc['fuzz_set_score'] = full_conc.apply(lambda x: fuzz_set(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz set scores done --- %s seconds ---\" % (time.time() - set_time))\n",
    "print \"\"\n",
    "\n",
    "print(\"name simularity scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING NAME SEQUENCE UNIQUENESS...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "org_tokens = []    \n",
    "for word in df['l_org_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "org_flat_list = [item for sublist in org_tokens for item in sublist]\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "org_cnt = Counter()\n",
    "for token in org_flat_list:\n",
    "    org_cnt[token] += 1\n",
    "\n",
    "org_cnt_dict = dict(org_cnt) #convert to dictionary\n",
    "\n",
    "def sequence_uniqueness(seq):\n",
    "    return sum(1/org_cnt_dict[str.lower(t)]**0.5 for t in seq)\n",
    "\n",
    "def name_similarity(a, b):\n",
    "    if isinstance(a,basestring) is True and isinstance(b,basestring) is True:\n",
    "        if len(a) > 1 and len(b) > 1:\n",
    "            a_tokens = set(tokenize_name(a))\n",
    "            b_tokens = set(tokenize_name(b))\n",
    "            a_uniq = sequence_uniqueness(a_tokens)\n",
    "            b_uniq = sequence_uniqueness(b_tokens)\n",
    "\n",
    "            return sequence_uniqueness(a_tokens.intersection(b_tokens))/(a_uniq * b_uniq) ** 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "full_conc['uniq'] = full_conc.apply(lambda x: name_similarity(x.l_org_name, x.r_org_name), axis=1)\n",
    "\n",
    "print(\"name uniqueness scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR STATE CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def sanitize_state(state):\n",
    "    if isinstance(state,basestring) is True:\n",
    "        return ''.join(c for c in (state or '') if c in 'abcdefghijklmnopqrstuvwxyz')\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def state_match(state_a, state_b):\n",
    "    sanitized_state_a = str(sanitize_state(state_a))\n",
    "    sanitized_state_b = str(sanitize_state(state_b))\n",
    "\n",
    "    # if the value is too short, means it's fubar\n",
    "    if len(sanitized_state_a) < 2 or len(sanitized_state_b) < 2:\n",
    "        return 0\n",
    "    if state_a == state_b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "full_conc['state_match'] = full_conc.apply(lambda x: state_match(x.l_state, x.r_state), axis=1)\n",
    "\n",
    "print(\"state codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR POSTAL CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching postal code\n",
    "\n",
    "def sanitize_postal(postal):\n",
    "    if isinstance(postal, basestring) is True:\n",
    "        return ''.join(c for c in (postal or '') if c in '1234567890')\n",
    "    if isinstance(postal, float) is False:\n",
    "        return postal\n",
    "\n",
    "def postal_simularity(postal_a, postal_b):\n",
    "    sanitized_postal_a = str(sanitize_postal(postal_a))\n",
    "    sanitized_postal_b = str(sanitize_postal(postal_b))\n",
    "\n",
    "    # if the number is too short, means it's fubar\n",
    "    if len(sanitized_postal_a) < 5 or len(sanitized_postal_b) < 5:\n",
    "        return 0\n",
    "    if float(max(len(sub) for sub in find_common_subsequences(sanitized_postal_a, sanitized_postal_b))) / 5 >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "full_conc['zip_match'] = full_conc.apply(lambda x: postal_simularity(x.l_postal_code, x.r_postal_code), axis=1)\n",
    "    \n",
    "print(\"postal codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR WEB DOMAIN MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def domain_match(domain_a, domain_b):\n",
    "    if isinstance(domain_a, basestring) is True and isinstance(domain_b, basestring) is True:\n",
    "        if len(domain_a) > 4 and len(domain_b) > 4:\n",
    "            if domain_a == domain_b:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "full_conc['domain_match'] = full_conc.apply(lambda x: domain_match(x.l_domain, x.r_domain), axis=1)\n",
    "\n",
    "print(\"web domains checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "#start_time = time.time()\n",
    "#print \"CHECKING FOR PHONE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching phone\n",
    "#def phone_simularity(phone_a, phone_b):\n",
    "\n",
    "#    if len(phone_a) < 10 or len(phone_b) < 10:\n",
    "#        return 0\n",
    "#    elif float(max(len(sub) for sub in find_common_subsequences(phone_a, phone_b))) / 10 >= 1:\n",
    "#        return 1\n",
    "#    else:\n",
    "#        return 0\n",
    "    \n",
    "#full_conc['phone_match'] = full_conc.apply(lambda x: phone_simularity(x.clean_phone, x.r_clean_phone), axis=1)\n",
    "    \n",
    "#print(\"phones checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "#print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"COMPOSITE SCORING, PREDICTING MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#full_conc['overall_name_score'] = full_conc.jaro_score * name_weight \\\n",
    "#+ full_conc.fuzz_partial_score * name_weight \\\n",
    "#+ full_conc.fuzz_sort_score * name_weight \\\n",
    "#+ full_conc.fuzz_set_score * name_weight \\\n",
    "#+ full_conc.uniq * name_uniqueness_weight\n",
    "\n",
    "#calculate composite match score based on component scores and weights\n",
    "#full_conc['composite_match_score'] = full_conc.overall_name_score \\\n",
    "#+ full_conc.zip_match * zip_weight \\\n",
    "#+ full_conc.state_match * state_weight \\\n",
    "#+ full_conc.domain_match * domain_weight \\\n",
    "#+ full_conc.phone_match * phone_weight\n",
    "\n",
    "#use labeled matches to train logistic regression to predict matches\n",
    "training_data = pd.read_table('training_data3.txt')\n",
    "\n",
    "feature_cols = ['jaro_score',\n",
    "                'fuzz_partial_score',\n",
    "                'fuzz_sort_score',\n",
    "                'fuzz_set_score',\n",
    "                'uniq',\n",
    "                'state_match',\n",
    "                'zip_match',\n",
    "                'domain_match']#,\n",
    "                #'phone_match'\n",
    "        \n",
    "\n",
    "# define X and y\n",
    "X = training_data[feature_cols]\n",
    "y = training_data['is_match']\n",
    "\n",
    "# logistic regression\n",
    "log = LogisticRegression()\n",
    "log.fit(X, y)\n",
    "y_pred_class = log.predict(full_conc[feature_cols])\n",
    "y_pred_proba = log.predict_proba(full_conc[feature_cols])[:,1]\n",
    "\n",
    "full_conc['match_pred'] = y_pred_class\n",
    "full_conc['pred_proba'] = y_pred_proba\n",
    "\n",
    "#we take any matches meeting either name match threshold or composite match threshold as matches for review\n",
    "#org_matches = full_conc[(full_conc.overall_name_score >= name_score_min) | (full_conc.composite_match_score >= composite_score_min)]\n",
    "org_matches = full_conc[full_conc.match_pred == 1]\n",
    "failed_matches_for_review = full_conc[(full_conc.match_pred == 0) & (full_conc.pred_proba > .4)]\n",
    "\n",
    "print(\"final matches isolated --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "print(\"TOTAL COMPUTE TIME --- %s seconds ---\" % (time.time() - overall_time))\n",
    "\n",
    "org_matches.sort_values(by='pred_proba', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_conc.to_csv('full_conc for grant recipients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2718: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id_l count</th>\n",
       "      <th>id_l</th>\n",
       "      <th>id_r</th>\n",
       "      <th>l_org_name</th>\n",
       "      <th>l_city</th>\n",
       "      <th>l_state</th>\n",
       "      <th>l_postal_code</th>\n",
       "      <th>l_domain</th>\n",
       "      <th>r_org_name</th>\n",
       "      <th>r_city</th>\n",
       "      <th>r_state</th>\n",
       "      <th>r_postal_code</th>\n",
       "      <th>r_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>100949</td>\n",
       "      <td>ONE HEART CHURCH</td>\n",
       "      <td>STOCKBRIDGE</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TLC Pet Rescue</td>\n",
       "      <td>Stockbridge</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>106556</td>\n",
       "      <td>ONE HEART CHURCH</td>\n",
       "      <td>STOCKBRIDGE</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMBASSADOR LIFE CENTER INC</td>\n",
       "      <td>Stockbridge</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100949</td>\n",
       "      <td>106556</td>\n",
       "      <td>TLC Pet Rescue</td>\n",
       "      <td>Stockbridge</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMBASSADOR LIFE CENTER INC</td>\n",
       "      <td>Stockbridge</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>121141</td>\n",
       "      <td>ONE HEART CHURCH</td>\n",
       "      <td>STOCKBRIDGE</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HANDS OF HOPE CLINIC INC</td>\n",
       "      <td>STOCKBRIDGE</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>handsofhopeclinic.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>100949</td>\n",
       "      <td>121141</td>\n",
       "      <td>TLC Pet Rescue</td>\n",
       "      <td>Stockbridge</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HANDS OF HOPE CLINIC INC</td>\n",
       "      <td>STOCKBRIDGE</td>\n",
       "      <td>ga</td>\n",
       "      <td>30281.0</td>\n",
       "      <td>handsofhopeclinic.org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id_l count    id_l    id_r        l_org_name       l_city  \\\n",
       "0           0           3      10  100949  ONE HEART CHURCH  STOCKBRIDGE   \n",
       "1           1           3      10  106556  ONE HEART CHURCH  STOCKBRIDGE   \n",
       "2           2           3  100949  106556    TLC Pet Rescue  Stockbridge   \n",
       "3           3           3      10  121141  ONE HEART CHURCH  STOCKBRIDGE   \n",
       "4           4           3  100949  121141    TLC Pet Rescue  Stockbridge   \n",
       "\n",
       "  l_state  l_postal_code l_domain                  r_org_name       r_city  \\\n",
       "0      ga        30281.0      NaN              TLC Pet Rescue  Stockbridge   \n",
       "1      ga        30281.0      NaN  AMBASSADOR LIFE CENTER INC  Stockbridge   \n",
       "2      ga        30281.0      NaN  AMBASSADOR LIFE CENTER INC  Stockbridge   \n",
       "3      ga        30281.0      NaN    HANDS OF HOPE CLINIC INC  STOCKBRIDGE   \n",
       "4      ga        30281.0      NaN    HANDS OF HOPE CLINIC INC  STOCKBRIDGE   \n",
       "\n",
       "  r_state  r_postal_code               r_domain  \n",
       "0      ga        30281.0                    NaN  \n",
       "1      ga        30281.0                    NaN  \n",
       "2      ga        30281.0                    NaN  \n",
       "3      ga        30281.0  handsofhopeclinic.org  \n",
       "4      ga        30281.0  handsofhopeclinic.org  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_conc = pd.read_csv('full_conc for grant recipients.csv')\n",
    "full_conc.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scoring match candidates based on edit distance of org names\n",
    "def jaro_simularity(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return jaro_winkler(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '')\n",
    "def fuzz_partial(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.partial_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_sort(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_sort_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_set(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_set_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "\n",
    "full_conc['l_org_name'] = full_conc['l_org_name'].replace('', 'none', regex=True).astype('str')\n",
    "full_conc['r_org_name'] = full_conc['r_org_name'].replace('', 'none', regex=True).astype('str')\n",
    "\n",
    "set_time = time.time()\n",
    "full_conc['fuzz_set_score'] = full_conc.apply(lambda x: fuzz_set(x.l_org_name, x.r_org_name), axis=1)\n",
    "full_conc.to_csv('full_conc all name scores done.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2718: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "full_conc = pd.read_csv('full_conc grant recipients up to fuzz_set_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecipientID</th>\n",
       "      <th>RecipientName</th>\n",
       "      <th>addressline1txt</th>\n",
       "      <th>citynm</th>\n",
       "      <th>stateabbreviationcd</th>\n",
       "      <th>zip</th>\n",
       "      <th>websiteaddresstxt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ST JAMES EPISCOPAL CHURCH</td>\n",
       "      <td>1 NORTH CHARLES STREET</td>\n",
       "      <td>BALTIMORE</td>\n",
       "      <td>MD</td>\n",
       "      <td>21201.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ST JAMES EPISCOPAL CHURCH</td>\n",
       "      <td>1001 DAVIS STREET</td>\n",
       "      <td>DUBUQUE</td>\n",
       "      <td>IA</td>\n",
       "      <td>52001.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ST JAMES EPISCOPAL CHURCH</td>\n",
       "      <td>1019 S COLLEGE STREET</td>\n",
       "      <td>GEORGETOWN</td>\n",
       "      <td>TX</td>\n",
       "      <td>78626.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ST JAMES EPISCOPAL CHURCH</td>\n",
       "      <td>10345 W Pico Boulevard</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>CA</td>\n",
       "      <td>90064.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ST JAMES EPISCOPAL CHURCH</td>\n",
       "      <td>1103 40TH STREET</td>\n",
       "      <td>ROCK ISLAND</td>\n",
       "      <td>IL</td>\n",
       "      <td>61201.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecipientID              RecipientName         addressline1txt  \\\n",
       "0            1  ST JAMES EPISCOPAL CHURCH  1 NORTH CHARLES STREET   \n",
       "1            1  ST JAMES EPISCOPAL CHURCH       1001 DAVIS STREET   \n",
       "2            1  ST JAMES EPISCOPAL CHURCH   1019 S COLLEGE STREET   \n",
       "3            1  ST JAMES EPISCOPAL CHURCH  10345 W Pico Boulevard   \n",
       "4            1  ST JAMES EPISCOPAL CHURCH        1103 40TH STREET   \n",
       "\n",
       "        citynm stateabbreviationcd      zip websiteaddresstxt  \n",
       "0    BALTIMORE                  MD  21201.0               NaN  \n",
       "1      DUBUQUE                  IA  52001.0               NaN  \n",
       "2   GEORGETOWN                  TX  78626.0               NaN  \n",
       "3  Los Angeles                  CA  90064.0               NaN  \n",
       "4  ROCK ISLAND                  IL  61201.0               NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 1329580: expected 7 fields, saw 12\n",
      "Skipping line 1385100: expected 7 fields, saw 8\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORING NAME SEQUENCE UNIQUENESS...\n",
      "name uniqueness scored --- 1801.4230001 seconds ---\n",
      "\n",
      "CHECKING FOR STATE CODE MATCHES...\n",
      "state codes checked --- 1193.46700001 seconds ---\n",
      "\n",
      "CHECKING FOR POSTAL CODE MATCHES...\n",
      "postal codes checked --- 1091.80100012 seconds ---\n",
      "\n",
      "CHECKING FOR WEB DOMAIN MATCHES...\n",
      "web domains checked --- 1064.63000011 seconds ---\n",
      "\n",
      "COMPOSITE SCORING, PREDICTING MATCHES...\n",
      "final matches isolated --- 23.4989998341 seconds ---\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'overall_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f71e8318f5a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TOTAL COMPUTE TIME --- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moverall_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[0morg_matches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pred_proba'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'overall_time' is not defined"
     ]
    }
   ],
   "source": [
    "full_conc = pd.read_csv('full_conc reduced to 1 percent proba after name scores.csv')\n",
    "df = pd.read_table('all_recipients_utf.txt',keep_default_na=True,error_bad_lines=False)\n",
    "\n",
    "def tokenize_name(name):\n",
    "    if isinstance(name, basestring) is True:\n",
    "        clean_name = ''.join(c if c.isalnum() else ' ' for c in name)\n",
    "        return clean_name.lower().split()\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING NAME SEQUENCE UNIQUENESS...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "org_tokens = []    \n",
    "for word in df['RecipientName']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "org_flat_list = [item for sublist in org_tokens for item in sublist]\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "org_cnt = Counter()\n",
    "for token in org_flat_list:\n",
    "    org_cnt[token] += 1\n",
    "\n",
    "org_cnt_dict = dict(org_cnt) #convert to dictionary\n",
    "\n",
    "def sequence_uniqueness(seq):\n",
    "    return sum(1/org_cnt_dict[str.lower(t)]**0.5 for t in seq)\n",
    "\n",
    "def name_similarity(a, b):\n",
    "    if isinstance(a,basestring) is True and isinstance(b,basestring) is True:\n",
    "        if len(a) > 1 and len(b) > 1:\n",
    "            a_tokens = set(tokenize_name(a))\n",
    "            b_tokens = set(tokenize_name(b))\n",
    "            a_uniq = sequence_uniqueness(a_tokens)\n",
    "            b_uniq = sequence_uniqueness(b_tokens)\n",
    "\n",
    "            return sequence_uniqueness(a_tokens.intersection(b_tokens))/(a_uniq * b_uniq) ** 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "full_conc['uniq'] = full_conc.apply(lambda x: name_similarity(x.l_org_name, x.r_org_name), axis=1)\n",
    "\n",
    "print(\"name uniqueness scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR STATE CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def sanitize_state(state):\n",
    "    if isinstance(state,basestring) is True:\n",
    "        return ''.join(c for c in (state or '') if c in 'abcdefghijklmnopqrstuvwxyz')\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def state_match(state_a, state_b):\n",
    "    sanitized_state_a = str(sanitize_state(state_a))\n",
    "    sanitized_state_b = str(sanitize_state(state_b))\n",
    "\n",
    "    # if the value is too short, means it's fubar\n",
    "    if len(sanitized_state_a) < 2 or len(sanitized_state_b) < 2:\n",
    "        return 0\n",
    "    if state_a == state_b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "full_conc['state_match'] = full_conc.apply(lambda x: state_match(x.l_state, x.r_state), axis=1)\n",
    "\n",
    "print(\"state codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR POSTAL CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching postal code\n",
    "\n",
    "def sanitize_postal(postal):\n",
    "    if isinstance(postal, basestring) is True:\n",
    "        return ''.join(c for c in (postal or '') if c in '1234567890')\n",
    "    if isinstance(postal, float) is False:\n",
    "        return postal\n",
    "\n",
    "def postal_simularity(postal_a, postal_b):\n",
    "    sanitized_postal_a = str(sanitize_postal(postal_a))\n",
    "    sanitized_postal_b = str(sanitize_postal(postal_b))\n",
    "\n",
    "    # if the number is too short, means it's fubar\n",
    "    if len(sanitized_postal_a) < 5 or len(sanitized_postal_b) < 5:\n",
    "        return 0\n",
    "    if float(max(len(sub) for sub in find_common_subsequences(sanitized_postal_a, sanitized_postal_b))) / 5 >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "full_conc['zip_match'] = full_conc.apply(lambda x: postal_simularity(x.l_postal_code, x.r_postal_code), axis=1)\n",
    "    \n",
    "print(\"postal codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR WEB DOMAIN MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def domain_match(domain_a, domain_b):\n",
    "    if isinstance(domain_a, basestring) is True and isinstance(domain_b, basestring) is True:\n",
    "        if len(domain_a) > 4 and len(domain_b) > 4:\n",
    "            if domain_a == domain_b:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "full_conc['domain_match'] = full_conc.apply(lambda x: domain_match(x.l_domain, x.r_domain), axis=1)\n",
    "\n",
    "print(\"web domains checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "#start_time = time.time()\n",
    "#print \"CHECKING FOR PHONE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching phone\n",
    "#def phone_simularity(phone_a, phone_b):\n",
    "\n",
    "#    if len(phone_a) < 10 or len(phone_b) < 10:\n",
    "#        return 0\n",
    "#    elif float(max(len(sub) for sub in find_common_subsequences(phone_a, phone_b))) / 10 >= 1:\n",
    "#        return 1\n",
    "#    else:\n",
    "#        return 0\n",
    "    \n",
    "#full_conc['phone_match'] = full_conc.apply(lambda x: phone_simularity(x.clean_phone, x.r_clean_phone), axis=1)\n",
    "    \n",
    "#print(\"phones checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "#print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"COMPOSITE SCORING, PREDICTING MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#full_conc['overall_name_score'] = full_conc.jaro_score * name_weight \\\n",
    "#+ full_conc.fuzz_partial_score * name_weight \\\n",
    "#+ full_conc.fuzz_sort_score * name_weight \\\n",
    "#+ full_conc.fuzz_set_score * name_weight \\\n",
    "#+ full_conc.uniq * name_uniqueness_weight\n",
    "\n",
    "#calculate composite match score based on component scores and weights\n",
    "#full_conc['composite_match_score'] = full_conc.overall_name_score \\\n",
    "#+ full_conc.zip_match * zip_weight \\\n",
    "#+ full_conc.state_match * state_weight \\\n",
    "#+ full_conc.domain_match * domain_weight \\\n",
    "#+ full_conc.phone_match * phone_weight\n",
    "\n",
    "#use labeled matches to train logistic regression to predict matches\n",
    "training_data = pd.read_table('training_data3.txt')\n",
    "\n",
    "feature_cols = ['id_l count',\n",
    "                'jaro_score',\n",
    "                'fuzz_partial_score',\n",
    "                'fuzz_sort_score',\n",
    "                'fuzz_set_score',\n",
    "                'uniq',\n",
    "                'state_match',\n",
    "                'zip_match',\n",
    "                'domain_match']#,\n",
    "                #'phone_match'\n",
    "        \n",
    "\n",
    "# define X and y\n",
    "X = training_data[feature_cols]\n",
    "y = training_data['is_match']\n",
    "\n",
    "# logistic regression\n",
    "log = LogisticRegression()\n",
    "log.fit(X, y)\n",
    "y_pred_class = log.predict(full_conc[feature_cols])\n",
    "y_pred_proba = log.predict_proba(full_conc[feature_cols])[:,1]\n",
    "\n",
    "full_conc['match_pred'] = y_pred_class\n",
    "full_conc['pred_proba'] = y_pred_proba\n",
    "\n",
    "#we take any matches meeting either name match threshold or composite match threshold as matches for review\n",
    "#org_matches = full_conc[(full_conc.overall_name_score >= name_score_min) | (full_conc.composite_match_score >= composite_score_min)]\n",
    "org_matches = full_conc[full_conc.match_pred == 1]\n",
    "failed_matches_for_review = full_conc[(full_conc.match_pred == 0) & (full_conc.pred_proba > .4)]\n",
    "\n",
    "print(\"final matches isolated --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "print(\"TOTAL COMPUTE TIME --- %s seconds ---\" % (time.time() - overall_time))\n",
    "\n",
    "org_matches.sort_values(by='pred_proba', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104250, 27)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54141, 27)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "org_matches.to_csv('grant recipient dups4 excluding domain, including l_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_table('training_data3.txt')\n",
    "\n",
    "feature_cols = ['id_l count',\n",
    "    'jaro_score',\n",
    "                'fuzz_partial_score',\n",
    "                'fuzz_sort_score',\n",
    "                'fuzz_set_score',\n",
    "                'uniq',\n",
    "                'state_match',\n",
    "                'zip_match']#,\n",
    "                #'domain_match']#,\n",
    "                #'phone_match'\n",
    "        \n",
    "\n",
    "# define X and y\n",
    "X = training_data[feature_cols]\n",
    "y = training_data['is_match']\n",
    "\n",
    "# logistic regression\n",
    "log = LogisticRegression()\n",
    "log.fit(X, y)\n",
    "y_pred_class = log.predict(full_conc[feature_cols])\n",
    "y_pred_proba = log.predict_proba(full_conc[feature_cols])[:,1]\n",
    "\n",
    "full_conc['match_pred'] = y_pred_class\n",
    "full_conc['pred_proba'] = y_pred_proba\n",
    "\n",
    "#we take any matches meeting either name match threshold or composite match threshold as matches for review\n",
    "#org_matches = full_conc[(full_conc.overall_name_score >= name_score_min) | (full_conc.composite_match_score >= composite_score_min)]\n",
    "org_matches = full_conc[full_conc.match_pred == 1]\n",
    "failed_matches_for_review = full_conc[(full_conc.match_pred == 0) & (full_conc.pred_proba > .4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35644204, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_conc[full_conc.pred_proba < .01].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ CROSS VALIDATION each fold ~~~~\n",
      "('Model', 1)\n",
      "('Recall:', 0.8586956521739131)\n",
      "('Precision:', 0.8186528497409327)\n",
      "('F1-Score:', 0.8381962864721486)\n",
      "('Model', 2)\n",
      "('Recall:', 0.8842105263157894)\n",
      "('Precision:', 0.9130434782608695)\n",
      "('F1-Score:', 0.8983957219251336)\n",
      "('Model', 3)\n",
      "('Recall:', 0.9132947976878613)\n",
      "('Precision:', 0.8729281767955801)\n",
      "('F1-Score:', 0.8926553672316384)\n",
      "('Model', 4)\n",
      "('Recall:', 0.8983050847457628)\n",
      "('Precision:', 0.8457446808510638)\n",
      "('F1-Score:', 0.8712328767123287)\n",
      "('Model', 5)\n",
      "('Recall:', 0.8235294117647058)\n",
      "('Precision:', 0.8700564971751412)\n",
      "('F1-Score:', 0.8461538461538461)\n",
      "('Model', 6)\n",
      "('Recall:', 0.8602150537634409)\n",
      "('Precision:', 0.8465608465608465)\n",
      "('F1-Score:', 0.8533333333333333)\n",
      "('Model', 7)\n",
      "('Recall:', 0.9052631578947369)\n",
      "('Precision:', 0.8514851485148515)\n",
      "('F1-Score:', 0.8775510204081632)\n",
      "('Model', 8)\n",
      "('Recall:', 0.8852459016393442)\n",
      "('Precision:', 0.8950276243093923)\n",
      "('F1-Score:', 0.8901098901098902)\n",
      "('Model', 9)\n",
      "('Recall:', 0.8609625668449198)\n",
      "('Precision:', 0.8385416666666666)\n",
      "('F1-Score:', 0.8496042216358839)\n",
      "('Model', 10)\n",
      "('Recall:', 0.835820895522388)\n",
      "('Precision:', 0.8795811518324608)\n",
      "('F1-Score:', 0.8571428571428571)\n",
      "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
      "('Mean recall scores for all folds:', 0.8725543048352863)\n",
      "('Mean precision scores for all folds:', 0.8631622120707805)\n",
      "('Mean F1 scores for all folds:', 0.8674375421125223)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "feature_cols = ['jaro_score',\n",
    "                'fuzz_partial_score',\n",
    "                'fuzz_sort_score',\n",
    "                'fuzz_set_score',\n",
    "                'uniq',\n",
    "                'state_match',\n",
    "                'zip_match',\n",
    "                #'domain_match',\n",
    "                'id_l count']\n",
    "\n",
    "#training_data = pd.read_table('training_data_utf.txt')\n",
    "X = training_data[feature_cols]\n",
    "y = training_data['is_match']\n",
    "\n",
    "kf = cross_validation.KFold(len(X), n_folds=10, shuffle=True)\n",
    "\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "f1_scores = []\n",
    "n= 0\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf:\n",
    "    lm = LogisticRegression().fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    recall_scores.append(metrics.recall_score(y.iloc[test_index], lm.predict(X.iloc[test_index])))\n",
    "    precision_scores.append(metrics.precision_score(y.iloc[test_index], lm.predict(X.iloc[test_index])))\n",
    "    f1_scores.append(metrics.f1_score(y.iloc[test_index], lm.predict(X.iloc[test_index])))\n",
    "    n+=1\n",
    "    print('Model', n)\n",
    "    print('Recall:', recall_scores[n-1])\n",
    "    print('Precision:', precision_scores[n-1])\n",
    "    print('F1-Score:', f1_scores[n-1])\n",
    "\n",
    "print(\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print('Mean recall scores for all folds:', np.mean(recall_scores))\n",
    "print('Mean precision scores for all folds:', np.mean(precision_scores))\n",
    "print('Mean F1 scores for all folds:', np.mean(f1_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
