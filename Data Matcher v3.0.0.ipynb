{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-do patches\n",
    "- Shorten candidate selection during deduplication by not matching against the whole right side (since it would be redundant because that pair has already been checked)\n",
    "- Matching should be able to consider dynamic columns, even if one column is not present on the other side - COMPLETE\n",
    "- Add capability to consider multiple columns of the same type, for example home + employer address - COMPLETE\n",
    "- Handle “future” dates for queening - COMPLETE\n",
    "- Add matching on gender, perhaps derived by prefix if gender not available\n",
    "- Add matching on position title\n",
    "- Include/enable joining matches file to grouped on pair key - for duplicate identification - COMPLETE\n",
    "- Include/enable joining matches file to grouped on pair key - for matching\n",
    "- Suppress non-matching middle initials?  perhaps only when len is <=2 for both.\n",
    "- Standardize country names, I dont think code does this at present (becuase we rely on transformer step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prologue\n",
    "### Setting things up\n",
    "\n",
    "Here we have configurations.\n",
    "\n",
    "First, we import all the libraries.\n",
    "\n",
    "Second, we load and prepare any and all static lookup tables (nicknames, state codes, and personal domains). These lookup tables are set up so that any updates to the underlying CSV files will automatically incorporate the change the next time those chunks are run.\n",
    "\n",
    "Third, we set up all the running parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gc, math, numbers, operator, os, re, time\n",
    "from collections import defaultdict, Counter\n",
    "from statistics import stdev\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from jellyfish import jaro_winkler, metaphone\n",
    "from py_common_subseq import find_common_subsequences\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the nicknames lookup table\n",
    "\n",
    "nicks will return all the possible nickname groups that a name belongs to.\n",
    "\n",
    "To add more nicknames, add more data to the nicknames file and run this chunk\n",
    "again. The file is not actually a data table (hence the sep='\\n' instead of ',');\n",
    "instead, every row represents a group of names that are considered equivalent.\n",
    "\"\"\"\n",
    "df_nicks = pd.read_csv('data/nicknames.csv', sep='\\n', header=None, names=[\"names\"])\n",
    "df_nicks[\"names\"] = df_nicks[\"names\"].str.lower().str.split(',')\n",
    "\n",
    "nicks = defaultdict(set)  # A dictionary with a default value of an empty set\n",
    "df_nicks.apply(lambda row: list(map(lambda name: nicks[name].add(row.name), row[\"names\"])),\n",
    "               axis = \"columns\")\n",
    "nicks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the places lookup table\n",
    "\n",
    "To update or add more state to state acronyms or country name to\n",
    "country acronyms, just add more rows to the respective csv files.\n",
    "To be honest it doesn't even matter which file you add it into;\n",
    "it all gets merged.\n",
    "\"\"\"\n",
    "# Get and lowercase the state codes data\n",
    "df_states = pd.read_csv('data/state_lkup.csv', keep_default_na=False)\n",
    "df_states = df_states.applymap(str.lower)\n",
    "us_states = df_states[df_states.country=='us']\n",
    "df_states = df_states.set_index(\"state\")\n",
    "\n",
    "# Get and lowercase the country codes data\n",
    "df_countries = pd.read_csv('data/country codes.csv', keep_default_na=False)\n",
    "df_countries = df_countries.applymap(str.lower)\n",
    "df_countries = df_countries.set_index(\"COUNTRY\")\n",
    "\n",
    "# Combine the two lookup tables into a master lookup table\n",
    "# See https://stackoverflow.com/a/26853961 for\n",
    "# more info on merging dicts with {**x, **y}\n",
    "place_acronyms = defaultdict(str,\n",
    "                             {**df_states[\"acronym\"].to_dict(),\n",
    "                              **df_countries[\"ISO2\"].to_dict()})\n",
    "place_acronyms;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the personal domains list\n",
    "\n",
    "This just pulls out the list of all personal email domains.\n",
    "\"\"\"\n",
    "personal_domains = pd.read_csv('data/personal email domains.csv')['domain']\n",
    "personal_domains;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictate_columns(*, index=None, updated_date = None, fname=None, lname=None, mname=None, prefix=None, person_suffix=None,\n",
    "                    person_utility1=None,person_utility2=None,person_utility3=None,\n",
    "                    position1=None,position2=None,position_metadata1=None,position_metadata2=None,\n",
    "                    gender=None,\n",
    "                    org_name=None, org_name2=None, org_name3=None,org_name4=None,org_name5=None,org_name6=None,\n",
    "                    address1_a=None, address2_a=None, address3_a=None, city_a=None,state_a=None, zip_a=None, zip4_a=None, country_a=None, \n",
    "                    address_a_utility1=None, address_a_utility2=None, address_a_utility3=None, address_a_utility4=None, \n",
    "                    address_a_utility5=None, address_a_utility6=None, address_a_utility7=None, address_a_utility8=None,\n",
    "                    address1_b=None, address2_b=None, address3_b=None, city_b=None,state_b=None, zip_b=None, zip4_b=None, country_b=None,\n",
    "                    address_b_utility1=None, address_b_utility2=None, address_b_utility3=None, address_b_utility4=None, \n",
    "                    address_b_utility5=None, address_b_utility6=None, address_b_utility7=None, address_b_utility8=None,\n",
    "                    address1_c=None, address2_c=None, address3_c=None, city_c=None,state_c=None, zip_c=None, zip4_c=None, country_c=None,\n",
    "                    address_c_utility1=None, address_c_utility2=None, address_c_utility3=None, address_c_utility4=None, \n",
    "                    address_c_utility5=None, address_c_utility6=None, address_c_utility7=None, address_c_utility8=None,\n",
    "                    address1_d=None, address2_d=None, address3_d=None, city_d=None,state_d=None, zip_d=None, zip4_d=None, country_d=None,\n",
    "                    address_d_utility1=None, address_d_utility2=None, address_d_utility3=None, address_d_utility4=None, \n",
    "                    address_d_utility5=None, address_d_utility6=None, address_d_utility7=None, address_d_utility8=None,\n",
    "                    email_a=None, email_b=None, email_c=None, email_d=None,\n",
    "                    website_a=None, website_b=None, website_c=None, website_d=None,\n",
    "                    phone_a=None, areacode_a=None, phone_number_a=None, extension_a=None,\n",
    "                    phone_a_utility1=None,phone_a_utility2=None,phone_a_utility3=None,\n",
    "                    phone_b=None, areacode_b=None, phone_number_b=None, extension_b=None,\n",
    "                    phone_b_utility1=None,phone_b_utility2=None,phone_b_utility3=None,\n",
    "                    phone_c=None, areacode_c=None, phone_number_c=None, extension_c=None,\n",
    "                    phone_c_utility1=None,phone_c_utility2=None,phone_c_utility3=None,\n",
    "                    phone_d=None, areacode_d=None, phone_number_d=None, extension_d=None,\n",
    "                    phone_d_utility1=None,phone_d_utility2=None,phone_d_utility3=None,\n",
    "                    fax_a=None, fax_areacode_a=None, fax_number_a=None,\n",
    "                    fax_a_utility1=None,fax_a_utility2=None,fax_a_utility3=None,\n",
    "                    utility1=None, utility2=None):\n",
    "    \"\"\"\n",
    "    This function is a to ensure that no misspelling or mistaking of columns happen\n",
    "    when defining which dataset-specific columns match to which general dataset columns.\n",
    "    \n",
    "    So by calling this function, Python will warn of any weird parameter names.\n",
    "\n",
    "    As a last detail, these columns are used to determine which columns are important for\n",
    "    scoring. So that's why nicks_groups is present. If it isn't, nicks_groups will be left\n",
    "    out of the scoring section's dataframes and cause trouble.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'id': index,\n",
    "        'updated_date': updated_date,\n",
    "        'prefix':prefix,\n",
    "        'fname': fname,\n",
    "        'mname': mname,\n",
    "        'lname': lname,\n",
    "        'person_suffix':person_suffix, \n",
    "        'person_utility1': person_utility1,\n",
    "        'person_utility2': person_utility2,\n",
    "        'person_utility3': person_utility3,\n",
    "        'gender': gender,\n",
    "        'position1': position1,\n",
    "        'position2': position2,\n",
    "        'position_metadata1': position_metadata1,\n",
    "        'position_metadata2': position_metadata2,\n",
    "        'org_name': org_name,\n",
    "        'org_name2':org_name2,\n",
    "        'org_name3':org_name3,\n",
    "        'org_name4':org_name4,\n",
    "        'org_name5':org_name5,\n",
    "        'org_name6':org_name6,\n",
    "        'address1_a': address1_a,\n",
    "        'address2_a': address2_a,\n",
    "        'address3_a': address3_a,\n",
    "        'city_a': city_a,\n",
    "        'state_a': state_a,\n",
    "        'zip_a': zip_a,\n",
    "        'country_a': country_a,\n",
    "        'address_a_utility1':address_a_utility1,\n",
    "        'address_a_utility2':address_a_utility2,\n",
    "        'address_a_utility3':address_a_utility3,\n",
    "        'address_a_utility4':address_a_utility4,\n",
    "        'address_a_utility5':address_a_utility5,\n",
    "        'address_a_utility6':address_a_utility6,\n",
    "        'address_a_utility7':address_a_utility7,\n",
    "        'address_a_utility8':address_a_utility8,\n",
    "        'address1_b': address1_b,\n",
    "        'address2_b': address2_b,\n",
    "        'address3_b': address3_b,\n",
    "        'city_b': city_b,\n",
    "        'state_b': state_b,\n",
    "        'zip_b': zip_b,\n",
    "        'country_b': country_b,\n",
    "        'address_b_utility1':address_b_utility1,\n",
    "        'address_b_utility2':address_b_utility2,\n",
    "        'address_b_utility3':address_b_utility3,\n",
    "        'address_b_utility4':address_b_utility4,\n",
    "        'address_b_utility5':address_b_utility5,\n",
    "        'address_b_utility6':address_b_utility6,\n",
    "        'address_b_utility7':address_b_utility7,\n",
    "        'address_b_utility8':address_b_utility8,\n",
    "        'address1_c': address1_c,\n",
    "        'address2_c': address2_c,\n",
    "        'address3_c': address3_c,\n",
    "        'city_c': city_c,\n",
    "        'state_c': state_c,\n",
    "        'zip_c': zip_c,\n",
    "        'country_c': country_c,\n",
    "        'address_c_utility1':address_c_utility1,\n",
    "        'address_c_utility2':address_c_utility2,\n",
    "        'address_c_utility3':address_c_utility3,\n",
    "        'address_c_utility4':address_c_utility4,\n",
    "        'address_c_utility5':address_c_utility5,\n",
    "        'address_c_utility6':address_c_utility6,\n",
    "        'address_c_utility7':address_c_utility7,\n",
    "        'address_c_utility8':address_c_utility8,\n",
    "        'address1_d': address1_d,\n",
    "        'address2_d': address2_d,\n",
    "        'address3_d': address3_d,\n",
    "        'city_d': city_d,\n",
    "        'state_d': state_d,\n",
    "        'zip_d': zip_d,\n",
    "        'country_d': country_d,\n",
    "        'address_d_utility1':address_d_utility1,\n",
    "        'address_d_utility2':address_d_utility2,\n",
    "        'address_d_utility3':address_d_utility3,\n",
    "        'address_d_utility4':address_d_utility4,\n",
    "        'address_d_utility5':address_d_utility5,\n",
    "        'address_d_utility6':address_d_utility6,\n",
    "        'address_d_utility7':address_d_utility7,\n",
    "        'address_d_utility8':address_d_utility8,\n",
    "        'email_a': email_a,\n",
    "        'email_b': email_b,\n",
    "        'email_c': email_c,\n",
    "        'email_d': email_d,\n",
    "        'website_a': website_a,\n",
    "        'website_b': website_b,\n",
    "        'website_c': website_c,\n",
    "        'website_d': website_d,\n",
    "        'phone_a':phone_a,'areacode_a':areacode_a,'phone_number_a':phone_number_a,'extension_a':extension_a,\n",
    "        'phone_a_utility1':phone_a_utility1,'phone_a_utility2':phone_a_utility2,'phone_a_utility3':phone_a_utility3,\n",
    "        'phone_b':phone_b,'areacode_b':areacode_b,'phone_number_b':phone_number_b,'extension_b':extension_b,\n",
    "        'phone_b_utility1':phone_b_utility1,'phone_b_utility2':phone_b_utility2,'phone_b_utility3':phone_b_utility3,\n",
    "        'phone_c':phone_c,'areacode_c':areacode_c,'phone_number_c':phone_number_c,'extension_c':extension_c,\n",
    "        'phone_c_utility1':phone_c_utility1,'phone_c_utility2':phone_c_utility2,'phone_c_utility3':phone_c_utility3,\n",
    "        'fax_a':fax_a,'fax_areacode_a':areacode_a,'fax_number_a':phone_number_a,\n",
    "        'fax_a_utility1':fax_a_utility1,'fax_a_utility2':fax_a_utility2,'fax_a_utility3':fax_a_utility3,\n",
    "        'utility1':utility1,\n",
    "        'utility2':utility2,\n",
    "        'nicks_groups': None,\n",
    "    }\n",
    "\n",
    "chunk_size = 1_000_000     # How many rows fit in memory at a time\n",
    "small_chunk_size = 100_000 # Use for cells that need memory help\n",
    "token_match_min = 2        # Min number of matched tokens to be considered a match candidate\n",
    "token_limiter = .995        # Top x% of scarcest non-single tokens to keep (aka drop the most common (1-x)% tokens)\n",
    "unique_token_freq_max = 5  # Max occurences of a token to be considered \"unique\" and be double-counted\n",
    "reduce_threshold = .6     # this is used to decide the line where we cull unlikely matches from being scored further (do not reduce to 0 as we use 0 to suppress, but you could go with '.01' if you want to be maximally inclusive)\n",
    "intermediary_dir = 'build' # Where intermediary files will be held\n",
    "match_threshold = .5      # Threshold for considering a match a match when creating composite score\n",
    "group_threshold = .5      # Threshold for a valid grouping\n",
    "\n",
    "#if you set this higher than 1 it may cause error during dedup score calculation at the end - need to resolve this\n",
    "group_merge_limiter = 1   #discourages two groups from merging; set to 1 to ignore. useful if you have reason to separate groups\n",
    "\n",
    "organization_only = False   # Ignore contact info; only focus on organization columns and features\n",
    "compress = False #set to True to force compression\n",
    "entity_ID = False #set to True for resolution of duplicates with multiple passes of grouping\n",
    "\n",
    "#if dataset includes gender column, define male and female values\n",
    "male_value = 'Male' \n",
    "female_value = 'Female'\n",
    "\n",
    "# Columns that, if present, will be used to collect tokens\n",
    "#NOTE - at present whether token type appears in person_token_columns or org_token_columns has no impact on candidate discovery\n",
    "#what DOES make a difference is where the features are considered as part of prediction step (for person or org model)\n",
    "person_token_columns = [\n",
    "    'fname',\n",
    "    'meta_fname',\n",
    "    'nicks_groups',\n",
    "    'lname',\n",
    "    'meta_lname',\n",
    "    'before_domain_a','before_domain_b','before_domain_c','before_domain_d', #derived from email\n",
    "    'mname',\n",
    "]\n",
    "\n",
    "org_token_columns = [\n",
    "    'org_name','org_name2','org_name3',\n",
    "    'address1_a','city_a','state_a','zip_a',#'country_a',\n",
    "    'address1_b','city_b','state_b','zip_b',#'country_b',\n",
    "    'address1_c','city_c','state_c','zip_c',#'country_c',\n",
    "    'address1_d','city_d','state_d','zip_d',#'country_d',\n",
    "    'clean_phone_a','clean_phone_b','clean_phone_c','clean_fax_a',\n",
    "    'email_domain_a','email_domain_b','email_domain_c','email_domain_d',\n",
    "    'web_domain_a','web_domain_b','web_domain_c','web_domain_d',\n",
    "    'utility1',\n",
    "    'utility2'\n",
    "]\n",
    "\n",
    "full_token_columns = person_token_columns + org_token_columns\n",
    "\n",
    "# If we're doing organization only, then we don't care about\n",
    "# person_token_columns and in fact want to treat it like they\n",
    "# don't exist at all, so we overwrite it here to be nothing.\n",
    "if organization_only: \n",
    "    person_token_columns = []\n",
    "    #org_token_columns.append('clean_phone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Details\n",
    "\n",
    "Here, the parameters for every dataset that goes into this program is set up. The `data_details` variable is a list of options that gets fed to `pd.read_csv`, and `cols` is a dictionary generated by `dictate_columns` that holds the names of all the columns.\n",
    "\n",
    "A couple are defined, one after the other. The last one overwrites the others, so only the last one is in effect. The other ones I leave as a record.\n",
    "\n",
    "#### Golden record creation notes\n",
    "\n",
    "Because reconciliation of duplicate records for the creation of golden records entails more detail than what is strictly required for duplicate identification, we should flesh out this step to the maximum degree.  The golden record code inherits a file containing all column mappings (to avoid having to do this step a second time).  This is why there are many \"_utility\" columns.  You should map ALL columns that should be considered together when merging duplicate records.  \n",
    "\n",
    "For example, if there is an address_id column, we'd like to bring that along with its corresponding address details.  It should be mapped as a address_X_utlity column.  Similarly if there are any ancillary fields associated with basic contact data (e.g. nicknames, alternative organization names, category codes associated with an email column, etc.) these should be mapped.  Note also: Job titles should be mapped even though they are not considered for data matching.  \n",
    "\n",
    "But you do NOT need to map any columns that were created during the transformer or infogroup (data axle) processor steps.  This is becasue the golden record code can identify these columns based on the known naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the metadata for dataset\n",
    "\"\"\"\n",
    "data_details = {\n",
    "    \"filepath_or_buffer\": \"data/A3/bb_clean_w_infogroup w queening.csv\",\n",
    "    \"sep\": \",\",\n",
    "    'nrows':5_000\n",
    "}\n",
    "cols = dictate_columns(\n",
    "    index='u_id', #don't recommend using an index column containing leading zeroes, ex. '00134'\n",
    "    #updated_date='(Do Not Modify) Modified On',\n",
    "    #\n",
    "    org_name='PrimaryOrganizationName',\n",
    "    #org_name2='Account Number (Parent Customer) (Account)', #note: include fields which can be generally prioritized based on count of characters\n",
    "    #org_name3='X',\n",
    "    #org_name4='X',\n",
    "    #org_name5='X',\n",
    "    #org_name6='X',\n",
    "    #\n",
    "    prefix='NamePrefix',\n",
    "    fname='FirstName',\n",
    "    mname='MiddleName',\n",
    "    lname='LastName',\n",
    "    #person_suffix='Suffix',\n",
    "    person_utility1='InformalName', #include columns that should be married to corresponding name, like \"nickname\"\n",
    "    #person_utility2='Birthday',\n",
    "    #person_utility3='X',\n",
    "    #\n",
    "    #gender='Gender',\n",
    "    #\n",
    "    position1='PrimaryOrganizationTitle',\n",
    "    #position2='X',\n",
    "    #\n",
    "    address1_a='AddressLines',\n",
    "    address2_a='CarrierRoute',\n",
    "    #address3_a='MailingState',\n",
    "    city_a='CityName',\n",
    "    state_a='CountrySubEntityCode',\n",
    "    zip_a='PostalCode',\n",
    "    country_a='CountryName',\n",
    "    #address_a_utility1='Address 1 Country Code',\n",
    "    #address_a_utility2='Address 1: Post Office Box',\n",
    "    #address_a_utility3='PREFERRED_MAIL_directory',\n",
    "    #address_a_utility4='PREFERRED_BILL_directory',\n",
    "    #address_a_utility5='PREFERRED_SHIP_directory',\n",
    "    #address_a_utility6='TIME_STAMP_directory',\n",
    "    #address_a_utility7='ADDRESS_3_directory',\n",
    "    #address_a_utility8='ADDRESS_3_directory',\n",
    "    #\n",
    "    #address1_b='ADDRESS_1_secondary',\n",
    "    #address2_b='ADDRESS_2_secondary',\n",
    "    #address3_b='ADDRESS_3_secondary',\n",
    "    #city_b='CITY_secondary',\n",
    "    #state_b='STATE_PROVINCE_secondary',\n",
    "    #zip_b='ZIP_secondary',\n",
    "    #country_b='COUNTRY_secondary',\n",
    "    #address_b_utility1='ADDRESS_NUM_secondary',\n",
    "    #address_b_utility2='BAD_ADDRESS_secondary',\n",
    "    #address_b_utility3='PREFERRED_MAIL_secondary',\n",
    "    #address_b_utility4='PREFERRED_BILL_secondary',\n",
    "    #address_b_utility5='PREFERRED_SHIP_secondary',\n",
    "    #address_b_utility6='TIME_STAMP_secondary',\n",
    "    #address_b_utility7='ADDRESS_3_directory',\n",
    "    #address_b_utility8='ADDRESS_3_directory',\n",
    "    #\n",
    "    #address1_c='ADDRESS_1_personal',\n",
    "    #address2_c='ADDRESS_2_personal',\n",
    "    #address3_c='ADDRESS_3_personal',\n",
    "    #city_c='CITY_personal',\n",
    "    #state_c='STATE_PROVINCE_personal',\n",
    "    #zip_c='ZIP_personal',\n",
    "    #country_c='COUNTRY_personal',\n",
    "    #address_c_utility1='ADDRESS_NUM_personal',\n",
    "    #address_c_utility2='BAD_ADDRESS_personal',\n",
    "    #address_c_utility3='PREFERRED_MAIL_personal',\n",
    "    #address_c_utility4='PREFERRED_BILL_personal',\n",
    "    #address_c_utility5='PREFERRED_SHIP_personal',\n",
    "    #address_c_utility6='TIME_STAMP_personal',\n",
    "    #address_c_utility7='ADDRESS_3_directory',\n",
    "    #address_c_utility8='ADDRESS_3_directory',\n",
    "    #\n",
    "    phone_a='Phone',\n",
    "    #phone_a_utility1 = 'x',\n",
    "    #phone_a_utility2 = 'x',\n",
    "    #phone_a_utility3 = 'x',\n",
    "    phone_b='Phones_Number',\n",
    "   # phone_b_utility1 = 'Phone 1 Country',\n",
    "    #phone_b_utility2 = 'Phone 1 Country Code',\n",
    "    #phone_b_utility3 = 'x',\n",
    "    phone_c='Phones_Number2',\n",
    "    #phone_c_utility1 = 'x',\n",
    "    #phone_c_utility2 = 'x',\n",
    "    #phone_c_utility3 = 'x',\n",
    "    #fax_a='fax',\n",
    "    #fax_a_utility1 = 'Fax Country',\n",
    "    #fax_a_utility2 = 'Fax Country Code',\n",
    "    #fax_a_utility3 = 'x',\n",
    "    #\n",
    "    website_a='WebsiteUrl',\n",
    "    #website_b='Personal Website',\n",
    "    #\n",
    "    email_a='Email',\n",
    "    email_b='EmailsAddress',\n",
    "    email_c='EmailsAddress2',\n",
    "    #\n",
    "    #utility1='CO_ID'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we're looking for matches, we want to define the dataset details for the second dataset. If these are not defined, then the program will act as a deduplicator. If so, then it will find matches.\n",
    "\n",
    "Essentially, the program will check for the existence of the `data_alt_details` and `cols_alt` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: this code is for reference in MATCHING projects only.  Not relevant to deduplication.\n",
    "#to MATCH between two datasets, include alt data details below\n",
    "#and make sure to #comment out the two del lines at bottom of this cell\n",
    "#script checks for existence of data_alt_details & cols_alt in order to determine whether this is a matching exercise\n",
    "'''\n",
    "Define the metadata for the engy_prospects dataset\n",
    "\n",
    "This is defined as a find matches dataset. So in effect\n",
    "it's still just deduplication, but this will allow us to\n",
    "test out the match finding capabilities, and doublecheck\n",
    "them against the deduplication function.\n",
    "'''\n",
    "\n",
    "data_alt_details = {\n",
    "    \"filepath_or_buffer\": \"data/SF & Cupola data match/all cupola orgs.csv\",\n",
    "    \"sep\": \",\",\n",
    "    #\"nrows\": 1_000,\n",
    "}\n",
    "cols_alt = dictate_columns(\n",
    "    index='organization_id', #don't recommend using an index column containing leading zeroes, ex. '00134'\n",
    "    #updated_date='(Do Not Modify) Modified On',\n",
    "    #\n",
    "    org_name='name',\n",
    "    org_name2='acronym', #note: include fields which can be generally prioritized based on count of characters\n",
    "    org_name3='Alt_Names',\n",
    "    #org_name4='X',\n",
    "    #org_name5='X',\n",
    "    #org_name6='X',\n",
    "    #\n",
    "    #prefix='PREFIX',\n",
    "    #fname='FIRST_NAME',\n",
    "    #mname='MIDDLE_NAME',\n",
    "    #lname='LAST_NAME',\n",
    "    #person_suffix='SUFFIX',\n",
    "    #person_utility1='Nickname', #include columns that should be married to corresponding name, like \"nickname\"\n",
    "    #person_utility2=' Full Name',\n",
    "    #person_utility3='X',\n",
    "    #\n",
    "    #gender='X',\n",
    "    #\n",
    "    #position1='Job Title',\n",
    "    #position2='X',\n",
    "    #\n",
    "    address1_a='address1',\n",
    "    address2_a='address2',\n",
    "    #address3_a='ADDRESS_3_directory',\n",
    "    city_a='city',\n",
    "    state_a='state',\n",
    "    zip_a='postal_code',\n",
    "    #country_a='COUNTRY_directory',\n",
    "    #address_a_utility1='ADDRESS_NUM_directory',\n",
    "    #address_a_utility2='BAD_ADDRESS_directory',\n",
    "    #address_a_utility3='PREFERRED_MAIL_directory',\n",
    "    #address_a_utility4='PREFERRED_BILL_directory',\n",
    "    #address_a_utility5='PREFERRED_SHIP_directory',\n",
    "    #address_a_utility6='TIME_STAMP_directory',\n",
    "    #address_a_utility7='ADDRESS_3_directory',\n",
    "    #address_a_utility8='ADDRESS_3_directory',\n",
    "    #\n",
    "    #address1_b='ADDRESS_1_secondary',\n",
    "    #address2_b='ADDRESS_2_secondary',\n",
    "    #address3_b='ADDRESS_3_secondary',\n",
    "    #city_b='CITY_secondary',\n",
    "    #state_b='STATE_PROVINCE_secondary',\n",
    "    #zip_b='ZIP_secondary',\n",
    "    #country_b='COUNTRY_secondary',\n",
    "    #address_b_utility1='ADDRESS_NUM_secondary',\n",
    "    #address_b_utility2='BAD_ADDRESS_secondary',\n",
    "    #address_b_utility3='PREFERRED_MAIL_secondary',\n",
    "    #address_b_utility4='PREFERRED_BILL_secondary',\n",
    "    #address_b_utility5='PREFERRED_SHIP_secondary',\n",
    "    #address_b_utility6='TIME_STAMP_secondary',\n",
    "    #address_b_utility7='ADDRESS_3_directory',\n",
    "    #address_b_utility8='ADDRESS_3_directory',\n",
    "    #\n",
    "    #address1_c='ADDRESS_1_personal',\n",
    "    #address2_c='ADDRESS_2_personal',\n",
    "    #address3_c='ADDRESS_3_personal',\n",
    "    #city_c='CITY_personal',\n",
    "    #state_c='STATE_PROVINCE_personal',\n",
    "    #zip_c='ZIP_personal',\n",
    "    #country_c='COUNTRY_personal',\n",
    "    #address_c_utility1='ADDRESS_NUM_personal',\n",
    "    #address_c_utility2='BAD_ADDRESS_personal',\n",
    "    #address_c_utility3='PREFERRED_MAIL_personal',\n",
    "    #address_c_utility4='PREFERRED_BILL_personal',\n",
    "    #address_c_utility5='PREFERRED_SHIP_personal',\n",
    "    #address_c_utility6='TIME_STAMP_personal',\n",
    "    #address_c_utility7='ADDRESS_3_directory',\n",
    "    #address_c_utility8='ADDRESS_3_directory',\n",
    "    #\n",
    "    phone_a='Phone',\n",
    "    #phone_b='PHONE_personal',\n",
    "    #phone_c='Home Phone',\n",
    "    #fax_a='Fax',\n",
    "    #\n",
    "    website_a='Website',\n",
    "    #website_b='Personal Website',\n",
    "    #\n",
    "    #email_a='EMAIL_directory',\n",
    "    #email_b='EMAIL_personal',\n",
    "    #\n",
    "    #utility1='Grantor_ID_for token'\n",
    ")\n",
    "\n",
    "##### Heads up running this code:\n",
    "#\n",
    "# I fixed a bug where even during matching mode\n",
    "# the candidate selection would filter out identical IDs\n",
    "# but if we're matching we don't want to assume that they're\n",
    "# the same. So that code is now under a `if not matching:`\n",
    "# block. This means: Running this code will match every\n",
    "# row to every row and take forever to run so uh don't run it.\n",
    "#\n",
    "# I did it while it was still broken and it worked because\n",
    "# it was a roundabout deduplication so I could check that it\n",
    "# works. Now though only use it for actual matching not dedups.\n",
    "\n",
    "# Right now I want to run dedup but I don't want to delete\n",
    "# the code, so I'll just erase the variables instead.\n",
    "\n",
    "##################################################################\n",
    "del data_alt_details\n",
    "del cols_alt\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a flag for whether we are matching.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    data_alt_details\n",
    "    cols_alt\n",
    "except NameError:\n",
    "    # If NameError, then the variables don't exist\n",
    "    # and we're not in a matching mode\n",
    "    matching = False\n",
    "else:\n",
    "    # If try goes through fine, then these variables\n",
    "    # do exist and we're matching\n",
    "    matching = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create string converters where appropriate\n",
    "try:\n",
    "    string_cols_df_filepath = '/'.join(data_details['filepath_or_buffer'].split('/')[:-1]) + '/infogroup_processor_string_columns_df.csv'\n",
    "    string_columns_df = pd.read_csv(string_cols_df_filepath)\n",
    "    string_columns = list(string_columns_df.column)\n",
    "    converters = {col: str for col in string_columns}\n",
    "    data_details.update({'converters':converters})\n",
    "except:\n",
    "    data_details.update({'converters':None})\n",
    "if matching:\n",
    "    try:\n",
    "        string_columns_alt = list(pd.read_csv('/'.join(data_alt_details['filepath_or_buffer'].split('/')[:-1]) + '/infogroup_processor_string_columns_df.csv').column)\n",
    "        converters_alt = {col: str for col in string_columns_alt}\n",
    "        data_alt_details.update({'converters':converters_alt})\n",
    "    except:\n",
    "        data_alt_details.update({'converters':None})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#don't think we need but this code is handy\\n#creating an inverse dictionary to return to original column names later\\ninv_col_dict = {d: c for c, d in cols.items()}\\nif matching:\\n    inv_alt_col_dict = {d: c for c, d in cols_alt.items()} #do I need to handle _l and _r suffixes?\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#don't think we need but this code is handy\n",
    "#creating an inverse dictionary to return to original column names later\n",
    "inv_col_dict = {d: c for c, d in cols.items()}\n",
    "if matching:\n",
    "    inv_alt_col_dict = {d: c for c, d in cols_alt.items()} #do I need to handle _l and _r suffixes?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building intermediary directory\n",
    "\n",
    "This chunk checks the existence of the directory to hold all the intermediary files. It then creates a subdirectory within that directory to hold all the files for *this* dataset on *this* run.\n",
    "\n",
    "Finally, it creates the `ns` function, that takes a filename and returns a filename for that file within the proper directory. The `ns` stands for `namespace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overriding; will create new subdirectory\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Override sub-directory for debugging purposes if necessary.\n",
    "\n",
    "If for example, I'm coming back to the program after a break or\n",
    "some restart of Jupyter, I can manually set folder to the subfolder\n",
    "I want (e.g. \"dataset.csv\" or \"orgfile.tsv3\" or something) instead of\n",
    "creating a new subfolder.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    folder\n",
    "except NameError:\n",
    "    print(\"No overriding; will create new subdirectory\")\n",
    "else:\n",
    "    print(\"Overriding subdirectory selection: Using subdirectory\", folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory build found\n",
      "Sub-directory bb_clean_w_infogroup w queening.csv created\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set parameters\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(intermediary_dir)\n",
    "    print(\"Directory\", intermediary_dir, \"created\") \n",
    "except FileExistsError:\n",
    "    print(\"Directory\", intermediary_dir, \"found\")\n",
    "\n",
    "\n",
    "try:\n",
    "    folder\n",
    "except NameError:\n",
    "    # The subfolder name will just be the name of the datafile. If this is\n",
    "    # a matching task, then it'll be \"datasetA.csv_datasetB.csv\"\n",
    "    namespace = data_details['filepath_or_buffer'].split(\"/\")[-1]\n",
    "    if matching:\n",
    "        namespace += \"_\" + data_alt_details['filepath_or_buffer'].split(\"/\")[-1]\n",
    "\n",
    "    unique = \"\"\n",
    "    while True:\n",
    "        try:\n",
    "            os.mkdir(os.path.join(intermediary_dir, namespace + str(unique)))\n",
    "            print(\"Sub-directory\", namespace + str(unique), \"created\")\n",
    "            break\n",
    "        except FileExistsError:\n",
    "            # If a subfolder already exists, start tacking on numbers.\n",
    "            # name -> name1 -> name2 -> ...\n",
    "            unique = 1 if unique == \"\" else unique + 1\n",
    "\n",
    "    def ns(file): return os.path.join(intermediary_dir, namespace + str(unique), file)\n",
    "else:\n",
    "    print(\"Using existing sub-directory\", folder)\n",
    "    def ns(file): return os.path.join(intermediary_dir, folder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunks(chunks, chunk_func, outfile=None, silent=False, **kwargs):\n",
    "    \"\"\"\n",
    "    The process_chunks function takes a generator of dataset chunks (created from\n",
    "    pd.read_csv(chunksize=X)) and a chunk_func that runs on that dataset, and runs\n",
    "    it for each chunk, writing the result to file.\n",
    "    \n",
    "    So it reads chunks, applies the function to each chunk, then optionally writes\n",
    "    the results to a new file `outfile` for further processing.\n",
    "    \n",
    "    **kwargs basically allows the caller to include any other keyword arguments, and\n",
    "    they will all be passed to the to_csv function. So mostly things like `index=False`.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    total_rows = 0\n",
    "    needs_init = True\n",
    "    for count, chunk in enumerate(chunks):\n",
    "        local_time = time.time()\n",
    "\n",
    "        # Some preprocessing that needs to be done every read_csv\n",
    "        chunk = chunk.fillna('')\n",
    "\n",
    "        processed = chunk_func(chunk)\n",
    "        \n",
    "        if outfile:\n",
    "            # mode=w overwrites the file, which we want on initial write\n",
    "            # mode=a appends to the end of file, which we want as we process more chunks\n",
    "            # header=True puts a header on the top, which we want on initial write\n",
    "            # header=False omits the header, which we want as we add more chunks to the existing file\n",
    "\n",
    "            default_args = {'mode': 'w' if needs_init else 'a', 'header': needs_init, 'compression': compression}\n",
    "            # **dict(default, **kwargs) creates a new dictionary of arguments. It basically combines\n",
    "            # the defaults with the passed in arguments. It also allows the keyword arguments to override\n",
    "            # any of the default arguments.\n",
    "            #\n",
    "            # Note: function(**{'a': 1, 'b': 2, 'c': 3}) == function(a=1, b=2, c=3)\n",
    "            processed.to_csv(outfile, **dict(default_args, **kwargs))\n",
    "            needs_init = False\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "        if not silent:\n",
    "            print(\"Processed chunk\", count,\n",
    "                  round(time.time() - local_time, 2), \"seconds\")\n",
    "    if not silent:\n",
    "        print(\"Finished in\", round(time.time() - start_time, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving forward the postal type dict for later use in next step, golden record creation\n",
    "postal_type_dict_filepath = '/'.join(data_details['filepath_or_buffer'].split('/')[:-1]) + '/postal_type_dict.csv'\n",
    "try:\n",
    "    postal_type_dict = pd.read_csv(postal_type_dict_filepath) \n",
    "    postal_type_dict.to_csv(ns('postal_type_dict.csv'),index=False)\n",
    "except:\n",
    "    postal_type_dict_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "### Do dataset-wide standardization\n",
    "Standardize the dataset by globally lowercasing and filling in nulls. Then, for applicable columns, preprocess them to be fit for tokenizing and for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/A3/bb_clean_w_infogroup w queening.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1c8bf76e6d98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_details\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filepath_or_buffer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/data1_utf.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/A3/bb_clean_w_infogroup w queening.csv'"
     ]
    }
   ],
   "source": [
    "#deal with utf-8 encoding\n",
    "import io\n",
    "import shutil\n",
    "\n",
    "with io.open(data_details['filepath_or_buffer'], encoding='utf-8', errors='ignore') as source:\n",
    "    with io.open('data/data1_utf.csv', mode='w', encoding='utf-8') as target:\n",
    "        shutil.copyfileobj(source,target)\n",
    "        \n",
    "data_details.update({'filepath_or_buffer':'data/data1_utf.csv'})\n",
    "\n",
    "if matching:\n",
    "    with io.open(data_alt_details['filepath_or_buffer'], encoding='utf-8', errors='ignore') as source:\n",
    "        with io.open('data/data2_utf.csv', mode='w', encoding='utf-8') as target:\n",
    "            shutil.copyfileobj(source,target)\n",
    "        \n",
    "    data_alt_details.update({'filepath_or_buffer':'data/data2_utf.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/A3/bb_clean_w_infogroup w queening.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_details['filepath_or_buffer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define compression variable based on input data size\n",
    "if compress is True:\n",
    "    compression = 'gzip'\n",
    "else:\n",
    "    try:\n",
    "        data_details['nrows']\n",
    "        compression = 'infer'\n",
    "    except:\n",
    "        if matching:\n",
    "            df1 = pd.read_csv(data_details['filepath_or_buffer'],sep=data_details['sep'])\n",
    "            df2 = pd.read_csv(data_alt_details['filepath_or_buffer'],sep=data_alt_details['sep'])\n",
    "            if (df1.shape[0] >= 150_000) | (df2.shape[0] >= 150_000):\n",
    "                compression = 'gzip'\n",
    "            else:\n",
    "                compression = 'infer'\n",
    "            df1=pd.DataFrame() #clearing memory\n",
    "            df2=pd.DataFrame()\n",
    "        else:\n",
    "            df = pd.read_csv(data_details['filepath_or_buffer'],sep=data_details['sep'])\n",
    "            if df.shape[0] >= 150_000:\n",
    "                compression = 'gzip'\n",
    "            else:\n",
    "                compression = 'infer'\n",
    "            df=pd.DataFrame() #clearing memory\n",
    "        \n",
    "compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data(data_details, cols, out_file=\"data.csv\"):\n",
    "    start_time = time.time()\n",
    "    print (\"LOADING INITIAL DATAFRAMES...\")\n",
    "\n",
    "    def _init_data(df):\n",
    "        # When renaming, invert the columns dictionary to use the keys as the standard \n",
    "        df = df.rename({column_name: standard_name for standard_name, column_name in cols.items()},\n",
    "                       axis=\"columns\")\n",
    "        df = df.dropna(how='all')                    # Drop all empty rows\n",
    "        df = df.set_index('id')                      # Make ID the index to the data\n",
    "        df = df[~df.index.duplicated(keep='first')]  # Drop all duplicate ID's (ID's assumed unique)\n",
    "        df = df.fillna('')                           # Make any NA an empty string\n",
    "        df = df.applymap(str.lower)                  # Lowercase all fields\n",
    "        df = df.applymap(str.strip)                  # Strip whitespace from all fields\n",
    "\n",
    "        return df\n",
    "\n",
    "    process_chunks(pd.read_csv(**data_details, chunksize=chunk_size,encoding='utf-8',\n",
    "                               error_bad_lines=False, dtype=object),\n",
    "                   _init_data,\n",
    "                   ns(out_file))\n",
    "\n",
    "    print(\"Dataframe loaded --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "if matching:\n",
    "    init_data(data_details, cols, \"data_l.csv\")\n",
    "    init_data(data_alt_details, cols_alt, \"data_r.csv\")\n",
    "else:\n",
    "    init_data(data_details, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "\n",
    "def preprocess(in_file=\"data.csv\", out_file=\"processed.csv\", match_items=cols):\n",
    "    start_time = time.time()\n",
    "    print (\"PRE-PROCESSING...\")\n",
    "\n",
    "    def _preprocess(df):\n",
    "        ################QUEEN SCORE CALCULATIONS###############        \n",
    "        cbis_valid_cols = []\n",
    "        email_val_cols = []\n",
    "        postal_val_cols = []\n",
    "        coa_cols = []\n",
    "        for col in df.columns:\n",
    "            if str(col)[-6:] == 'Valid?':\n",
    "                cbis_valid_cols.append(col)\n",
    "            if str(col)[-10:] == 'Risk Score':\n",
    "                email_val_cols.append(col)\n",
    "            if 'Mailability_Score' in col:\n",
    "                postal_val_cols.append(col)\n",
    "            if 'Change of Address?' in col:\n",
    "                coa_cols.append(col)\n",
    "        \n",
    "        if 'recent_activity_date' in df:\n",
    "            queen_date_cols = []\n",
    "            for col in df.columns:\n",
    "                if 'recent_activity_date' in col:\n",
    "                    queen_date_cols.append(col)\n",
    "            \n",
    "            import dateutil.parser\n",
    "            from datetime import date\n",
    "            today = date.today()\n",
    "            today_days = (50 * 365) + (today.month * 30) + today.day #surely 50 years is enough\n",
    "            \n",
    "            n=1\n",
    "            for col in queen_date_cols:\n",
    "                delta_days_col = 'delta_days' + str(n)\n",
    "                z_score_col = 'z_score_delta_days' + str(n)\n",
    "                delta_days = [] #find number of days between today's date and most recent transaction as proportion of total days\n",
    "                for date in df[col]:\n",
    "                    try:\n",
    "                        delta_days.append((today - dateutil.parser.parse(date).date()).days / today_days)\n",
    "                    except:\n",
    "                        delta_days.append(1) #high values are penalized as this means it is an OLD transaction date\n",
    "                df[delta_days_col] = delta_days\n",
    "                '''\n",
    "                #calculating the second oldest value (basically the oldest value that's not a NULL)\n",
    "                transaction_cnt_counts = pd.DataFrame(pd.DataFrame(delta_days,columns=['transaction_cnt_proportion'])['transaction_cnt_proportion'].value_counts())\n",
    "                transaction_cnt_counts.sort_values(by='transaction_cnt_proportion',ascending=False,inplace=True)\n",
    "                transaction_cnt_counts.reset_index(inplace=True)\n",
    "                #now replacing 1s with oldest value +.001 to soften the scores\n",
    "                delta_days = [x if x < max(delta_days) else transaction_cnt_counts['index'][1] + .001 for x in delta_days]\n",
    "                df['delta_days2'] = delta_days\n",
    "                '''\n",
    "                #calculating z score\n",
    "                delta_day_values = df[df[delta_days_col]<1][delta_days_col]\n",
    "                sd_delta_days = stdev(delta_day_values)\n",
    "                avg_delta_days = sum(delta_day_values) / len(delta_day_values)\n",
    "                z_delta_days = []\n",
    "                for v in df[delta_days_col]:\n",
    "                    if v == 1:\n",
    "                        z_delta_days.append(0)\n",
    "                    else:\n",
    "                        z_delta_days.append(((avg_delta_days - v) / sd_delta_days) + (5-n))\n",
    "                df[z_score_col] = z_delta_days\n",
    "                n=n+1\n",
    "        else:\n",
    "            df['z_score_delta_days1'] = 0\n",
    "        \n",
    "        if 'z_score_delta_days2' not in df: #add this in case it was not just created\n",
    "            df['z_score_delta_days2'] = 0\n",
    "            \n",
    "        #handle date columns which are \"future\" dates, for example 'Membership_expire_date'\n",
    "        if 'future_date' in df:\n",
    "            future_date_cols = []\n",
    "            for col in df.columns:\n",
    "                if 'future_date' in col:\n",
    "                    future_date_cols.append(col)\n",
    "        \n",
    "            import dateutil.parser\n",
    "            from datetime import date\n",
    "            today = date.today() #use to calculate number of days difference from today vs future date\n",
    "                \n",
    "            n=1\n",
    "            for col in future_date_cols:\n",
    "                delta_future_days_col = 'delta_future_days' + str(n)\n",
    "                z_score_col = 'z_score_delta_future_days' + str(n)\n",
    "                delta_future_days =[]\n",
    "                for date in df[col]:\n",
    "                    try:\n",
    "                        delta_future_days.append(int((dateutil.parser.parse(date).date() - today).days))\n",
    "                    except:\n",
    "                        delta_future_days.append(0)\n",
    "                df[delta_future_days_col] = delta_future_days     \n",
    "                #calculating z score\n",
    "                delta_future_day_values = df[df[delta_future_days_col] != 0][delta_future_days_col]\n",
    "                sd_delta_future_days = stdev(delta_future_day_values)\n",
    "                avg_delta_future_days = sum(delta_future_day_values) / len(delta_future_day_values)\n",
    "                z_delta_future_days = []\n",
    "                for v in df[delta_future_days_col]:\n",
    "                    if v == 0:\n",
    "                        z_delta_future_days.append(0)\n",
    "                    else:\n",
    "                        z_delta_future_days.append(((v-avg_delta_future_days) / sd_delta_future_days) + (3-n))\n",
    "                df[z_score_col] = z_delta_future_days\n",
    "                n=n+1   \n",
    "        else:\n",
    "            df['z_score_delta_future_days1'] = 0\n",
    "            \n",
    "        if 'z_score_delta_future_days2' not in df:\n",
    "            df['z_score_delta_future_days2'] = 0\n",
    "            \n",
    "        if 'transaction_count' in df:\n",
    "            df['transaction_count'] = df['transaction_count'].fillna(0).replace('',0).astype(int)\n",
    "            #df['transaction_count'] = df['transaction_count'].astype(int)\n",
    "            max_transactions = df.transaction_count.max(skipna=True)\n",
    "            transactions = []\n",
    "            for t in df.transaction_count:\n",
    "                try:\n",
    "                    transactions.append(t / max_transactions)\n",
    "                except:\n",
    "                    transactions.append(0)\n",
    "            df['transaction_score'] = transactions\n",
    "            transaction_score_values = df[df['transaction_score']>0]['transaction_score']\n",
    "            sd_transactions = stdev(transaction_score_values)\n",
    "            avg_transactions = sum(transaction_score_values) / len(transaction_score_values)\n",
    "            z_transactions = []\n",
    "            for v in df['transaction_score']:\n",
    "                if v == 0:\n",
    "                    z_transactions.append(0)\n",
    "                else:\n",
    "                    z_transactions.append(((v - avg_transactions) / sd_transactions) + 5)\n",
    "            df['z_score_transactions'] = z_transactions\n",
    "        else:\n",
    "            df['z_score_transactions'] = 0\n",
    "\n",
    "        #summing any validity penalties from invalid data (postal addresses, bad domains, etc.)\n",
    "        valid_penalties = []\n",
    "        for index, row in df[cbis_valid_cols].iterrows():\n",
    "            row_valid_score = 0\n",
    "            for col in cbis_valid_cols:\n",
    "                try:\n",
    "                    if row[col] == 'invalid':\n",
    "                        row_valid_score = row_valid_score -1\n",
    "                except:\n",
    "                    row_valid_score\n",
    "            valid_penalties.append(row_valid_score)\n",
    "        df['valid_penalties'] = valid_penalties  \n",
    "        \n",
    "        #summing validity scores for emails\n",
    "        e_lkup = {1:1,2:.5,3:0,4:-2} #I think I don't want a 2 and a 3 together to beat a 1 by itself\n",
    "        email_scores = []\n",
    "        for index, row in df[email_val_cols].iterrows():\n",
    "            email_val_score = 0\n",
    "            for col in email_val_cols:\n",
    "                try:\n",
    "                    email_val_score = email_val_score + e_lkup[int(row[col][0])]\n",
    "                except:\n",
    "                    email_val_score\n",
    "            email_scores.append(email_val_score)\n",
    "        df['email_scores'] = email_scores\n",
    "        \n",
    "        #summing validity scores for postal addresses\n",
    "        p_lkup = {1:1,2:.7,3:.25,4:-.5,5:-2} #I think I don't want a 2 and a 3 together to beat a 1 by itself\n",
    "        postal_scores = []\n",
    "        for index, row in df[postal_val_cols].iterrows():\n",
    "            postal_val_score = 0\n",
    "            for col in postal_val_cols:\n",
    "                try:\n",
    "                    postal_val_score = postal_val_score + p_lkup[int(row[col][0])]\n",
    "                except:\n",
    "                    postal_val_score\n",
    "            postal_scores.append(postal_val_score)\n",
    "        df['postal_scores'] = postal_scores\n",
    "        \n",
    "        #change of address penalties\n",
    "        coa_scores = []\n",
    "        for index, row in df[coa_cols].iterrows():\n",
    "            coa_val_score = 0\n",
    "            for col in coa_cols:\n",
    "                try:\n",
    "                    if row[col] == 'y':\n",
    "                        coa_val_score = coa_val_score - .5\n",
    "                except:\n",
    "                    coa_val_score\n",
    "            coa_scores.append(coa_val_score)\n",
    "        df['coa_scores'] = coa_scores\n",
    "        \n",
    "        #use whatever columns we are considering for matching as queen columns - columns we use to assess completeness\n",
    "        #or, should we be using ALL columns?\n",
    "        queen_cols = []\n",
    "        cols_df = pd.DataFrame(list(match_items.items()),columns=['key','value'])\n",
    "        for k, v in zip(cols_df.key,cols_df.value):\n",
    "            if (v != None): #& (k != v): #we previously had & (k != v) but I'm not sure why, removing this\n",
    "                queen_cols.append(k)\n",
    "        queen_cols.remove('id')\n",
    "        \n",
    "        chars_totals = []\n",
    "        for index, row in df[queen_cols].iterrows():\n",
    "            chars = 0\n",
    "            for col in queen_cols:\n",
    "                try:\n",
    "                    chars = chars + len(row[col])\n",
    "                except:\n",
    "                    chars\n",
    "            chars_totals.append(chars * .001)\n",
    "        df['chars'] = chars_totals\n",
    "        \n",
    "        field_counts = []\n",
    "        for index, row in df[queen_cols].iterrows():\n",
    "            fields = 0\n",
    "            for col in queen_cols:\n",
    "                try:\n",
    "                    if len(row[col]) > 0:\n",
    "                        fields = fields + 1\n",
    "                except:\n",
    "                    fields\n",
    "            field_counts.append(fields / len(queen_cols))\n",
    "        df['fields'] = field_counts\n",
    "        \n",
    "        #use index to virtually ensure that no two queen scores will be equal\n",
    "        index_values = []\n",
    "        for i in range(0,df.shape[0]):\n",
    "            index_values.append((df.shape[0] - i) / (df.shape[0]*10))\n",
    "        df['index_queen_score'] = index_values\n",
    "        \n",
    "        #ad hoc queening criteria can be manually calculated and added here\n",
    "        if 'priority_flag' in df:\n",
    "            priority_scores = []\n",
    "            for item in df['priority_flag']:\n",
    "                try:\n",
    "                    if item == '1':\n",
    "                        priority_scores.append(100)\n",
    "                    else:\n",
    "                        priority_scores.append(0)\n",
    "                except:\n",
    "                    priority_scores.append(0)\n",
    "            df['priority_flag'] = priority_scores\n",
    "        else:\n",
    "            df['priority_flag'] = 0\n",
    "            \n",
    "        if 'priority1' not in df:\n",
    "            df['priority1'] = 0\n",
    "        else:\n",
    "            df['priority1'] = pd.to_numeric(df['priority1'], errors='coerce')\n",
    "            df['priority1'] = df['priority1'].fillna(0).astype(float)\n",
    "            \n",
    "        if 'priority2' not in df:\n",
    "            df['priority2'] = 0\n",
    "        else:\n",
    "            df['priority2'] = pd.to_numeric(df['priority2'], errors='coerce')\n",
    "            df['priority2'] = df['priority2'].fillna(0).astype(float)\n",
    "            \n",
    "        if 'priority3' not in df:\n",
    "            df['priority3'] = 0\n",
    "        else:\n",
    "            df['priority3'] = pd.to_numeric(df['priority3'], errors='coerce')\n",
    "            df['priority3'] = df['priority3'].fillna(0).astype(float)       \n",
    "            \n",
    "        if 'priority4' not in df:\n",
    "            df['priority4'] = 0\n",
    "        else:\n",
    "            df['priority4'] = pd.to_numeric(df['priority4'], errors='coerce')\n",
    "            df['priority4'] = df['priority4'].fillna(0).astype(float)        \n",
    "            \n",
    "        if 'priority5' not in df:\n",
    "            df['priority5'] = 0\n",
    "        else:\n",
    "            df['priority5'] = pd.to_numeric(df['priority5'], errors='coerce')\n",
    "            df['priority5'] = df['priority5'].fillna(0).astype(float)  \n",
    "            \n",
    "        if 'priority6' not in df:\n",
    "            df['priority6'] = 0\n",
    "        else:\n",
    "            df['priority6'] = pd.to_numeric(df['priority6'], errors='coerce')\n",
    "            df['priority6'] = df['priority6'].fillna(0).astype(float)  \n",
    "            \n",
    "        if 'priority7' not in df:\n",
    "            df['priority7'] = 0\n",
    "        else:\n",
    "            df['priority7'] = pd.to_numeric(df['priority7'], errors='coerce')\n",
    "            df['priority7'] = df['priority7'].fillna(0).astype(float)  \n",
    "            \n",
    "        if 'priority8' not in df:\n",
    "            df['priority8'] = 0\n",
    "        else:\n",
    "            df['priority8'] = pd.to_numeric(df['priority8'], errors='coerce')\n",
    "            df['priority8'] = df['priority8'].fillna(0).astype(float)  \n",
    "            \n",
    "        if 'priority9' not in df:\n",
    "            df['priority9'] = 0\n",
    "        else:\n",
    "            df['priority9'] = pd.to_numeric(df['priority9'], errors='coerce')\n",
    "            df['priority9'] = df['priority9'].fillna(0).astype(float)  \n",
    "            \n",
    "        if 'priority10' not in df:\n",
    "            df['priority10'] = 0\n",
    "        else:\n",
    "            df['priority10'] = pd.to_numeric(df['priority10'], errors='coerce')\n",
    "            df['priority10'] = df['priority10'].fillna(0).astype(float)  \n",
    "            \n",
    "        if 'priority11' not in df:\n",
    "            df['priority11'] = 0\n",
    "        else:\n",
    "            df['priority11'] = pd.to_numeric(df['priority11'], errors='coerce')\n",
    "            df['priority11'] = df['priority11'].fillna(0).astype(float)  \n",
    "            \n",
    "        if 'priority12' not in df:\n",
    "            df['priority12'] = 0\n",
    "        else:\n",
    "            df['priority12'] = pd.to_numeric(df['priority12'], errors='coerce')\n",
    "            df['priority12'] = df['priority12'].fillna(0).astype(float)  \n",
    "\n",
    "        #calculating composite queen score\n",
    "        try:\n",
    "            df['queen_score'] = df['z_score_delta_days1'] + df['z_score_delta_days2'] + df['z_score_transactions'] \\\n",
    "            + df['z_score_delta_future_days1'] + df['z_score_delta_future_days2'] \\\n",
    "            + df['valid_penalties'] + df['email_scores'] + df['postal_scores'] + df['coa_scores'] + df['chars'] + df['fields'] \\\n",
    "            + df['priority_flag'] + df['priority1'] + df['priority2'] + df['priority3'] + df['priority4']  + df['priority5'] \\\n",
    "            + df['priority6'] + df['priority7'] + df['priority8'] + df['priority9'] + df['priority10'] + df['priority11'] + df['priority12'] + df['index_queen_score']\n",
    "        except:\n",
    "            df['queen_score'] = df['chars'] + df['fields'] + df['index_queen_score']\n",
    "            print('WARNING: Queen Score Failure - check calculation error')\n",
    "        \n",
    "        df['queen_score'] = df['queen_score'].astype(float)\n",
    "        \n",
    "        #I need to account for matching, probably need to re-think how queen cols are determined.  We would need to take\n",
    "        #only coluns that are in both datasets.  or arguably, and more simply, just base on completeness factors.\n",
    "        \n",
    "        ##############END QUEEN SCORE CALCULATIONS##############\n",
    "\n",
    "        global a #this is a marker used to determine if we are working on alt data\n",
    "        \n",
    "        # Standard data read prep\n",
    "        df = df.fillna('')\n",
    "        \n",
    "        #creating col lists\n",
    "        state_cols = []\n",
    "        for col in ['state_a','state_b', 'state_c','state_d']:\n",
    "            if col in df:\n",
    "                state_cols.append(col)\n",
    "        phone_cols = []\n",
    "        for col in ['phone_a','phone_b','phone_c','fax_a']:\n",
    "            if col in df:\n",
    "                phone_cols.append(col)\n",
    "        email_cols = []\n",
    "        for col in ['email_a','email_b','email_c','email_d']:\n",
    "            if col in df:\n",
    "                email_cols.append(col)\n",
    "        website_cols = []\n",
    "        for col in ['website_a','website_b','website_c','website_d']:\n",
    "            if col in df:\n",
    "                website_cols.append(col)\n",
    "        org_cols = []\n",
    "        for col in ['org_name','org_name2','org_name3']:\n",
    "            if col in df:\n",
    "                org_cols.append(col)\n",
    "                \n",
    "        # Normalize state codes\n",
    "        for col in state_cols:\n",
    "            df[col] = df[col].replace(regex=place_acronyms)\n",
    "        for col in org_cols:\n",
    "            df[col] = df[col].replace(regex=place_acronyms)\n",
    "\n",
    "        # Identify nickname groups\n",
    "        if 'fname' in df and not organization_only:\n",
    "            df[\"nicks_groups\"] = df[\"fname\"].apply(lambda n: \" \".join(map(lambda grp: \"nick\" + str(grp), nicks[n])))\n",
    "            # Include phoneticization\n",
    "            df['meta_fname'] = df['fname'].apply(metaphone)\n",
    "        \n",
    "        if 'lname' in df and not organization_only:\n",
    "            df['meta_lname'] = df['lname'].apply(metaphone)\n",
    "        \n",
    "        #create full_name col to later use to assess name uniqueness\n",
    "        if 'fname' in df and 'lname' in df and not organization_only:\n",
    "            df['full_name'] = df['fname'] + ' ' + df['lname']\n",
    "            if 'full_name' not in match_items.keys():\n",
    "                match_items['full_name'] = 'full_name'\n",
    "            if a==1:\n",
    "                if 'full_name' not in cols_alt.keys():\n",
    "                    cols_alt['full_name'] = 'full_name' \n",
    "                   \n",
    "        #predict gender\n",
    "        if 'fname' in df and not organization_only:\n",
    "            gender_cols = []\n",
    "            for col in df.columns:\n",
    "                if col in ['gender','prefix','fname','mname']:\n",
    "                    gender_cols.append(col)\n",
    "            import gender_guesser.detector as gender\n",
    "            gender_pred = gender.Detector(case_sensitive=False)\n",
    "            if 'gender' in gender_cols:\n",
    "                gender_dict = {\n",
    "                    male_value:-5,\n",
    "                    female_value:5\n",
    "                }\n",
    "            gender_name_dict = {\n",
    "                'male':-2,\n",
    "                'mostly_male':-1,\n",
    "                'andy':0,\n",
    "                'unknown':0,\n",
    "                'mostly_female':1,\n",
    "                'female':2\n",
    "            }\n",
    "            bb_gender = []\n",
    "            for index, row in df.iterrows():\n",
    "                g = 0\n",
    "                for col in gender_cols:\n",
    "                    if (col == 'gender') and (row[col] in [male_value,female_value]): #check any pre-existing gender values\n",
    "                        g = g + gender_dict[row[col]]\n",
    "                    if (col == 'prefix') and (len(str(row[col])))>0:\n",
    "                        prefix_values = row[col].replace('.','').replace(',','').lower().split()\n",
    "                        if 'mr' in prefix_values:\n",
    "                            g = g-5\n",
    "                        elif len(set.intersection(set(prefix_values),['ms','mrs','miss']))>0:\n",
    "                            g = g+5\n",
    "                    if col in ['fname','mname']:\n",
    "                        for name in row[col].split():\n",
    "                            g = g + gender_name_dict[gender_pred.get_gender(name)]\n",
    "                if g<0:\n",
    "                    bb_gender.append('male')\n",
    "                elif g>0:\n",
    "                    bb_gender.append('female')\n",
    "                else:\n",
    "                    bb_gender.append(np.nan)\n",
    "            df['bb_gender'] = bb_gender\n",
    "            if 'bb_gender' not in match_items.keys():\n",
    "                match_items['bb_gender'] = 'bb_gender'\n",
    "                \n",
    "        # Reduce phone numbers to just numbers\n",
    "        for col in phone_cols:\n",
    "            col_name = 'clean_' + col\n",
    "            df[col_name] = df[col].replace('[^0-9]', '',regex=True)\n",
    "            if col_name not in match_items.keys():\n",
    "                match_items[col_name] = match_items[col] + '_clean'\n",
    "            if a==1:\n",
    "                if col_name not in cols_alt.keys():\n",
    "                    cols_alt[col_name] = cols_alt[col] + '_clean'\n",
    "\n",
    "        # Split emails into usernames and domains\n",
    "        for col in email_cols:\n",
    "            before_domain_col = 'before_domain_' + col[-1:]\n",
    "            domain_col = 'email_domain_' + col[-1:]\n",
    "            df[[before_domain_col, domain_col]] = df[col].str.split('@', expand=True)[[0, 1]]\n",
    "            # Drop all domains that are just personal domains \n",
    "            df[domain_col] = df[domain_col].map(lambda d: '' if d in list(personal_domains) else d)\n",
    "            #df['domain'] = df['email_domain']\n",
    "            if before_domain_col not in match_items.keys():\n",
    "                match_items[before_domain_col] = match_items[col] + '_before_domain'\n",
    "            if domain_col not in match_items.keys():\n",
    "                match_items[domain_col] = match_items[col] + '_domain'\n",
    "            if a==1:\n",
    "                if before_domain_col not in cols_alt.keys():\n",
    "                    cols_alt[before_domain_col] = cols_alt[col] + '_before_domain'\n",
    "                if domain_col not in cols_alt.keys():\n",
    "                    cols_alt[domain_col] = cols_alt[col] + '_domain'\n",
    "        \n",
    "        for col in website_cols:\n",
    "            domain_col = 'web_domain_' + col[-1:]\n",
    "            domains = []\n",
    "            for web in df[col]:\n",
    "                if '@' in web: #handle domain extraction in case we want to use an email address\n",
    "                    domain = web.split('@')[-1]\n",
    "                    if domain in list(personal_domains): \n",
    "                        domains.append('')\n",
    "                    else:\n",
    "                        domains.append(domain)\n",
    "                elif '/' in web or 'www' in web: #assuming we have a URL\n",
    "                    domains.append(web.split('//')[-1].split('/')[0].strip('www.'))\n",
    "                elif '.' in web: #handling cases where URL is already a domain\n",
    "                    domains.append(web)\n",
    "                else:\n",
    "                    domains.append('')\n",
    "            df[domain_col] = domains\n",
    "            if domain_col not in match_items.keys():\n",
    "                match_items[domain_col] = match_items[col] + '_domain'\n",
    "            if a==1:\n",
    "                if domain_col not in cols_alt.keys():\n",
    "                    cols_alt[domain_col] = cols_alt[col] + '_domain'\n",
    "\n",
    "        return df\n",
    "\n",
    "    process_chunks(pd.read_csv(ns(in_file), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                               dtype=object, index_col='id'),\n",
    "                   _preprocess,\n",
    "                   ns(out_file))\n",
    "\n",
    "    print(\"states, phones, domains normalized --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "if matching:\n",
    "    preprocess(\"data_l.csv\", \"processed_l.csv\", match_items=cols)\n",
    "    a=1 #marker to indicate that we are working on alt data\n",
    "    preprocess(\"data_r.csv\", \"processed_r.csv\", match_items=cols_alt)\n",
    "else:\n",
    "    preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset\n",
    "\n",
    "Take our dataset and count up all the tokens inside to create our tokenlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(cell):\n",
    "    \"\"\"\n",
    "    Reduces cell string contents to lowercase\n",
    "    alphanumeric characters, then splits into a list on space.\n",
    "    \n",
    "    So given one string \"  Aaa bBb ccC \", this returns [\"aaa\", \"bbb\", \"ccc\"]\n",
    "    \"\"\"\n",
    "    if cell is None: return cell\n",
    "    # For every lowercased word (from .lower.split), filter out non-alphanumeric chars\n",
    "    # and then ''.join it back together to get a list of tokens.\n",
    "    return map(lambda string: ''.join(filter(str.isalnum, string)), re.split('\\W+',cell.lower()))\n",
    "\n",
    "def extract_tokens(df):\n",
    "    \"\"\"\n",
    "    A chunk processing function.\n",
    "    For each chunk, select all relevant columns\n",
    "        -> df.columns.isin(...) chooses all the columns in df that are\n",
    "           included in the given list\n",
    "    and use applymap to process every single cell\n",
    "        -> applymap(lambda cell: ...)\n",
    "    so that, for each cell, we 1) tokenize it to get a list, and then\n",
    "    2) update the counter with them with tokens.update(...)\n",
    "    \n",
    "    FYI: Counter is a dict(). Counter.update updates the counts for the\n",
    "    passed in values, e.g. Counter.update([1,2,3,2,1]) -> {1: 2, 2: 2, 3: 1}\n",
    "    \"\"\"\n",
    "    df = df.fillna('')\n",
    "    df.loc[:, df.columns.isin(full_token_columns)] \\\n",
    "        .applymap(lambda cell: tokens.update(tokenize(cell)))\n",
    "\n",
    "# lowercase the name and split on spaces, remove non-alphanumeric chars\n",
    "tokens = Counter()\n",
    "\n",
    "# These calls to process_chunks don't write to a file; their purpose is filling\n",
    "# up the tokens Counter dicitonary.\n",
    "if matching:\n",
    "    process_chunks(pd.read_csv(ns(\"processed_l.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                           dtype=object, index_col='id'),\n",
    "               extract_tokens)\n",
    "    process_chunks(pd.read_csv(ns(\"processed_r.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                           dtype=object, index_col='id'),\n",
    "               extract_tokens)\n",
    "else:\n",
    "    process_chunks(pd.read_csv(ns(\"processed.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                               dtype=object, index_col='id'),\n",
    "                   extract_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_dict = dict(tokens)\n",
    "tok_df = pd.DataFrame(list(token_dict.values()), index=token_dict.keys(), columns=['count'])\n",
    "tok_df.reset_index(inplace=True)\n",
    "tok_df.rename(columns={'index':'token'},inplace=True)\n",
    "# Filter out single-instance tokens\n",
    "tok_df = tok_df.loc[tok_df['count'] > 1]\n",
    "# Filter out tokens over the commonality threshold set by token_limiter\n",
    "tok_df = tok_df.sort_values('count')\n",
    "tok_df = tok_df.reset_index(drop=True)\n",
    "stopwords = list(tok_df[tok_df.index >= (len(tok_df) * token_limiter)].token.drop_duplicates()) #consider using for CountVectorizer?\n",
    "tok_df = tok_df[tok_df.index < (len(tok_df) * token_limiter)]\n",
    "\n",
    "# CountVectorizer creates a frequency count of every token in the vocab. We feed it the whole\n",
    "# dataframe (constructed by str.join(' ')-ing the whole frame as a list of strings) to get a\n",
    "# frequency count of all the tokens per row.\n",
    "cv = CountVectorizer(binary=True)\n",
    "vocab = np.array(list(map(cv.build_preprocessor(), tok_df['token'].unique())), dtype=object)\n",
    "cv.set_params(vocabulary=vocab)\n",
    "\n",
    "def count_freqs(df):\n",
    "    token_cols = []\n",
    "    for t in full_token_columns:\n",
    "        if t in df:\n",
    "            token_cols.append(t)\n",
    "    tokenmapping = cv.fit_transform(pd.Series(df[token_cols].fillna('').values.tolist()).str.join(' '))\n",
    "    row_index, vocab_index = tokenmapping.nonzero()\n",
    "\n",
    "    tokenids = pd.DataFrame({\"token\": vocab[vocab_index],\n",
    "                             \"token_id\": vocab_index,\n",
    "                             \"id\": df.index.values[row_index]})\n",
    "    tokenids = tokenids.drop_duplicates()\n",
    "    return tokenids\n",
    "\n",
    "if matching:\n",
    "    process_chunks(pd.read_csv(ns(\"processed_l.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                               dtype=object, index_col='id'),\n",
    "                   count_freqs,\n",
    "                   ns(\"tokenlists_l.csv\"),\n",
    "                   index=False)\n",
    "    process_chunks(pd.read_csv(ns(\"processed_r.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                               dtype=object, index_col='id'),\n",
    "                   count_freqs,\n",
    "                   ns(\"tokenlists_r.csv\"),\n",
    "                   index=False)\n",
    "else:\n",
    "    process_chunks(pd.read_csv(ns(\"processed.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                               dtype=object, index_col='id'),\n",
    "                   count_freqs,\n",
    "                   ns(\"tokenlists.csv\"),\n",
    "                   index=False)\n",
    "\n",
    "# Calculate bonus tokens by getting token list from tok_df, then turning\n",
    "# tokenids into a token -> tok_id dictionary and translating the bonus\n",
    "# tokens with .map(dict)\n",
    "# Clean it up by dropping NAs, making it int type instead of float, and\n",
    "# grab just the values as a numpy.array\n",
    "bonus_toks = tok_df['token'][tok_df['count'] <= unique_token_freq_max] \\\n",
    "                 .map(vocab.tolist().index) \\\n",
    "                 .dropna() \\\n",
    "                 .astype(int) \\\n",
    "                 .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Candidates\n",
    "\n",
    "Take the tokenlists and join them chunk by chunk until a candidate list is formed. Then add all the feature data back in once the ID's for the candidates are determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SELECTING CANDIDATES')\n",
    "start_time = time.time()\n",
    "\n",
    "'''\n",
    "#old way of creating pair, we retired because it seemed to be creating improper pair IDs \n",
    "#that then led to missing valid candidates\n",
    "\n",
    "def unique_id_pairs(df, id_l='id_l', id_r='id_r', sep=\":\"):\n",
    "    \"\"\"\n",
    "    Given a dataframe with the given two ID columns, creates a single ID \"footprint\" such that\n",
    "    IDA, IDB and IDB, IDA have the same footprint. Basically IDA and IDB are sorted alphabetically\n",
    "    and then joined with the separator.\n",
    "    \n",
    "    e.g. {id_l: 10, id_r: 20, sep: \":\"} -> \"10:20\", and {id_l: 20, id_r: 10, sep: \":\"} -> \"10:20\"\n",
    "    \"\"\"\n",
    "    # df[[id_l, id_r]].to_numpy(copy=True)\n",
    "    #     Take the two columns and convert them to numpy arrays, to get [[id_l1, id_r1], [id_l2, id_r2], ...]\n",
    "    # np.sort(...)\n",
    "    #     Sort each ID pair in the list, so that each sublist pair is in alphabetical order\n",
    "    # apply_along_axis(sep.join, -1, _)\n",
    "    #     For each sublist pair, join them together with the separator\n",
    "    return np.apply_along_axis(sep.join, -1, np.sort(df[[id_l, id_r]].to_numpy(copy=True)))\n",
    "'''\n",
    "# Change where the rightside comes from depending on if we're matching\n",
    "tokens_right = pd.read_csv(ns('tokenlists_r.csv' if matching else 'tokenlists.csv'),encoding='utf-8',compression=compression,\n",
    "                           usecols=['id', 'token_id'], dtype={'id': object, 'token_id': int}) \\\n",
    "                 .set_index('token_id')\n",
    "\n",
    "p=0\n",
    "def select_candidates(chunk):\n",
    "    global p\n",
    "    global pairs\n",
    "    joined = chunk.set_index('token_id').join(tokens_right, how='inner', lsuffix='_l', rsuffix='_r')\n",
    "\n",
    "    # Get all rows that are bonus tokens, then copy them and add them to the joined again\n",
    "    bonuses = np.intersect1d(bonus_toks, joined.index.values)\n",
    "    joined = pd.concat([joined, joined.loc[bonuses].copy()])\n",
    "\n",
    "    match_count = joined.groupby(['id_l', 'id_r']).size()\n",
    "    match_count = match_count[match_count >= token_match_min]\n",
    "\n",
    "    if not matching:\n",
    "        # Get rid of self matches\n",
    "        candidates = match_count.reset_index().drop(0, axis=\"columns\")\n",
    "        candidates = candidates[candidates['id_l'] != candidates['id_r']]\n",
    "\n",
    "        # Get rid of flip-flopped duplicate matches\n",
    "        #candidates = candidates.assign(pair=unique_id_pairs(candidates)) #old way of deriving pair\n",
    "        candidates['pair'] = candidates.apply(lambda row: ':'.join(sorted([row.id_l,row.id_r])), axis = 1)\n",
    "        candidates = candidates.drop_duplicates(\"pair\")\n",
    "        if p==0:\n",
    "            pairs = candidates[['pair']]\n",
    "            p=1\n",
    "        else:\n",
    "            candidates = candidates[~candidates['pair'].isin(pairs.pair)]\n",
    "            pairs = pd.concat([pairs,candidates[['pair']]],ignore_index=True)\n",
    "            pairs = pairs.drop_duplicates('pair')\n",
    "        candidates = candidates.reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        candidates = match_count.reset_index().drop(0, axis=\"columns\")\n",
    "        # Get rid of flip-flopped duplicate matches\n",
    "        #candidates = candidates.assign(pair=unique_id_pairs(candidates)) #old way of deriving pair\n",
    "        candidates['pair'] = candidates.apply(lambda row: ':'.join(sorted([row.id_l,row.id_r])), axis = 1)\n",
    "        candidates = candidates.drop_duplicates(\"pair\")\n",
    "        if p==0:\n",
    "            pairs = candidates[['pair']]\n",
    "            p=1\n",
    "        else:\n",
    "            candidates = candidates[~candidates['pair'].isin(pairs.pair)]\n",
    "            pairs = pd.concat([pairs,candidates[['pair']]],ignore_index=True)\n",
    "            pairs = pairs.drop_duplicates('pair')\n",
    "        candidates = candidates.reset_index(drop=True)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "#print(\"Expected chunks:\")\n",
    "#print(int(pd.read_csv(ns('tokenlists_l.csv' if matching else 'tokenlists.csv')).shape[0] / chunk_size ** 0.5))\n",
    "#print('')\n",
    "process_chunks(pd.read_csv(ns('tokenlists_l.csv' if matching else 'tokenlists.csv'),encoding='utf-8',compression=compression,\n",
    "                           chunksize=chunk_size ** 0.5, usecols=['id', 'token_id'],\n",
    "                           dtype={'id': object, 'token_id': int}),\n",
    "               select_candidates,\n",
    "               ns(\"candidates.csv\"),\n",
    "               index=False)\n",
    "print(\"Candidates selected in\", time.time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs)\n",
    "#consider using tuple rather than string pairs to conserve memory, I think it is more efficient\n",
    "#but need to confirm that we can use tuples the same way that pairs are being used at present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while this is clear in my head\n",
    "#there is a problem here wherein duplicate candidate pairs are not being removed due to being outside of a given chunk (I tink)\n",
    "#reason is we continuously match against the full tokenlist so same unique pair may be added again in a different chunk\n",
    "#potential solutions:\n",
    "#1. store complete list of unique pairs in memory and check against it <--going with this for now\n",
    "#2. dynamically slide down in tokenlist index so we don't re-compare the same two records again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print (\"MERGING MATCH CANDIDATES WITH ORIGINAL DATA...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "if matching:\n",
    "    left_cols = list(map(lambda col: col + '_l', cols.keys()))\n",
    "    left_df = pd.read_csv(ns(\"processed_l.csv\"), encoding='utf-8',compression=compression\n",
    "                          ,dtype=object) \\\n",
    "                .set_index('id') \\\n",
    "                .fillna('') \\\n",
    "                .rename(columns=dict(zip(cols.keys(), left_cols)))\n",
    "    left_df = left_df.loc[:, left_df.columns.isin(left_cols)]\n",
    "\n",
    "    right_cols = list(map(lambda col: col + '_r', cols_alt.keys()))\n",
    "    right_df = pd.read_csv(ns(\"processed_r.csv\"), encoding='utf-8',compression=compression\n",
    "                           ,dtype=object) \\\n",
    "                .set_index('id') \\\n",
    "                .fillna('') \\\n",
    "                .rename(columns=dict(zip(cols_alt.keys(), right_cols)))\n",
    "    right_df = right_df.loc[:, right_df.columns.isin(right_cols)]\n",
    "else:\n",
    "    left_cols = list(map(lambda col: col + '_l', cols.keys()))\n",
    "    right_cols = list(map(lambda col: col + '_r', cols.keys()))\n",
    "\n",
    "    df = pd.read_csv(ns(\"processed.csv\"), encoding='utf-8',compression=compression\n",
    "                     ,dtype=object).set_index('id').fillna('')\n",
    "    # Create a left_df and a right_df that consist of the columns dictated by cols\n",
    "    # that was set in the parameter setting part at the top of the program\n",
    "    left_df = df.rename(columns=dict(zip(cols.keys(), left_cols)))\n",
    "    left_df = left_df.loc[:, left_df.columns.isin(left_cols)]\n",
    "    right_df = df.rename(columns=dict(zip(cols.keys(), right_cols)))\n",
    "    right_df = right_df.loc[:, right_df.columns.isin(right_cols)]\n",
    "\n",
    "def merge_data(chunk):\n",
    "    candidata = pd.merge(chunk, left_df,\n",
    "                         left_on='id_l', right_index=True)\n",
    "    candidata = pd.merge(candidata, right_df,\n",
    "                         left_on='id_r', right_index=True)\n",
    "    return candidata\n",
    "\n",
    "#print(\"Expected chunks:\")\n",
    "#print(int(pd.read_csv(ns('candidates.csv')).shape[0] / chunk_size ))\n",
    "#print('')\n",
    "process_chunks(pd.read_csv(ns('candidates.csv'),encoding='utf-8',compression=compression,\n",
    "                           chunksize=small_chunk_size, dtype=object), #small chunk size because this cell is doing a large merge\n",
    "               merge_data,\n",
    "               ns(\"candidata.csv\"),index=False)\n",
    "\n",
    "print(\"original data merged with matches --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "Scoring is done in three mostly arbitrary chunks.\n",
    "1. Organization name simularity\n",
    "    - This is done with Jaro-Winkler and three fuzz-type tests\n",
    "2. Organization name sequence uniqueness\n",
    "3. Various field similarities\n",
    "    - names, states, cities, zips, etc.\n",
    "    - These depend on the presence of the required columns in the data in the first place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print (\"SCORING ORG, PERSON NAME SIMULARITY...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on edit distance of org names\n",
    "def score_compare(left, right, method):\n",
    "    \"\"\"\n",
    "    Given two strings, returns the comparison score based on the specified methodology.\n",
    "    \"\"\"\n",
    "    if len(left) > 0 and len(right) > 0:\n",
    "        if (method == \"jaro\"): return jaro_winkler(left, right)\n",
    "        if (method == \"fuzz-partial\"): return fuzz.partial_ratio(left, right) / 100\n",
    "        if (method == \"fuzz-sort\"): return fuzz.token_sort_ratio(left, right) / 100\n",
    "        if (method == \"fuzz-set\"): return fuzz.token_set_ratio(left, right) / 100\n",
    "        raise ValueError(\"Method {} unknown; \" \\\n",
    "                         \"must be 'jaro', 'fuzz-partial', 'fuzz-sort' or 'fuzz-set'\".format(method))\n",
    "    else: return np.nan\n",
    "\n",
    "def name_match(left, right):\n",
    "    left_isalnum = ''.join(e for e in left if e.isalnum()) #removing spaces and special characters from left and right\n",
    "    right_isalnum = ''.join(e for e in right if e.isalnum())\n",
    "    if len(left_isalnum) > 0 and len(right_isalnum) > 0:\n",
    "        return score_compare(left_isalnum, right_isalnum, 'jaro')\n",
    "    #elif len(left) > 0 and len(right) > 0: #we may as well still score two orgs named '$$#' and '#$$'\n",
    "        #return score_compare(left, right, 'jaro')\n",
    "    else: return np.nan\n",
    "\n",
    "def get_field(data, field, left_suffix='_l', right_suffix='_r'):\n",
    "    \"\"\"\n",
    "    For a list of series, this function returns a list of fields from those series.\n",
    "    So field_of('name', [row_bob, row_joe]) might return [\"Bob\", \"Joe\"].\n",
    "    \"\"\"\n",
    "    return (data[field + left_suffix], data[field + right_suffix])\n",
    "\n",
    "def get_org_data(data, left_suffix='_l', right_suffix='_r'):\n",
    "    #handle gathering and concatenating of org name-type data\n",
    "    \n",
    "    left_cols = []\n",
    "    right_cols = []\n",
    "    for org in ['org_name','org_name2','org_name3']:\n",
    "        if org + left_suffix in data:\n",
    "            left_cols.append(org + left_suffix)\n",
    "        if org + right_suffix in data:\n",
    "            right_cols.append(org + right_suffix)\n",
    "    return (' '.join(data[left_cols]), ' '.join(data[right_cols]))\n",
    "    \n",
    "def dedup_list(x):\n",
    "    return list(dict.fromkeys(x))\n",
    "\n",
    "def get_multi_field(data, field, left_suffix='l', right_suffix='r'):\n",
    "    \"\"\"\n",
    "    handle data types with multiple per record, ex: phones, addresses\n",
    "    \"\"\"\n",
    "    left_cols = []\n",
    "    right_cols = []\n",
    "    for t in ['_a_','_b_','_c_','_d_']:\n",
    "        if field + t + left_suffix in data:\n",
    "            left_cols.append(field + t + left_suffix)\n",
    "        if field + t + right_suffix in data:\n",
    "            right_cols.append(field + t + right_suffix)\n",
    "    return (dedup_list(data[left_cols]),dedup_list(data[right_cols]))\n",
    "\n",
    "def get_domain_fields(data, email, web, left_suffix='l',right_suffix='r'):\n",
    "    '''\n",
    "    handle domains which are special case as they come from both email and website fields\n",
    "    '''\n",
    "    left_cols = []\n",
    "    right_cols = []\n",
    "    for t in ['_a_','_b_','_c_','_d_']:\n",
    "        if email + t + left_suffix in data:\n",
    "            left_cols.append(email + t + left_suffix)\n",
    "        if web + t + left_suffix in data:\n",
    "            left_cols.append(web + t + left_suffix)\n",
    "        if email + t + right_suffix in data:\n",
    "            right_cols.append(email + t + right_suffix)\n",
    "        if web + t + right_suffix in data:\n",
    "            right_cols.append(web + t + right_suffix)\n",
    "    return (dedup_list(data[left_cols]),dedup_list(data[right_cols]))\n",
    "\n",
    "'''#removing as this is redundant with how we now handle gender\n",
    "personal_prefixes = ['mr','mrs','ms','miss']\n",
    "def prefix_match(left, right):\n",
    "    if len(left) > 0 and len(right) > 0:\n",
    "        l_prefix = left.replace('.','').replace(',','').split()\n",
    "        l_personal_prefix = set.intersection(set(l_prefix),personal_prefixes)\n",
    "        if len(l_personal_prefix) > 0:\n",
    "            r_prefix = right.replace('.','').replace(',','').split()\n",
    "            r_personal_prefix = set.intersection(set(r_prefix),personal_prefixes)\n",
    "            if len(r_personal_prefix) > 0: #so now we at least know there is personal prefixes on both sides\n",
    "                if ('mr' in l_personal_prefix and 'mr' not in r_personal_prefix) | ('mr' in r_personal_prefix and 'mr' not in l_personal_prefix):\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return np.nan\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "'''\n",
    "\n",
    "personal_suffixes = ['ii','iii','iv','v','jr','sr']\n",
    "def suffix_match(left, right):\n",
    "    if len(left) > 0 and len(right) > 0:\n",
    "        l_suffix = left.replace('.','').replace(',','').split()\n",
    "        l_personal_suffix = set.intersection(set(l_suffix),personal_suffixes)\n",
    "        if len(l_personal_suffix) > 0:\n",
    "            r_suffix = right.replace('.','').replace(',','').split()\n",
    "            r_personal_suffix = set.intersection(set(r_suffix),personal_suffixes)\n",
    "            if len(r_personal_suffix) > 0:\n",
    "                if (len(set.intersection(l_personal_suffix,r_personal_suffix)) > 0):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return np.nan\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def m_initial_rejection(left, right):\n",
    "    if (len(left) > 0) and (len(right) > 0):\n",
    "        l_mname = left.replace(',','')\n",
    "        r_mname = right.replace(',','')\n",
    "        if (len(l_mname) == 1) and (len(r_mname) == 1):\n",
    "            if l_mname == r_mname:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def gender_match(left, right):\n",
    "    if (len(left)>0) and (len(right)>0):\n",
    "        if left == right:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else: return np.nan\n",
    "\n",
    "subsidiary_words = ['chapter','division', 'department']\n",
    "def org_word_mismatch(left, right):\n",
    "    if (len(left)>0) and (len(right)>0):\n",
    "        left_states = set.intersection(set(left.split()),us_states['acronym'])\n",
    "        right_states = set.intersection(set(right.split()),us_states['acronym'])\n",
    "        len_left_states = len(left_states)\n",
    "        len_right_states = len(right_states)\n",
    "        left_subsidiary = set.intersection(set(left.split()),subsidiary_words)\n",
    "        right_subsidiary = set.intersection(set(right.split()),subsidiary_words)\n",
    "        len_left_subsidiary = len(left_subsidiary)\n",
    "        len_right_subsidiary = len(right_subsidiary)\n",
    "        #check if there are mis-matched keywords\n",
    "        if ((len_left_states > 0) and (len_right_states == 0)) | ((len_right_states > 0) and (len_left_states == 0)):\n",
    "            return 0\n",
    "        elif ((len_left_subsidiary > 0) and (len_right_subsidiary == 0)) | ((len_right_subsidiary > 0) and (len_left_subsidiary == 0)):\n",
    "            return 0\n",
    "        #retain matches that share the same states\n",
    "        elif (len_left_states > 0) and (len_right_states > 0):\n",
    "            if len(set.intersection(left_states,right_states))>0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        #ignore all others\n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "def name_scores(df):\n",
    "    prelim_score_cols = []\n",
    "    reject_cols = []\n",
    "    if 'org_name_l' in df:\n",
    "        jaro_time = time.time()\n",
    "        df['jaro_score'] = df.apply(lambda row:\n",
    "            score_compare(*get_field(row, 'org_name'), 'jaro'),\n",
    "            axis=\"columns\")\n",
    "        prelim_score_cols.append('jaro_score')\n",
    "\n",
    "        print(\"\\tjaro scores done --- %s seconds ---\" % (time.time() - jaro_time))\n",
    "\n",
    "        # consider skipping these; they have no major effect on outcomes & require a lot of compute\n",
    "        '''\n",
    "        partial_time = time.time()\n",
    "        df['fuzz_partial_score'] = df.apply(lambda row:\n",
    "            score_compare(*get_field(row, 'org_name'), 'fuzz-partial'),\n",
    "            axis=\"columns\")\n",
    "        print(\"\\tfuzz partial scores done --- %s seconds ---\" % (time.time() - partial_time))\n",
    "\n",
    "        sort_time = time.time()\n",
    "        df['fuzz_sort_score'] = df.apply(lambda row:\n",
    "            score_compare(*get_field(row, 'org_name'), 'fuzz-sort'),\n",
    "            axis=\"columns\")\n",
    "        print(\"\\tfuzz sort scores done --- %s seconds ---\" % (time.time() - sort_time))\n",
    "\n",
    "        set_time = time.time()\n",
    "        df['fuzz_set_score'] = df.apply(lambda row:\n",
    "            score_compare(*get_field(row, 'org_name'), 'fuzz-set'),\n",
    "            axis=\"columns\")\n",
    "        print(\"\\tfuzz set scores done --- %s seconds ---\" % (time.time() - set_time))\n",
    "        '''\n",
    "        \n",
    "    if 'fname_l' in df and not organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR FIRST NAME MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        df['fname_match'] = df.apply(lambda row:\n",
    "            name_match(*get_field(row, 'fname')),\n",
    "            axis=\"columns\")\n",
    "        prelim_score_cols.append('fname_match')\n",
    "        print(\"\\tfname values compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "\n",
    "    if 'lname_l' in df and not organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR LAST NAME MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        df['lname_match'] = df.apply(lambda row:\n",
    "            name_match(*get_field(row, 'lname')),\n",
    "            axis=\"columns\")\n",
    "        prelim_score_cols.append('lname_match')\n",
    "        print(\"\\tlname values compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    '''\n",
    "    #redundant with gender amtch\n",
    "    if 'prefix_l' in df and not organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR PREFIX MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        \n",
    "        df['prefix_match'] = df.apply(lambda row:\n",
    "            prefix_match(*get_field(row, 'prefix')),\n",
    "            axis='columns')\n",
    "        reject_cols.append('prefix_match')\n",
    "        print(\"\\tprefix values compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    '''    \n",
    "    if 'person_suffix_l' in df and not organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR SUFFIX MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        \n",
    "        df['suffix_match'] = df.apply(lambda row:\n",
    "            suffix_match(*get_field(row, 'person_suffix')),\n",
    "            axis='columns')\n",
    "        reject_cols.append('suffix_match')\n",
    "        print(\"\\tsuffix values compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    \n",
    "    if 'mname_l' in df and not organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR MIDDLE INITIAL MIS-MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        \n",
    "        df['m_initial_rejection'] = df.apply(lambda row:\n",
    "            m_initial_rejection(*get_field(row, 'mname')),\n",
    "            axis='columns')\n",
    "        reject_cols.append('m_initial_rejection')\n",
    "        print(\"\\tmname values compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "        \n",
    "    if 'bb_gender_l' in df and not organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR GENDER MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        \n",
    "        df['gender_match'] = df.apply(lambda row:\n",
    "            gender_match(*get_field(row, 'bb_gender')),\n",
    "            axis='columns')\n",
    "        reject_cols.append('gender_match')\n",
    "        print(\"\\tgender values compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    \n",
    "    if 'org_name_l' in df and organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR REGIONAL/SUBSIDIARY MIS-MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        \n",
    "        df['org_word_mismatch'] = df.apply(lambda row:\n",
    "            org_word_mismatch(*get_org_data(row)),\n",
    "            axis='columns')\n",
    "        reject_cols.append('org_word_mismatch')\n",
    "        print(\"\\torg names compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "        \n",
    "    #calculating preliminary score that we will use to suppress unlikely matches from further calculation\n",
    "    #arguably I should also include some \"score boosters\" like email is exact match, we dont want to suppress\n",
    "    if len(prelim_score_cols)>0:\n",
    "        start_time = time.time()\n",
    "        prelim_score = []\n",
    "        for index, row in df.iterrows():\n",
    "            v = 0\n",
    "            for col in prelim_score_cols:\n",
    "                if pd.isnull(row[col]):\n",
    "                    v = v+1\n",
    "                else:\n",
    "                    v = v+row[col]\n",
    "            for col in reject_cols: #change value to 0 for any rejection criteria\n",
    "                if row[col] == 0:\n",
    "                    v = 0\n",
    "            prelim_score.append(v / len(prelim_score_cols))\n",
    "        df['prelim_score'] = prelim_score\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        df['prelim_score'] = 1\n",
    "        \n",
    "    #df['prelim_score'] = pd.to_numeric(df['prelim_score'], errors='coerce')\n",
    "    print(\"\\tprelim score calculated --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    \n",
    "    return df\n",
    "#print(\"Expected chunks:\")\n",
    "#print(int(pd.read_csv(ns('candidata.csv')).shape[0] / chunk_size ))\n",
    "#print('')\n",
    "process_chunks(pd.read_csv(ns('candidata.csv'),encoding='utf-8',compression=compression,\n",
    "                           chunksize=chunk_size, dtype=object),\n",
    "               name_scores,\n",
    "               ns(\"scored-1.csv\"),\n",
    "               index=False)\n",
    "\n",
    "print(\"name simularity scored --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print (\"REDUCE, REMOVING UNLIKELY MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def reduce_matches(df):\n",
    "    return df[df.prelim_score >= reduce_threshold]\n",
    "\n",
    "process_chunks(pd.read_csv(ns('scored-1.csv'),encoding='utf-8',compression=compression,\n",
    "                           chunksize=chunk_size),# dtype=object),\n",
    "               reduce_matches,\n",
    "               ns(\"scored-1_reduced.csv\"),\n",
    "               index=False)\n",
    "\n",
    "print(\"match candidates reduced --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print (\"SCORING ORG NAME SEQUENCE UNIQUENESS...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "org_names_cnt = Counter()\n",
    "person_names_cnt = Counter()\n",
    "\n",
    "def count_orgnames(df):\n",
    "    if 'org_name' in df:\n",
    "        df['org_name'].dropna().apply(lambda c: org_names_cnt.update(tokenize(c)))\n",
    "\n",
    "if matching:\n",
    "    process_chunks(pd.read_csv(ns(\"processed_l.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                               dtype=object, index_col='id'),\n",
    "                   count_orgnames)\n",
    "    process_chunks(pd.read_csv(ns(\"processed_r.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                               dtype=object, index_col='id'),\n",
    "                   count_orgnames)\n",
    "else:\n",
    "    process_chunks(pd.read_csv(ns(\"processed.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                               dtype=object, index_col='id'),\n",
    "                   count_orgnames)\n",
    "    \n",
    "def org_name_similarity(left, right):\n",
    "    def org_sequence_uniqueness(seq):\n",
    "        try:\n",
    "            return sum(1 / org_names_cnt[t.lower()] ** 0.5 for t in seq)\n",
    "        except:\n",
    "            print(seq, org_names_cnt)\n",
    "            raise ValueError()\n",
    "\n",
    "    if len(left) > 0 and len(right) > 0:\n",
    "        left_toks = set(tokenize(left))\n",
    "        right_toks = set(tokenize(right))\n",
    "\n",
    "        left_uniq = org_sequence_uniqueness(left_toks)\n",
    "        right_uniq = org_sequence_uniqueness(right_toks)\n",
    "\n",
    "        return org_sequence_uniqueness(left_toks & right_toks) / (left_uniq * right_uniq) ** 0.5\n",
    "    else: return np.nan\n",
    "\n",
    "if not organization_only:\n",
    "    def count_personnames(df):\n",
    "        df['full_name'].dropna().apply(lambda c: person_names_cnt.update(tokenize(c)))\n",
    "\n",
    "    if matching:\n",
    "        process_chunks(pd.read_csv(ns(\"processed_l.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                                   dtype=object, index_col='id'),\n",
    "                       count_personnames)\n",
    "        process_chunks(pd.read_csv(ns(\"processed_r.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                                   dtype=object, index_col='id'),\n",
    "                       count_personnames)\n",
    "    else:\n",
    "        process_chunks(pd.read_csv(ns(\"processed.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression,\n",
    "                                   dtype=object, index_col='id'),\n",
    "                       count_personnames)\n",
    "        \n",
    "    def person_name_similarity(left, right):\n",
    "        def person_sequence_uniqueness(seq):\n",
    "            try:\n",
    "                return sum(1 / person_names_cnt[t.lower()] ** 0.5 for t in seq)\n",
    "            except:\n",
    "                print(seq, org_names_cnt)\n",
    "                raise ValueError()\n",
    "\n",
    "        if len(left) > 0 and len(right) > 0:\n",
    "            left_toks = set(tokenize(left))\n",
    "            right_toks = set(tokenize(right))\n",
    "\n",
    "            left_uniq = person_sequence_uniqueness(left_toks)\n",
    "            right_uniq = person_sequence_uniqueness(right_toks)\n",
    "\n",
    "            return person_sequence_uniqueness(left_toks & right_toks) / (left_uniq * right_uniq) ** 0.5\n",
    "        else: return np.nan\n",
    "\n",
    "def calculate_unique(df):\n",
    "    if 'org_name_l' in df:\n",
    "        df['org_uniq'] = df.apply(lambda row:\n",
    "            org_name_similarity(*get_field(row, 'org_name')),\n",
    "            axis=\"columns\")\n",
    "    #else:\n",
    "        #df['org_uniq'] = np.nan\n",
    "    if not organization_only:\n",
    "        if 'full_name_l' in df:\n",
    "            df['person_uniq'] = df.apply(lambda row:\n",
    "                person_name_similarity(*get_field(row, 'full_name')),\n",
    "                axis=\"columns\")\n",
    "        #else:\n",
    "            #df['person_uniq'] = np.nan\n",
    "    return df\n",
    "\n",
    "#print(\"Expected chunks:\")\n",
    "#print(int(pd.read_csv(ns('scored-1.csv')).shape[0] / chunk_size ))\n",
    "#print('')\n",
    "process_chunks(pd.read_csv(ns(\"scored-1_reduced.csv\"),encoding='utf-8',compression=compression,\n",
    "                           chunksize=chunk_size, dtype=object),\n",
    "               calculate_unique,\n",
    "               ns(\"scored-2.csv\"),\n",
    "               index=False)\n",
    "\n",
    "print(\"name uniqueness scored --- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_match(left, right): #same as name match but does not remove spaces and special characters\n",
    "    if len(left) > 0 and len(right) > 0:\n",
    "        return score_compare(left, right, 'jaro')\n",
    "    else: return np.nan\n",
    "    \n",
    "def multi_word_match(left, right): #handling cities, emails which could have many of each\n",
    "    match_scores = []\n",
    "    for word_l in left:\n",
    "        for word_r in right:\n",
    "            if len(word_l) > 0 and len(word_r) > 0:\n",
    "                match_scores.append(score_compare(word_l, word_r, 'jaro'))\n",
    "    if len(match_scores)>0:\n",
    "        return max(match_scores)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def initial_match(left, right):\n",
    "    if len(left) > 0 and len(right) > 0:\n",
    "        if str(left[0]).lower() == str(right[0]).lower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def match_fields(candidata):\n",
    "    if 'state_a_l' in candidata:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR STATE CODE MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        def state_match(left, right):\n",
    "            state_match_scores = []\n",
    "            for state_l in left:\n",
    "                for state_r in right:\n",
    "                    if len(state_l) >=2 and len(state_r) >= 2:\n",
    "                        if state_l == state_r:\n",
    "                            state_match_scores.append(1)\n",
    "                        else:\n",
    "                            state_match_scores.append(0)\n",
    "            if len(state_match_scores)>0:\n",
    "                return max(state_match_scores)\n",
    "            else:\n",
    "                return np.nan        \n",
    "            \n",
    "        candidata['state_match'] = candidata.apply(lambda row:\n",
    "            state_match(*get_multi_field(row, 'state')),\n",
    "            axis=\"columns\")\n",
    "\n",
    "        print(\"\\tstate codes checked --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "\n",
    "    if 'fname_l' in candidata and not organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR FIRST NAME MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        #check if first initials match\n",
    "        candidata['f_initial_match'] = candidata.apply(lambda row:\n",
    "            initial_match(*get_field(row, 'fname')),\n",
    "            axis='columns')\n",
    "        \n",
    "        print(\"\\tfname values compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "        \n",
    "        # If l_fname is present then the nick group should be too\n",
    "        start_time = time.time()\n",
    "        print(\"\\tCHECKING FOR NICKNAME GROUP MATCHES...\")\n",
    "\n",
    "        # This is a little complicated, but essentially it counts how many nicks groups are shared:\n",
    "        #\n",
    "        # get_field(row, 'nicks_groups'):\n",
    "        #     look up the data for each row, and get the nicks_groups from the left and right.\n",
    "        # map(lambda nicks: set(nicks.split()), _):\n",
    "        #     for the left and right nicks in nicks_groups, split it and create a set of nick groups\n",
    "        # set.intersection(*_):\n",
    "        #     pass these two sets to set.intersection to get the intersection of the two\n",
    "        # len(_):\n",
    "        #     take the length of the intersection\n",
    "        \n",
    "        #changed to a 0/1 flag rather then len of intersection\n",
    "        def nick_match(left, right):\n",
    "            if len(left) > 4 and len(right) > 4:\n",
    "                if len(set.intersection(set(left.split()),set(right.split()))) > 0:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return np.nan\n",
    "        candidata['nick_matches'] = candidata.apply(lambda row:\n",
    "                                                         nick_match(*get_field(row, 'nicks_groups')),\n",
    "                                                         axis='columns')\n",
    "        \n",
    "        print('\\tnickname group matches compared --- %s seconds ---' % round(time.time() - start_time, 2))\n",
    "        \n",
    "    if 'mname_l' in candidata and not organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR MIDDLE INITIAL MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        \n",
    "        candidata['m_initial_match'] = candidata.apply(lambda row:\n",
    "            initial_match(*get_field(row, 'mname')),\n",
    "            axis='columns')\n",
    "        print(\"\\tmname values compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "        \n",
    "    if 'city_a_l' in candidata:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR CITY MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        candidata['city_match'] = candidata.apply(lambda row:\n",
    "            multi_word_match(*get_multi_field(row, 'city')),\n",
    "            axis=\"columns\")\n",
    "        print(\"\\tcity values compared --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "\n",
    "    if 'zip_a_l' in candidata:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR POSTAL CODE MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        def postal_similarity(left, right):\n",
    "            zip_match_scores = []\n",
    "            for zip_l in left:\n",
    "                for zip_r in right:\n",
    "                    # if the number is too short, means it's fubar\n",
    "                    if len(zip_l) >= 5 and len(zip_r) >= 5:\n",
    "                        if max(len(sub) for sub in find_common_subsequences(zip_l[:5], zip_r[:5])) / 5 == 1:\n",
    "                            zip_match_scores.append(1)\n",
    "                        else:\n",
    "                            zip_match_scores.append(0)\n",
    "            if len(zip_match_scores)>0:\n",
    "                return max(zip_match_scores)\n",
    "            else:\n",
    "                return np.nan\n",
    "\n",
    "        candidata['zip_match'] = candidata.apply(lambda row:\n",
    "            postal_similarity(*get_multi_field(row, 'zip')),\n",
    "            axis=\"columns\")\n",
    "\n",
    "        print(\"\\tpostal codes checked --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "\n",
    "    if 'before_domain_a_l' in candidata and not organization_only:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR BEFORE EMAIL DOMAIN MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        candidata['before_domain_match'] = candidata.apply(lambda row:\n",
    "            multi_word_match(*get_multi_field(row, 'before_domain')),\n",
    "            axis=\"columns\")\n",
    "\n",
    "        print(\"\\temail before domains checked --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "\n",
    "    if ('email_domain_a_l' in candidata) | ('web_domain_a_l' in candidata):\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR WEB DOMAIN MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        def domain_match(left, right):\n",
    "            domain_match_scores = []\n",
    "            for domain_l in left:\n",
    "                for domain_r in right:\n",
    "                    if len(domain_l) > 4 and len(domain_r) > 4:\n",
    "                        if domain_l == domain_r:\n",
    "                            domain_match_scores.append(1)\n",
    "                        else:\n",
    "                            domain_match_scores.append(0)\n",
    "            if len(domain_match_scores)>0:\n",
    "                return max(domain_match_scores)\n",
    "            else:\n",
    "                return np.nan\n",
    "\n",
    "        candidata['domain_match'] = candidata.apply(lambda row:\n",
    "            domain_match(*get_domain_fields(row, 'email_domain', 'web_domain')),\n",
    "            axis=\"columns\")\n",
    "\n",
    "        print(\"\\tweb domains checked --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "\n",
    "    if 'clean_phone_a_l' in candidata:\n",
    "        start_time = time.time()\n",
    "        print (\"\\tCHECKING FOR PHONE MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        #scoring match candidates based on matching phone\n",
    "        def phone_simularity(left, right):\n",
    "            phone_match_scores = []\n",
    "            for phone_l in left:\n",
    "                for phone_r in right:\n",
    "                    if len(phone_l) > 9 and len(phone_r) > 9:\n",
    "                        if max(len(sub) for sub in find_common_subsequences(phone_l, phone_r)) / 10 >= 1:\n",
    "                            phone_match_scores.append(1)\n",
    "                        else:\n",
    "                            phone_match_scores.append(0)\n",
    "            if len(phone_match_scores)>0:\n",
    "                return max(phone_match_scores)\n",
    "            else:\n",
    "                return np.nan\n",
    "\n",
    "        candidata['phone_match'] = candidata.apply(lambda row:\n",
    "            phone_simularity(*get_multi_field(row, 'clean_phone')),\n",
    "            axis=\"columns\")\n",
    "\n",
    "        print(\"\\tphones checked --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    \n",
    "    return candidata\n",
    "\n",
    "#print(\"Expected chunks:\")\n",
    "#print(int(pd.read_csv(ns('scored-2.csv')).shape[0] / chunk_size ))\n",
    "#print('')\n",
    "process_chunks(pd.read_csv(ns(\"scored-2.csv\"), chunksize=chunk_size, dtype=object,encoding='utf-8',compression=compression),\n",
    "               match_fields,\n",
    "               ns(\"scored-3.csv\"),\n",
    "\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure we add nicks_groups column to column dictionaries so they can be properly renamed at the end\n",
    "if (cols['nicks_groups'] is None) & (cols['fname'] is not None):\n",
    "    cols['nicks_groups'] = 'nicks_groups'\n",
    "if matching:\n",
    "    if (cols_alt['nicks_groups'] is None) & (cols_alt['fname'] is not None):\n",
    "        cols_alt['nicks_groups'] = 'nicks_groups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_chunks(chunks, chunk_func, outfile=None, silent=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Same as process_chunks but changing the fillna('') line\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    total_rows = 0\n",
    "    needs_init = True\n",
    "    for count, chunk in enumerate(chunks):\n",
    "        local_time = time.time()\n",
    "\n",
    "        # Some preprocessing that needs to be done every read_csv\n",
    "        #chunk = chunk.fillna('')\n",
    "\n",
    "        processed = chunk_func(chunk)\n",
    "        \n",
    "        if outfile:\n",
    "            # mode=w overwrites the file, which we want on initial write\n",
    "            # mode=a appends to the end of file, which we want as we process more chunks\n",
    "            # header=True puts a header on the top, which we want on initial write\n",
    "            # header=False omits the header, which we want as we add more chunks to the existing file\n",
    "\n",
    "            default_args = {'mode': 'w' if needs_init else 'a', 'header': needs_init, 'compression': compression, 'encoding':'utf-8'}\n",
    "            # **dict(default, **kwargs) creates a new dictionary of arguments. It basically combines\n",
    "            # the defaults with the passed in arguments. It also allows the keyword arguments to override\n",
    "            # any of the default arguments.\n",
    "            #\n",
    "            # Note: function(**{'a': 1, 'b': 2, 'c': 3}) == function(a=1, b=2, c=3)\n",
    "            processed.to_csv(outfile, **dict(default_args, **kwargs))\n",
    "            needs_init = False\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "        if not silent:\n",
    "            print(\"Processed chunk\", count,\n",
    "                  round(time.time() - local_time, 2), \"seconds\")\n",
    "    if not silent:\n",
    "        print(\"Finished in\", round(time.time() - start_time, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use labeled matches to train logistic regression to predict matches\n",
    "pers_train = pd.read_csv('data/person_training_records_w_imputed_features.csv').dropna(how='all')\n",
    "org_train = pd.read_csv('data/org_training_records_w_imputed_features.csv').dropna(how='all')\n",
    "\n",
    "if organization_only:\n",
    "    org_features = [\n",
    "        'jaro_score',\n",
    "        'fuzz_partial_score',\n",
    "        'fuzz_sort_score',\n",
    "        'fuzz_set_score',\n",
    "        'org_uniq',\n",
    "        'city_match',\n",
    "        'state_match',\n",
    "        'zip_match',\n",
    "        'domain_match',\n",
    "        'phone_match' #include phone match for org matching\n",
    "    ]\n",
    "else:\n",
    "    org_features = [\n",
    "        'jaro_score',\n",
    "        'fuzz_partial_score',\n",
    "        'fuzz_sort_score',\n",
    "        'fuzz_set_score',\n",
    "        'org_uniq',\n",
    "        'city_match',\n",
    "        'state_match',\n",
    "        'zip_match',\n",
    "        'domain_match'\n",
    "    ]\n",
    "                \n",
    "pers_features = [\n",
    "    'fname_match',\n",
    "    'f_initial_match',\n",
    "    'm_initial_match',\n",
    "    'lname_match',\n",
    "    'person_uniq',\n",
    "    'suffix_match',\n",
    "    'nick_matches',\n",
    "    'phone_match',\n",
    "    'before_domain_match'\n",
    "]\n",
    "\n",
    "# Get the columns that are features and are also present in our dataset right now\n",
    "present_columns = pd.read_csv(ns(\"scored-3.csv\"), nrows=1,encoding='utf-8',compression=compression).columns\n",
    "\n",
    "# Get all the org features that are present\n",
    "cand_org_features = present_columns[present_columns.isin(org_features)]\n",
    "# Do the same with the contact features IF there are contact features\n",
    "cand_pers_features = present_columns[present_columns.isin(pers_features)]\n",
    "\n",
    "feature_avg_dict= {}\n",
    "for feature in cand_pers_features:\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        mode = pers_train[(pers_train[feature] != 1) & (pers_train[feature] != 0)][feature].mode()[0]\n",
    "    except:\n",
    "        mode = pers_train[feature].mean()\n",
    "    feature_avg_dict.update({feature:mode})\n",
    "    print(feature + \" avg value added to dictionary --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "for feature in cand_org_features:\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        mode = org_train[(org_train[feature] != 1) & (org_train[feature] != 0)][feature].mode()[0]\n",
    "    except:\n",
    "        mode = org_train[feature].mean()\n",
    "    feature_avg_dict.update({feature:mode})\n",
    "    print(feature + \" avg value added to dictionary --- %s seconds ---\" % round(time.time() - start_time, 2))    \n",
    "\n",
    "all_features = [] #creating list of all features used in ML for imputation\n",
    "for value in cand_org_features: all_features.append(value)\n",
    "for value in cand_pers_features: all_features.append(value)\n",
    "    \n",
    "'''#OLD WAY.  This code imputes based on the average value of assessment data (rather than of the training data)\n",
    "\n",
    "#imputing np.nan values in ML features.  we need to do this because we want the model to treat differently\n",
    "#cases where two records have a data type present and it DOES NOT MATCH and where one record is simply MISSING that data\n",
    "\n",
    "#creating a dictionary we will use to determine the value to impute into feature NULLs\n",
    "#####Arguably I should be using the avgs from the training set?  \n",
    "#but really I think what I should try is creating a series of models to predict null values for each feature \n",
    "#based on training data\n",
    "feature_avg_dict= {}\n",
    "for feature in all_features:\n",
    "    start_time = time.time()\n",
    "    #should consider deleting this after using because of memory usage\n",
    "    feature_df = pd.read_csv(ns(\"scored-3.csv\"),usecols=[feature],encoding='utf-8',compression=compression)\n",
    "    avg = feature_df[feature_df[feature].notnull()].mean()[0]\n",
    "    feature_avg_dict.update({feature:avg})\n",
    "    print(feature + \" avg value added to dictionary --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "'''\n",
    "#right now I am imputing NULLs based on average values of testing set not training set.  I'm not sure this is proper\n",
    "#BUT, it's probably fine.  \n",
    "def impute_nulls(candidata):\n",
    "    for feature in all_features:\n",
    "        start_time = time.time()\n",
    "        candidata[feature] = candidata[feature].fillna(feature_avg_dict[feature])\n",
    "        print(feature + \" imputed --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "        #candidata[feature] = candidata[feature].replace(np.nan,feature_avg_dict[feature])\n",
    "\n",
    "    return candidata\n",
    "\n",
    "impute_chunks(pd.read_csv(ns(\"scored-3.csv\"), chunksize=chunk_size,encoding='utf-8',compression=compression, dtype=object),\n",
    "               impute_nulls,\n",
    "               ns(\"scored-4.csv\"),\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_avg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Use our training data to whip up some ML models and use them to predict our results. We generate one model based on oragnization deduplication training data, and one based on contact training data.\n",
    "\n",
    "For contact matching/deduping, we evaluate with both models then create a composite score. For organization matching/deduping, we just use the organization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print (\"COMPOSITE SCORING, PREDICTING MATCHES...\") #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#fit org match model to training data\n",
    "org_cls = LogisticRegression()      # Model organization data from organization dataset\n",
    "org_cls.fit(org_train[cand_org_features], org_train['is_match'])\n",
    "\n",
    "#fit person match model to training data\n",
    "if not organization_only:\n",
    "    pers_cls = LogisticRegression()     # Model person data from person dataset\n",
    "    pers_cls.fit(pers_train[cand_pers_features], pers_train['is_match'])\n",
    "\n",
    "\"\"\"\n",
    "# Unused subpar ML models preserved for posterity\n",
    "all_cls = RandomForestClassifier()      # Model both data together from person dataset\n",
    "all_org_cls = RandomForestClassifier()  # Model organization data from both datasets\n",
    "all_cls.fit(pers_train[list(cand_pers_features) + list(cand_org_features)], pers_train['is_match'])\n",
    "all_org_cls.fit(pd.concat([org_train[cand_org_features], pers_train[cand_org_features]]),\n",
    "                pd.concat([org_train['is_match'], pers_train['is_match']]))\n",
    "\"\"\"\n",
    "\n",
    "def predict(candidata, method=\"split-source\"):\n",
    "    if (method == \"split-source\"):\n",
    "        # Train and fit on training data, then predict on our data\n",
    "        #NOTE: the reason why we do not use org details from person training data is because the labels are for\n",
    "        #whether the person matches, not the org.  so it may mislead the model to consider those labels.\n",
    "        candidata['org_match_pred'] = org_cls.predict(candidata[cand_org_features])\n",
    "        candidata['org_pred_proba'] = org_cls.predict_proba(candidata[cand_org_features])[:, 1]\n",
    "\n",
    "        if not organization_only:\n",
    "            # If this is a contact mode run, generate contact predictions and then create a composite score\n",
    "            candidata['pers_match_pred'] = pers_cls.predict(candidata[cand_pers_features])\n",
    "            candidata['pers_pred_proba'] = pers_cls.predict_proba(candidata[cand_pers_features])[:, 1]\n",
    "\n",
    "            # Create a composite score by hand\n",
    "            candidata['composite_match_proba'] = (candidata['pers_pred_proba'] * 2 + candidata['org_pred_proba']) / 3\n",
    "            candidata['pred'] = candidata['composite_match_proba'] >= match_threshold\n",
    "        else:\n",
    "            # If this is an org mode run then just use the org mode predictions\n",
    "            candidata['composite_match_proba'] = candidata['org_pred_proba']\n",
    "            candidata['pred'] = candidata['org_match_pred']\n",
    "        \n",
    "    else: raise ValueError(\"Prediction method \\\"\" + method + \"\\\" not recognized.\")\n",
    "    \n",
    "    return candidata\n",
    "\n",
    "#print(\"Expected chunks:\")\n",
    "#print(int(pd.read_csv(ns('scored-4.csv')).shape[0] / chunk_size ))\n",
    "#print('')\n",
    "process_chunks(pd.read_csv(ns(\"scored-4.csv\"), chunksize=chunk_size, index_col='pair',encoding='utf-8',compression=compression,\n",
    "                           usecols=list(cand_pers_features) + list(cand_org_features) + ['id_l', 'id_r', 'pair'],\n",
    "                           dtype={**dict(zip(list(cand_pers_features) + list(cand_org_features),\n",
    "                                             [float] * len(list(cand_pers_features) + list(cand_org_features)))),\n",
    "                                  **dict(zip(['id_l', 'id_r', 'pair'], [object] * 3))}),\n",
    "               predict,\n",
    "               ns(\"matched.csv\")) #file containing features for each match pair, whether or not it is a predicted match\n",
    "\n",
    "print(\"final matches isolated --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ISOLATING MATCHES...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# matched.csv is all the candidates; this filters out just the matches into matches.csv\n",
    "process_chunks(pd.read_csv(ns(\"matched.csv\"), chunksize=chunk_size * 10,encoding='utf-8',compression=compression),\n",
    "               lambda chunk: chunk[chunk['pred'] == 1],\n",
    "               ns(\"matches.csv\"), #file containing features for each predicted match pair\n",
    "               index=False)\n",
    "\n",
    "print(\"final matches isolated --- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Queening\n",
    "\n",
    "In cases of deduplication, we designate groups of rows that are all considered duplicates of each other. Each group also gets a \"queen\" that contains the most authoritative information.\n",
    "\n",
    "More information about how the queen is determined and about how groups are chosen are given in the comments of their respective functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"GROUPING DUPLICATE RECORDS TOGETHER...\")\n",
    "\n",
    "# Set up the graph:\n",
    "# Each edge represents a match between two records, and has the match_probability as a weight\n",
    "# Each node represents a record, and has a \"group_id\" value that indicates what group it is in\n",
    "# A group_id of 0 will indicate no group.\n",
    "# Each node also has a \"connection\" value that indicates how connected it is to its group\n",
    "\n",
    "#removed usecols argument\n",
    "edges = pd.read_csv(ns(\"matches.csv\"),usecols=['pair', 'id_l', 'id_r', 'composite_match_proba'],encoding='utf-8',compression=compression,\n",
    "    dtype={'id_l': object, 'id_r': object, 'composite_match_proba': float})\n",
    "\n",
    "# Gather up the IDs and the queen_score for each row.\n",
    "if matching:\n",
    "    # Here, for matches, we potentially have duplicate IDs across the main\n",
    "    # and alt dataset, so we tag each row with a 'source' field of 'main'\n",
    "    # or 'alt', then give all the rows combined a new index\n",
    "    # (generated by using pd.concat(..., ignore_index=True)).\n",
    "    data_left = pd.read_csv(ns(\"processed_l.csv\"),encoding='utf-8',compression=compression,\n",
    "                   dtype={'id': object, 'queen_score': float},\n",
    "                   usecols=['id', 'queen_score']) \\\n",
    "         .assign(source=\"main\")\n",
    "    data_right = pd.read_csv(ns(\"processed_r.csv\"),encoding='utf-8',compression=compression,\n",
    "                   dtype={'id': object, 'queen_score': float},\n",
    "                   usecols=['id', 'queen_score']) \\\n",
    "         .assign(source=\"alt\")\n",
    "    data = pd.concat([data_left, data_right], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Finally, we modify edges' 'id_l' and 'id_r' to reflect the new universal IDs\n",
    "    # We create series that's just the universal index, with an index of the original\n",
    "    # IDs. This way, if we call .to_dict() and create a dictionary of index->value, we\n",
    "    # get a dictionary of original ID to universal ID.\n",
    "    mainID_maps = pd.Series(data.index[data['source'] == 'main'],\n",
    "                            index=data[data['source'] == 'main']['id'])\n",
    "    altID_maps = pd.Series(data.index[data['source'] == 'alt'],\n",
    "                           index=data[data['source'] == 'alt']['id'])\n",
    "\n",
    "    edges['id_l'] = edges['id_l'].map(mainID_maps.to_dict())\n",
    "    edges['id_r'] = edges['id_r'].map(altID_maps.to_dict())\n",
    "else:\n",
    "    data = pd.read_csv(ns(\"processed.csv\"),encoding='utf-8',compression=compression,\n",
    "                       dtype={'id': object, 'queen_score': float},\n",
    "                       usecols=['id', 'queen_score']) \\\n",
    "             .set_index('id')\n",
    "\n",
    "G = nx.convert_matrix.from_pandas_edgelist(edges, 'id_l', 'id_r', ['pair','composite_match_proba'])\n",
    "nx.set_node_attributes(G, name=\"queen\", values=False)\n",
    "nx.set_node_attributes(G, name=\"group_id\", values=0)\n",
    "nx.set_node_attributes(G, name=\"connection\", values=0)\n",
    "nx.set_node_attributes(G, name=\"queen_score\", values=data[\"queen_score\"].to_dict())\n",
    "nx.set_node_attributes(G, name=\"pair\", values=0)\n",
    "\n",
    "###########################################\n",
    "##########  Graph Basics for Us  ##########\n",
    "###########################################\n",
    "# Our graph has nodes and edges\n",
    "# Each node is the ID of a record\n",
    "# Each edge is a match between two IDs\n",
    "#\n",
    "# Each node has a queen attribute indicating whether it's the queen of the group\n",
    "# Each node has a group_id attribute indicating which group it's in, or 0 if none\n",
    "# Each node has a connection attribute indicating its distance from the queen of\n",
    "#     the group. This is calculated by the match probability times other match probabilities\n",
    "#     on the path to the queen through the graph\n",
    "# Each node has a queen_score attribute for reference indicating how much data it has. The\n",
    "#     queen should have the highest queen_score in its group\n",
    "# Each edge has a composite_match_proba attribute indicating how likely it is the two nodes\n",
    "#     it connects are a match.\n",
    "#\n",
    "# x and y are node IDs, e.g. \"11846309\"\n",
    "# G[x] gets us all nodes connected to x as a list of IDs\n",
    "# G[x][y] gets us the edge connecting x and y in the form of a dictionary of edge attributes\n",
    "# G.nodes[x] gets us the node x in the form of a dictionary of node attributes\n",
    "\n",
    "\n",
    "# Start grouping\n",
    "# Create a queen_score for each edge (aka the max queen score of the two nodes)\n",
    "edges = edges.merge(data[\"queen_score\"], how=\"left\", left_on=\"id_l\", right_index=True)\n",
    "edges = edges.merge(data[\"queen_score\"], how=\"left\", left_on=\"id_r\", right_index=True, suffixes=['_l', '_r'])\n",
    "edges[\"queen_score\"] = edges.apply(lambda row: max(row[[\"queen_score_l\", \"queen_score_r\"]]), axis=\"columns\")\n",
    "# Sort primarily by descending match, then by descending queen score\n",
    "edges = edges.sort_values([\"composite_match_proba\", \"queen_score\"], ascending=[False, False])\n",
    "\n",
    "# Initalize the first group ID, and provide a function to update them\n",
    "last_group = 1\n",
    "def new_group():\n",
    "    \"\"\"\n",
    "    This function hands out a group ID and updates the value\n",
    "    so that the next call of new_group() will give a new group ID\n",
    "    \"\"\"\n",
    "    global last_group\n",
    "    group_id = last_group\n",
    "    last_group += 1\n",
    "    return group_id\n",
    "\n",
    "total_rows = len(edges)  # Total rows to track percentage complete\n",
    "\n",
    "# A basic overview:\n",
    "# We go through all the matches (aka all the edges), starting with those\n",
    "# with the highest queen_score and highest probability of match. That's\n",
    "# because these will almost definitely be the matches that contain the queen\n",
    "# and, essentially, they're easy pickings.\n",
    "# As we go on down the list, we let the group membership \"infect\" other nodes\n",
    "# as long as they're within the group_threshold.\n",
    "#\n",
    "# group_threshold score is calculated as the product of the match probabilities\n",
    "# connecting any node in a group to that group's queen.\n",
    "for current, row in enumerate(edges.itertuples(), 1):\n",
    "    local_time = time.time()\n",
    "\n",
    "    # Shortcut variables so I don't have to type so much:\n",
    "    # left is a dictionary of node attributes of the node id_l\n",
    "    # right is a dictionary of node attributes of the node id_r\n",
    "    # edge is the composite_match_proba attribute of the shared edge (float)\n",
    "    left = G.nodes[row.id_l]\n",
    "    right = G.nodes[row.id_r]\n",
    "    edge = G[row.id_l][row.id_r]['composite_match_proba']\n",
    "    pair = G[row.id_l][row.id_r]['pair']\n",
    "\n",
    "    # There are four possibilities for each edge\n",
    "    # 1) Neither node has a group\n",
    "    if left['group_id'] == 0 and right['group_id'] == 0:\n",
    "        if edge < group_threshold:\n",
    "            # This shouldn't happen as long as the\n",
    "            # match threshold >= group threshold\n",
    "            continue\n",
    "\n",
    "        # First assign them their own group\n",
    "        new_group_id = new_group()\n",
    "        left['group_id'] = new_group_id\n",
    "        right['group_id'] = new_group_id\n",
    "        # Then pick who will be the de facto queen\n",
    "        if (left['queen_score'] > right['queen_score']):\n",
    "            left['connection'] = 1\n",
    "            left['queen'] = True\n",
    "            right['connection'] = edge\n",
    "            right['pair'] = pair\n",
    "            left['pair'] = pair\n",
    "        else:\n",
    "            right['connection'] = 1\n",
    "            right['queen'] = True\n",
    "            left['connection'] = edge\n",
    "            right['pair'] = pair\n",
    "            left['pair'] = pair\n",
    "\n",
    "    # 2) Both nodes have the same group\n",
    "    elif left['group_id'] != 0 and left['group_id'] == right['group_id']:\n",
    "        # For both nodes, see if they would get a higher connection if\n",
    "        # they connected to the queen through each other instead of\n",
    "        # however they got connected in the first place.\n",
    "        # This is unlikely to happen in practice.\n",
    "        if right['connection'] < left['connection'] * edge:\n",
    "            right['connection'] = left['connection'] * edge\n",
    "            right['pair'] = pair\n",
    "            #left['pair'] = pair\n",
    "        elif left['connection'] < right['connection'] * edge:\n",
    "            left['connection'] = right['connection'] * edge\n",
    "            left['pair'] = pair\n",
    "            #right['pair'] = pair\n",
    "\n",
    "    # 3) Only one node has a group\n",
    "    elif left['group_id'] != 0 and right['group_id'] == 0:\n",
    "        # Check if the ungrouped node could\n",
    "        # connect to the grouped node\n",
    "        if left['connection'] * edge >= group_threshold:\n",
    "            right['group_id'] = left['group_id']\n",
    "            right['connection'] = left['connection'] * edge\n",
    "            right['pair'] = pair\n",
    "            #left['pair'] = pair\n",
    "    elif right['group_id'] != 0 and left['group_id'] == 0:\n",
    "        # Check if the ungrouped node could\n",
    "        # connect to the grouped node\n",
    "        if right['connection'] * edge >= group_threshold:\n",
    "            left['group_id'] = right['group_id']\n",
    "            left['connection'] = right['connection'] * edge\n",
    "            left['pair'] = pair\n",
    "            #right['pair'] = pair\n",
    "\n",
    "    # 4) The two nodes are in different groups\n",
    "    #\n",
    "    # When a node switches groups, we should check its old groupmates\n",
    "    # and see if they also want to switch groups. Practically, this won't\n",
    "    # happen because how quickly the connection to the queen degrades. But\n",
    "    # this will be a recursive function, checking neighbors and neighbors of\n",
    "    # neighbors.\n",
    "    elif (left['group_id'] != 0 and right['group_id'] != 0 and\n",
    "          left['group_id'] != right['group_id']):\n",
    "        def cascade_conversion(converted_id):\n",
    "            \"\"\"\n",
    "            Checks to see if the neighbors of converted_id would get better\n",
    "            connections if they also converted to the same group as converted_id.\n",
    "            \"\"\"\n",
    "            converted = G.nodes[converted_id]\n",
    "            for neighbor_id in G[converted_id]:\n",
    "                neighbor = G.nodes[neighbor_id]\n",
    "                edge = G[converted_id][neighbor_id][\"composite_match_proba\"]\n",
    "                if neighbor['group_id'] == converted['group_id']:\n",
    "                    continue\n",
    "                if neighbor['connection'] < converted['connection'] * edge * group_merge_limiter: #discourages/encourages groups from merging\n",
    "                    neighbor['group_id'] = converted['group_id']\n",
    "                    neighbor['connection'] = converted['connection'] * edge\n",
    "                    #neighbor['pair'] = pair\n",
    "                    #converted['pair'] = pair\n",
    "                    cascade_conversion(neighbor_id)\n",
    "\n",
    "        # Check if each node would have a better connection with\n",
    "        # the other node's group, and change accordingly\n",
    "        if right['connection'] < left['connection'] * edge * group_merge_limiter: #discourages/encourages groups from merging\n",
    "            right['group_id'] = left['group_id']\n",
    "            right['connection'] = left['connection'] * edge\n",
    "            #right['pair'] = pair\n",
    "            #left['pair'] = pair\n",
    "            cascade_conversion(row.id_r)\n",
    "        elif left['connection'] < right['connection'] * edge * group_merge_limiter: #discourages/encourages groups from merging\n",
    "            left['group_id'] = right['group_id']\n",
    "            left['connection'] = right['connection'] * edge  \n",
    "            #left['pair'] = pair\n",
    "            #right['pair'] = pair\n",
    "            cascade_conversion(row.id_l)\n",
    "    else:\n",
    "        # This... shouldn't ever ever happen\n",
    "        # Pretty sure the four cases above cover all possible\n",
    "        # outcomes. But if we do come across something, raise\n",
    "        # an error and print out some details about where it happened.\n",
    "        details = repr(G.nodes[row.id_l]) + \\\n",
    "                  repr(G.nodes[row.id_r]) + \\\n",
    "                  repr(G[row.id_l][row.id_r])\n",
    "        raise ValueError(\"Didn't expect an excerpt \" + details)\n",
    "\n",
    "# Consolidate graph group data back into dataframes\n",
    "# G.nodes.__getitem__(x) = G.nodes[x], and we *map* it to every node in G.nodes\n",
    "groups = pd.DataFrame.from_records(list(map(G.nodes.__getitem__, G.nodes)), index=list(G.nodes))\n",
    "groups = groups.rename({'connection': 'queen_proba'}, axis='columns')\n",
    "\n",
    "# Join with the processed data\n",
    "if matching:\n",
    "    data_left = pd.read_csv(ns(\"processed_l.csv\"), encoding='utf-8',compression=compression,dtype=object) \\\n",
    "         .assign(source=\"main\")\n",
    "    data_right = pd.read_csv(ns(\"processed_r.csv\"), encoding='utf-8',compression=compression,dtype=object) \\\n",
    "         .assign(source=\"alt\")\n",
    "    data = pd.concat([data_left, data_right], ignore_index=True)\n",
    "    grouped = data.join(groups[['pair','group_id', 'queen_proba', 'queen']])\n",
    "    grouped = grouped.set_index(['source', 'id'])\n",
    "else:\n",
    "    data = pd.read_csv(ns(\"processed.csv\"), encoding='utf-8',compression=compression,dtype=object).set_index('id')\n",
    "    grouped = data.join(groups[['pair','group_id', 'queen_proba', 'queen']])\n",
    "\n",
    "grouped = grouped.sort_values(['group_id', 'queen_proba', 'queen_score'], ascending=[True, False, False])\n",
    "grouped.to_csv(ns(\"grouped.csv\"))\n",
    "\n",
    "print(\"Grouped all matches --\", time.time() - start_time, \"seconds\")\n",
    "grouped[grouped.group_id != 0].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CREATE SENSIBLE DEDUP SCORE...\")\n",
    "start_time = time.time()\n",
    "\n",
    "grouped = pd.read_csv(ns(\"grouped.csv\"))\n",
    "# Initialize dedup_score with queen_proba\n",
    "grouped['dedup_score'] = grouped['queen_proba']\n",
    "\n",
    "#handle for scenario where group_merge_limiter > 1 which can create multiple queens for same group\n",
    "if group_merge_limiter >1:\n",
    "    for g in grouped.group_id.drop_duplicates():\n",
    "        if grouped[(grouped.group_id == g) & (grouped.queen == True)].shape[0] > 1:\n",
    "            n=0\n",
    "            for index, row in grouped[grouped.group_id == g].iterrows():\n",
    "                if n==0:\n",
    "                    n=1 #skip the first row which is the true queen\n",
    "                else:\n",
    "                    grouped.at[index,'queen'] = False\n",
    "        if grouped[grouped.group_id == g].shape[0] == 1: #this should really be fixed during clustering step, but I am being lazy\n",
    "            for index, row in grouped[grouped.group_id == g].iterrows():\n",
    "                grouped.at[index,'group_id'] = 0\n",
    "                grouped.at[index,'queen'] = False\n",
    "    grouped = grouped.sort_values(['group_id', 'queen_proba', 'queen_score'], ascending=[True, False, False])\n",
    "                    \n",
    "# Take all non-queens in valid groups (all group_ids > 0), find max score by group\n",
    "queens_score = grouped[(grouped['queen'] == False) & (grouped['group_id'] != 0)] \\\n",
    "    .groupby('group_id')['queen_proba'].max()\n",
    "# Assign back to all queens. Since the order is retained, we shouldn't have to\n",
    "# worry about assigning to the wrong group queen\n",
    "grouped.loc[grouped['queen'] == True, 'dedup_score'] = queens_score.values\n",
    "\n",
    "grouped['id'] = grouped['id'].astype(str)\n",
    "\n",
    "print(\"Dedup scores generated --- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thoughts about entity resolver:\n",
    "# isolate queens, compare all queens to each other.  If match > merge_variable (suggest .9) then merge corresponding groups\n",
    "#### Actually, you can't compare all queens to each other.  Imagine there are 20000 groups, that is too many comparisons\n",
    "#so that means I probably need to redo the full process? OR I reuse ungrouped matches from scored files\n",
    "#but then the downside is I would be missing the exclusions.  I need to test on a real dataset to see how well it works.\n",
    "# This comparison will work around exclusions (eg. gender mis-match) so these need to be added to features for reference...\n",
    "# ^^not true if I am using only already vetted matches from the scored-4 file.  That is probably the simplest path forward\n",
    "#do this in waves, begining one at a time working through all groups.  \n",
    "#use some dictionary to convert old queen members to the new group ID\n",
    "#stop waves when 0 groups are matched at or above merge_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if entity_ID is True: \n",
    "    #we will do multiple passes of the grouping step, each time taking only the queens from the previous step\n",
    "    #then we'll be adding multiple group ID columns and queen coumns for each pass.  \n",
    "    #the purpose of this is to resolve cases where multiple group_ids from the original pass should really be merged\n",
    "    #having multiple passes allows for the data owner to review and determine the optimal level of grouping granularity\n",
    "    max_passes = 5\n",
    "    grouped = grouped.set_index('id')\n",
    "    matched = pd.read_csv(ns(\"matched.csv\"),usecols=['pair', 'id_l', 'id_r', 'composite_match_proba'],encoding='utf-8',compression=compression,\n",
    "            dtype={'id_l': object, 'id_r': object, 'composite_match_proba': float})\n",
    "    queen_col = 'queen'\n",
    "    group_col = 'group_id'\n",
    "    queen_proba_col = 'queen_proba'\n",
    "    dedup_score_col = 'dedup_score'\n",
    "    pair_col = 'pair'\n",
    "    \n",
    "    rounds = []\n",
    "    for p in range(max_passes):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Set up the graph:\n",
    "        # Each edge represents a match between two records, and has the match_probability as a weight\n",
    "        # Each node represents a record, and has a \"group_id\" value that indicates what group it is in\n",
    "        # A group_id of 0 will indicate no group.\n",
    "        # Each node also has a \"connection\" value that indicates how connected it is to its group\n",
    "\n",
    "        #only using queens from the previous round of grouping\n",
    "        edges = matched[(matched.id_l.isin(grouped[grouped[queen_col]==True].index)) & (matched.id_r.isin(grouped[grouped[queen_col]==True].index))]\n",
    "        \n",
    "        if edges.shape[0] > 0:\n",
    "            print(\"GROUPING DUPLICATE GROUPS TOGETHER...{}\".format(group_col))\n",
    "            queen_col = queen_col.split()[0] + ' ' + str(p+2)\n",
    "            group_col = group_col.split()[0] + ' ' + str(p+2)\n",
    "            queen_proba_col = queen_proba_col.split()[0] + ' ' + str(p+2)\n",
    "            dedup_score_col = dedup_score_col.split()[0] + ' ' + str(p+2)\n",
    "            pair_col = pair_col.split()[0] + ' ' + str(p+2)\n",
    "            \n",
    "            rounds.append(group_col) #we'll iterate through this list later to flesh out the dedup scores and groups\n",
    "\n",
    "            '''#not sure this section is needed?\n",
    "\n",
    "            # Gather up the IDs and the queen_score for each row.\n",
    "            if matching:\n",
    "                # Here, for matches, we potentially have duplicate IDs across the main\n",
    "                # and alt dataset, so we tag each row with a 'source' field of 'main'\n",
    "                # or 'alt', then give all the rows combined a new index\n",
    "                # (generated by using pd.concat(..., ignore_index=True)).\n",
    "                data_left = pd.read_csv(ns(\"processed_l.csv\"),encoding='utf-8',compression=compression,\n",
    "                               dtype={'id': object, 'queen_score': float},\n",
    "                               usecols=['id', 'queen_score']) \\\n",
    "                     .assign(source=\"main\")\n",
    "                data_right = pd.read_csv(ns(\"processed_r.csv\"),encoding='utf-8',compression=compression,\n",
    "                               dtype={'id': object, 'queen_score': float},\n",
    "                               usecols=['id', 'queen_score']) \\\n",
    "                     .assign(source=\"alt\")\n",
    "                data = pd.concat([data_left, data_right], ignore_index=True)\n",
    "\n",
    "\n",
    "                # Finally, we modify edges' 'id_l' and 'id_r' to reflect the new universal IDs\n",
    "                # We create series that's just the universal index, with an index of the original\n",
    "                # IDs. This way, if we call .to_dict() and create a dictionary of index->value, we\n",
    "                # get a dictionary of original ID to universal ID.\n",
    "                mainID_maps = pd.Series(data.index[data['source'] == 'main'],\n",
    "                                        index=data[data['source'] == 'main']['id'])\n",
    "                altID_maps = pd.Series(data.index[data['source'] == 'alt'],\n",
    "                                       index=data[data['source'] == 'alt']['id'])\n",
    "\n",
    "                edges['id_l'] = edges['id_l'].map(mainID_maps.to_dict())\n",
    "                edges['id_r'] = edges['id_r'].map(altID_maps.to_dict())\n",
    "            else:\n",
    "                data = pd.read_csv(ns(\"processed.csv\"),encoding='utf-8',compression=compression,\n",
    "                                   dtype={'id': object, 'queen_score': float},\n",
    "                                   usecols=['id', 'queen_score']) \\\n",
    "                         .set_index('id')\n",
    "             '''\n",
    "\n",
    "            G = nx.convert_matrix.from_pandas_edgelist(edges, 'id_l', 'id_r', ['pair','composite_match_proba'])\n",
    "            nx.set_node_attributes(G, name=queen_col, values=False)\n",
    "            nx.set_node_attributes(G, name=group_col, values=0)\n",
    "            nx.set_node_attributes(G, name=\"connection\", values=0)\n",
    "            nx.set_node_attributes(G, name=\"queen_score\", values=data[\"queen_score\"].to_dict())\n",
    "            nx.set_node_attributes(G, name=pair_col, values=0)\n",
    "\n",
    "            ###########################################\n",
    "            ##########  Graph Basics for Us  ##########\n",
    "            ###########################################\n",
    "            # Our graph has nodes and edges\n",
    "            # Each node is the ID of a record\n",
    "            # Each edge is a match between two IDs\n",
    "            #\n",
    "            # Each node has a queen attribute indicating whether it's the queen of the group\n",
    "            # Each node has a group_id attribute indicating which group it's in, or 0 if none\n",
    "            # Each node has a connection attribute indicating its distance from the queen of\n",
    "            #     the group. This is calculated by the match probability times other match probabilities\n",
    "            #     on the path to the queen through the graph\n",
    "            # Each node has a queen_score attribute for reference indicating how much data it has. The\n",
    "            #     queen should have the highest queen_score in its group\n",
    "            # Each edge has a composite_match_proba attribute indicating how likely it is the two nodes\n",
    "            #     it connects are a match.\n",
    "            #\n",
    "            # x and y are node IDs, e.g. \"11846309\"\n",
    "            # G[x] gets us all nodes connected to x as a list of IDs\n",
    "            # G[x][y] gets us the edge connecting x and y in the form of a dictionary of edge attributes\n",
    "            # G.nodes[x] gets us the node x in the form of a dictionary of node attributes\n",
    "\n",
    "\n",
    "            # Start grouping\n",
    "            # Create a queen_score for each edge (aka the max queen score of the two nodes)\n",
    "            edges = edges.merge(data[\"queen_score\"], how=\"left\", left_on=\"id_l\", right_index=True)\n",
    "            edges = edges.merge(data[\"queen_score\"], how=\"left\", left_on=\"id_r\", right_index=True, suffixes=['_l', '_r'])\n",
    "            edges[\"queen_score\"] = edges.apply(lambda row: max(row[[\"queen_score_l\", \"queen_score_r\"]]), axis=\"columns\")\n",
    "            # Sort primarily by descending match, then by descending queen score\n",
    "            edges = edges.sort_values([\"composite_match_proba\", \"queen_score\"], ascending=[False, False])\n",
    "\n",
    "            # Initalize the first group ID, and provide a function to update them\n",
    "            last_group = 1\n",
    "            def new_group():\n",
    "                \"\"\"\n",
    "                This function hands out a group ID and updates the value\n",
    "                so that the next call of new_group() will give a new group ID\n",
    "                \"\"\"\n",
    "                global last_group\n",
    "                group_id = last_group\n",
    "                last_group += 1\n",
    "                return group_id\n",
    "\n",
    "            total_rows = len(edges)  # Total rows to track percentage complete\n",
    "\n",
    "            # A basic overview:\n",
    "            # We go through all the matches (aka all the edges), starting with those\n",
    "            # with the highest queen_score and highest probability of match. That's\n",
    "            # because these will almost definitely be the matches that contain the queen\n",
    "            # and, essentially, they're easy pickings.\n",
    "            # As we go on down the list, we let the group membership \"infect\" other nodes\n",
    "            # as long as they're within the group_threshold.\n",
    "            #\n",
    "            # group_threshold score is calculated as the product of the match probabilities\n",
    "            # connecting any node in a group to that group's queen.\n",
    "            for current, row in enumerate(edges.itertuples(), 1):\n",
    "                local_time = time.time()\n",
    "\n",
    "                # Shortcut variables so I don't have to type so much:\n",
    "                # left is a dictionary of node attributes of the node id_l\n",
    "                # right is a dictionary of node attributes of the node id_r\n",
    "                # edge is the composite_match_proba attribute of the shared edge (float)\n",
    "                left = G.nodes[row.id_l]\n",
    "                right = G.nodes[row.id_r]\n",
    "                edge = G[row.id_l][row.id_r]['composite_match_proba']\n",
    "                pair = G[row.id_l][row.id_r]['pair']\n",
    "\n",
    "                # There are four possibilities for each edge\n",
    "                # 1) Neither node has a group\n",
    "                if left[group_col] == 0 and right[group_col] == 0:\n",
    "                    if edge < group_threshold:\n",
    "                        # This shouldn't happen as long as the\n",
    "                        # match threshold >= group threshold\n",
    "                        continue\n",
    "\n",
    "                    # First assign them their own group\n",
    "                    new_group_id = new_group()\n",
    "                    left[group_col] = new_group_id\n",
    "                    right[group_col] = new_group_id\n",
    "                    # Then pick who will be the de facto queen\n",
    "                    if (left['queen_score'] > right['queen_score']):\n",
    "                        left['connection'] = 1\n",
    "                        left[queen_col] = True\n",
    "                        right['connection'] = edge\n",
    "                        right[pair_col] = pair\n",
    "                        left[pair_col] = pair\n",
    "                    else:\n",
    "                        right['connection'] = 1\n",
    "                        right[queen_col] = True\n",
    "                        left['connection'] = edge\n",
    "                        right[pair_col] = pair\n",
    "                        left[pair_col] = pair\n",
    "\n",
    "                # 2) Both nodes have the same group\n",
    "                elif left[group_col] != 0 and left[group_col] == right[group_col]:\n",
    "                    # For both nodes, see if they would get a higher connection if\n",
    "                    # they connected to the queen through each other instead of\n",
    "                    # however they got connected in the first place.\n",
    "                    # This is unlikely to happen in practice.\n",
    "                    if right['connection'] < left['connection'] * edge:\n",
    "                        right['connection'] = left['connection'] * edge\n",
    "                        right[pair_col] = pair\n",
    "                        #left['pair'] = pair\n",
    "                    elif left['connection'] < right['connection'] * edge:\n",
    "                        left['connection'] = right['connection'] * edge\n",
    "                        left[pair_col] = pair\n",
    "                        #right['pair'] = pair\n",
    "\n",
    "                # 3) Only one node has a group\n",
    "                elif left[group_col] != 0 and right[group_col] == 0:\n",
    "                    # Check if the ungrouped node could\n",
    "                    # connect to the grouped node\n",
    "                    if left['connection'] * edge >= group_threshold:\n",
    "                        right[group_col] = left[group_col]\n",
    "                        right['connection'] = left['connection'] * edge\n",
    "                        right[pair_col] = pair\n",
    "                        #left['pair'] = pair\n",
    "                elif right[group_col] != 0 and left[group_col] == 0:\n",
    "                    # Check if the ungrouped node could\n",
    "                    # connect to the grouped node\n",
    "                    if right['connection'] * edge >= group_threshold:\n",
    "                        left[group_col] = right[group_col]\n",
    "                        left['connection'] = right['connection'] * edge\n",
    "                        left[pair_col] = pair\n",
    "                        #right['pair'] = pair\n",
    "\n",
    "                # 4) The two nodes are in different groups\n",
    "                #\n",
    "                # When a node switches groups, we should check its old groupmates\n",
    "                # and see if they also want to switch groups. Practically, this won't\n",
    "                # happen because how quickly the connection to the queen degrades. But\n",
    "                # this will be a recursive function, checking neighbors and neighbors of\n",
    "                # neighbors.\n",
    "                elif (left[group_col] != 0 and right[group_col] != 0 and\n",
    "                      left[group_col] != right[group_col]):\n",
    "                    def cascade_conversion(converted_id):\n",
    "                        \"\"\"\n",
    "                        Checks to see if the neighbors of converted_id would get better\n",
    "                        connections if they also converted to the same group as converted_id.\n",
    "                        \"\"\"\n",
    "                        converted = G.nodes[converted_id]\n",
    "                        for neighbor_id in G[converted_id]:\n",
    "                            neighbor = G.nodes[neighbor_id]\n",
    "                            edge = G[converted_id][neighbor_id][\"composite_match_proba\"]\n",
    "                            if neighbor[group_col] == converted[group_col]:\n",
    "                                continue\n",
    "                            if neighbor['connection'] < converted['connection'] * edge * group_merge_limiter: #discourages/encourages groups from merging\n",
    "                                neighbor[group_col] = converted[group_col]\n",
    "                                neighbor['connection'] = converted['connection'] * edge\n",
    "                                #neighbor['pair'] = pair\n",
    "                                #converted['pair'] = pair\n",
    "                                cascade_conversion(neighbor_id)\n",
    "\n",
    "                    # Check if each node would have a better connection with\n",
    "                    # the other node's group, and change accordingly\n",
    "                    if right['connection'] < left['connection'] * edge * group_merge_limiter: #discourages/encourages groups from merging\n",
    "                        right[group_col] = left[group_col]\n",
    "                        right['connection'] = left['connection'] * edge\n",
    "                        #right['pair'] = pair\n",
    "                        #left['pair'] = pair\n",
    "                        cascade_conversion(row.id_r)\n",
    "                    elif left['connection'] < right['connection'] * edge * group_merge_limiter: #discourages/encourages groups from merging\n",
    "                        left[group_col] = right[group_col]\n",
    "                        left['connection'] = right['connection'] * edge  \n",
    "                        #left['pair'] = pair\n",
    "                        #right['pair'] = pair\n",
    "                        cascade_conversion(row.id_l)\n",
    "                else:\n",
    "                    # This... shouldn't ever ever happen\n",
    "                    # Pretty sure the four cases above cover all possible\n",
    "                    # outcomes. But if we do come across something, raise\n",
    "                    # an error and print out some details about where it happened.\n",
    "                    details = repr(G.nodes[row.id_l]) + \\\n",
    "                              repr(G.nodes[row.id_r]) + \\\n",
    "                              repr(G[row.id_l][row.id_r])\n",
    "                    raise ValueError(\"Didn't expect an excerpt \" + details)\n",
    "\n",
    "            # Consolidate graph group data back into dataframes\n",
    "            # G.nodes.__getitem__(x) = G.nodes[x], and we *map* it to every node in G.nodes\n",
    "            groups = pd.DataFrame.from_records(list(map(G.nodes.__getitem__, G.nodes)), index=list(G.nodes))\n",
    "            groups = groups.rename({'connection': queen_proba_col}, axis='columns')\n",
    "\n",
    "            # Join with the processed data\n",
    "            if matching:\n",
    "                data_left = pd.read_csv(ns(\"processed_l.csv\"), encoding='utf-8',compression=compression,dtype=object) \\\n",
    "                     .assign(source=\"main\")\n",
    "                data_right = pd.read_csv(ns(\"processed_r.csv\"), encoding='utf-8',compression=compression,dtype=object) \\\n",
    "                     .assign(source=\"alt\")\n",
    "                data = pd.concat([data_left, data_right], ignore_index=True)\n",
    "                grouped = data.join(groups[[pair_col,group_col, queen_proba_col, queen_col]])\n",
    "                grouped = grouped.set_index(['source', 'id'])\n",
    "            else:\n",
    "                #data = pd.read_csv(ns(\"processed.csv\"), encoding='utf-8',compression=compression,dtype=object).set_index('id')\n",
    "                grouped = grouped.join(groups[[pair_col,group_col, queen_proba_col, queen_col]])\n",
    "\n",
    "            grouped = grouped.sort_values([group_col, queen_proba_col, 'queen_score'], ascending=[True, False, False])\n",
    "\n",
    "            grouped[dedup_score_col] = grouped[queen_proba_col]\n",
    "\n",
    "            #handle for scenario where group_merge_limiter > 1 which can create multiple queens for same group\n",
    "            if group_merge_limiter >1:\n",
    "                for g in grouped[group_col].drop_duplicates():\n",
    "                    if grouped[(grouped[group_col] == g) & (grouped[queen_col] == True)].shape[0] > 1:\n",
    "                        n=0\n",
    "                        for index, row in grouped[grouped[group_col] == g].iterrows():\n",
    "                            if n==0:\n",
    "                                n=1 #skip the first row which is the true queen\n",
    "                            else:\n",
    "                                grouped.at[index,queen_col] = False\n",
    "                    if grouped[grouped[group_col] == g].shape[0] == 1: #this should really be fixed during clustering step, but I am being lazy\n",
    "                        for index, row in grouped[grouped[group_col] == g].iterrows():\n",
    "                            grouped.at[index,group_col] = 0\n",
    "                            grouped.at[index,queen_cpl] = False\n",
    "                grouped = grouped.sort_values([group_col, queen_proba_col, 'queen_score'], ascending=[True, False, False])\n",
    "\n",
    "            # Take all non-queens in valid groups (all group_ids > 0), find max score by group\n",
    "            queens_score = grouped[(grouped[queen_col] == False) & (grouped[group_col] != 0)] \\\n",
    "                .groupby(group_col)[queen_proba_col].max()\n",
    "            # Assign back to all queens. Since the order is retained, we shouldn't have to\n",
    "            # worry about assigning to the wrong group queen\n",
    "            grouped.loc[grouped[queen_col] == True, dedup_score_col] = queens_score.values\n",
    "            \n",
    "        else:\n",
    "            #do nothing if edges is empty, process will continue untill the end of range.  probably a better way to do this.\n",
    "            edges\n",
    "            \n",
    "    #now some code to flesh out all the dedup scores for groups 2, 3, etc.\n",
    "    for r in rounds: \n",
    "        round_number = r[-1]\n",
    "        queen = 'queen ' + str(round_number)\n",
    "        dedup_score = 'dedup_score ' + str(round_number)\n",
    "        if round_number == '2':\n",
    "            previous_round = 'group_id'\n",
    "            previous_queen = 'queen'\n",
    "            previous_dedup_score = 'dedup_score'\n",
    "        else:\n",
    "            previous_round = 'group_id ' + str(int(round_number)-1)\n",
    "            previous_queen = 'queen ' + str(int(round_number)-1)\n",
    "            previous_dedup_score = 'dedup_score ' + str(int(round_number)-1)\n",
    "        \n",
    "        #filling in groups, dedup scores for re-grouped groups\n",
    "        for g in grouped[(grouped[r].notnull()) & (grouped[r] != 0)][r].drop_duplicates():\n",
    "            for p in grouped[grouped[r] == g][previous_round].drop_duplicates():\n",
    "                queen_proba = grouped[(grouped[previous_round]==p) & (grouped[previous_queen]==True)][dedup_score][0]\n",
    "                for index, row in grouped[(grouped[previous_round]==p) & (grouped[previous_queen]==False)].iterrows():\n",
    "                    grouped.at[index,r] = g #fill in the group_id\n",
    "                    grouped.at[index,queen] = False\n",
    "                    grouped.at[index,dedup_score] = row[previous_dedup_score] * queen_proba #calculate match probability \n",
    "                    \n",
    "        #filling in groups, dedup scores for the blanks in the new regrouped columns to provide cohesive perspective in each\n",
    "        max_g = grouped[grouped[r].notnull()][r].max()\n",
    "        for p in grouped[((grouped[previous_round].notnull() & (grouped[previous_round] != 0))) \n",
    "                        & ((grouped[r].isnull()) | (grouped[r] == 0))][previous_round].drop_duplicates():\n",
    "            for index, row in grouped[grouped[previous_round] == p].iterrows():\n",
    "                grouped.at[index,r] = max_g + 1\n",
    "                grouped.at[index,queen] = row[previous_queen]\n",
    "                grouped.at[index,dedup_score] = row[previous_dedup_score]\n",
    "            max_g = max_g + 1\n",
    "            \n",
    "        \n",
    "    grouped=grouped.reset_index()\n",
    "    grouped.to_csv(ns(\"grouped.csv\"))\n",
    "    print(\"Grouped all matches --\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "else:\n",
    "    #return to original column names where appropriate\n",
    "    for col in grouped.columns:\n",
    "        if (col in cols):\n",
    "            grouped.rename(columns={col:cols[col]},inplace=True)\n",
    "\n",
    "    if matching:\n",
    "        for col in grouped.columns:\n",
    "            if (col in cols_alt) & (col != 'id'):\n",
    "                grouped.rename(columns={col:cols_alt[col]},inplace=True)\n",
    "    \n",
    "    grouped.to_csv(ns(\"grouped.csv\"), index=False)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not matching: #still need to figure out how I am handling matching\n",
    "    print(\"MERGE DATA MATCH ELEMENTS WITH ORIGINAL FILES\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #saving the cols dictionary for later use\n",
    "    cols_df = pd.DataFrame()\n",
    "    cols_df['original_column_name'] = cols.values()\n",
    "    cols_df['standard_column_name'] = cols.keys()\n",
    "    cols_df.dropna(inplace=True)\n",
    "    cols_df.to_csv(ns('cols_df.csv'),index=False)\n",
    "    \n",
    "    #isolating any new columns to append to original data\n",
    "    original_df = pd.read_table(data_details['filepath_or_buffer'],sep=data_details['sep'],converters=data_details['converters'])\n",
    "    start_cols = pd.read_table(data_details['filepath_or_buffer'],sep=data_details['sep'],converters=data_details['converters'],nrows=0).shape[1]\n",
    "    end_cols = grouped.shape[1]\n",
    "    new_cols = list(grouped.iloc[:,end_cols-(end_cols-start_cols):].columns)\n",
    "    new_cols.append(cols['id'])\n",
    "     \n",
    "    original_df[cols['id']] = original_df[cols['id']].astype(str)\n",
    "    #added below code to handle scenario where there was 1 NULL value for some reason, messing up typing of column for join\n",
    "    if grouped[cols['id']].isnull().sum() >0:\n",
    "        grouped = pd.read_csv(ns('grouped.csv'))\n",
    "        grouped.dropna(subset=[cols['id']],inplace=True)\n",
    "        grouped[cols['id']] = grouped[cols['id']].astype(float)\n",
    "        grouped[cols['id']] = grouped[cols['id']].astype(int)\n",
    "    \n",
    "    grouped[cols['id']] = grouped[cols['id']].astype(str)\n",
    "    original_w_grouped = pd.merge(original_df,grouped[new_cols],left_on=cols['id'],right_on=cols['id'])\n",
    "\n",
    "    print(\"Grouped features appended to original data --- %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    original_w_grouped[original_w_grouped.group_id != 0].sort_values(by=['group_id','queen_proba'],ascending=[True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining grouped file with match features\n",
    "if not matching:\n",
    "    print(\"MERGE GROUPED WITH ORIGINAL MATCH FEATURES\")\n",
    "    start_time = time.time()\n",
    "    matches = pd.read_csv(ns('matches.csv'),compression=compression)\n",
    "    merged = pd.merge(original_w_grouped, matches, how='left',left_on='pair',right_on='pair')\n",
    "    merged=merged.sort_values(by=['group_id', 'queen_proba'], ascending=[True, False])\n",
    "    merged.to_csv(ns('grouped_w_features.csv'),index=False)\n",
    "    merged.to_excel(ns('grouped_w_features.xlsx'),index=False)\n",
    "    try:\n",
    "        string_columns_df.to_csv(ns('string_columns.csv'),index=False)\n",
    "    except:\n",
    "        print(\"no string columns df available\")\n",
    "    print(\"Match features appended to original data --- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged[merged.group_id != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
