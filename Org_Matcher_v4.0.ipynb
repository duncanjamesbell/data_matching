{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from jellyfish import jaro_winkler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from py_common_subseq import find_common_subsequences\n",
    "import numbers\n",
    "import time\n",
    "from collections import Counter \n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NOTES\n",
    "#removing duplicate token-ID keys inplace caused some strange ID issue, so I am not removing duplicates anymore.  could be improved.\n",
    "#unique_token_freq_max should really be derived dynamically by the number of tokens, some math\n",
    "#zip matcher does not handle foreign postal codes eg ZVR OTS\n",
    "#need to add code to remove duplicate ID pairs: A-B and B-A are the same - actually, Wait, do I?  I don't think so.  that's only for de-dup\n",
    "#keep_default_na = True I think is necessary as we need to purge NULL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import shutil\n",
    "\n",
    "with io.open('990 match data.csv', encoding='utf-8', errors='ignore') as source:\n",
    "    with io.open('990 match data_utf.csv', mode='w', encoding='utf-8') as target:\n",
    "        shutil.copyfileobj(source,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_time = time.time()\n",
    "\n",
    "#define column names\n",
    "l_id = 'organization_id'\n",
    "l_name = 'org_name'\n",
    "l_address1 = 'address1'\n",
    "l_city = 'city'\n",
    "l_state = 'state'\n",
    "l_zip = 'postal_code'\n",
    "l_phone = 'phone'\n",
    "l_web = 'website'\n",
    "l_acronym = 'acronym'\n",
    "l_alt_name = 'alt_name'\n",
    "\n",
    "r_id = 'ein'\n",
    "r_name = 'organizationname'\n",
    "r_address1 = 'FA_AddressLine1Txt'\n",
    "r_city = 'FA_CityNm'\n",
    "r_state = 'FA_StateAbbreviationCd'\n",
    "r_zip = 'FA_ZIPCd'\n",
    "r_phone = 'F_PhoneNum'\n",
    "r_web = 'WebsiteAddressTxt'\n",
    "#r_acronym = 'CBI_Acronym'\n",
    "\n",
    "#set parameters\n",
    "token_match_min = 2 # minimum number of matched tokens to be considered a match\n",
    "token_limiter = .995 # percent of non-single tokens to tokenize, where rare tokens are at the bottom and common at the top\n",
    "unique_token_freq_max = 5 #threshold <= to a token is considered \"unique\" and links to these tokens are counted double\n",
    "name_weight = .75 #note that this is really .75 * 4 because there are 4 org name simularity metrics\n",
    "name_uniqueness_weight = 1.5\n",
    "state_weight = 1\n",
    "zip_weight = 1\n",
    "phone_weight = 2\n",
    "domain_weight = 2\n",
    "name_score_min = 3\n",
    "composite_score_min = 4 #minimum composite match score to be considered a match\n",
    "\n",
    "start_time = time.time()\n",
    "print \"LOADING INITIAL DATAFRAMES...\"\n",
    "\n",
    "left_df = pd.read_csv('all cupola orgs w phone_db_utf.csv',keep_default_na=True)\n",
    "right_df = pd.read_csv('990 match data_utf.csv',keep_default_na=True,error_bad_lines=False)\n",
    "\n",
    "left_df.rename(columns={l_id:'id',l_name:'l_org_name',l_address1:'l_address1',l_city:'l_city',l_state:'l_state',l_zip:'l_postal_code',l_web:'l_web',l_phone:'l_phone',l_acronym:'l_acronym',l_alt_name:'l_alt_name'}, inplace=True)\n",
    "right_df.rename(columns={r_id:'id',r_name:'r_org_name',r_address1:'r_address1',r_city:'r_city',r_state:'r_state',r_zip:'r_postal_code',r_web:'r_web',r_phone:'r_phone'}, inplace=True)\n",
    "\n",
    "left_df = left_df.replace(np.nan, '', regex=True)\n",
    "right_df = right_df.replace(np.nan, '', regex=True)\n",
    "\n",
    "right_df.r_org_name = right_df.r_org_name.astype('str')\n",
    "right_df.r_address1 = right_df.r_address1.astype('str')\n",
    "right_df.r_city = right_df.r_city.astype('str')\n",
    "right_df.r_state = right_df.r_state.astype('str')\n",
    "right_df.r_postal_code = right_df.r_postal_code.astype('str')\n",
    "right_df.r_phone = right_df.r_phone.astype('str')\n",
    "right_df.r_web = right_df.r_web.astype('str')\n",
    "\n",
    "print(\"Dataframes loaded --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"PRE-PROCESSING...\"\n",
    "#normalize state codes\n",
    "state_lkup = pd.read_csv('state_lkup.csv',keep_default_na=False)\n",
    "\n",
    "from collections import defaultdict\n",
    "state_dict = defaultdict(list)\n",
    "for state, acronym in zip(state_lkup.state.values,state_lkup.acronym.values):\n",
    "    state_dict[state].append(acronym)\n",
    "\n",
    "left_df.l_state = left_df.l_state.replace(np.nan, '', regex=True).str.lower()\n",
    "left_df.l_state = left_df.l_state.replace(state_dict)\n",
    "right_df.r_state = right_df.r_state.replace(np.nan, '', regex=True).str.lower()\n",
    "right_df.r_state = right_df.r_state.replace(state_dict)\n",
    "\n",
    "#clean up non numeric characters in phones\n",
    "l_clean_phones = []\n",
    "left_df.l_phone = left_df.l_phone.astype('str')\n",
    "left_df.l_phone = left_df.l_phone.replace(np.nan, '', regex=True)\n",
    "for phone in left_df.l_phone:\n",
    "    l_clean_phones.append(re.sub('[^0-9]','', phone))\n",
    "    \n",
    "left_df['l_clean_phone'] = l_clean_phones\n",
    "\n",
    "r_clean_phones = []\n",
    "right_df.r_phone = right_df.r_phone.astype('str')\n",
    "right_df.r_phone = right_df.r_phone.replace(np.nan, '', regex=True)\n",
    "for phone in right_df.r_phone:\n",
    "    r_clean_phones.append(re.sub('[^0-9]','', phone))\n",
    "\n",
    "right_df['r_clean_phone'] = r_clean_phones\n",
    "\n",
    "#isolate domains from web URLs\n",
    "l_domains = []\n",
    "left_df.l_web = left_df.l_web.replace(np.nan, '', regex=True).str.lower()\n",
    "for web in left_df.l_web:\n",
    "    l_domains.append(web.split('//')[-1].split('/')[0].strip('www.'))\n",
    "left_df['l_domain'] = l_domains\n",
    "    \n",
    "r_domains = []\n",
    "right_df.r_web = right_df.r_web.replace(np.nan, '', regex=True).str.lower()\n",
    "for web in right_df.r_web:\n",
    "    r_domains.append(web.split('//')[-1].split('/')[0].strip('www.'))\n",
    "right_df['r_domain'] = r_domains\n",
    "\n",
    "print(\"states, phones, domains normalized --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"TOKENIZING, IDENTIFYING CANDIDATE MATCH PAIRS...\"\n",
    "\n",
    "left_unique_token_columns = [ #tokens here circumvent the token_limiter which is cutting off the most commonly occuring tokens\n",
    "    'l_state', \n",
    "    'l_postal_code',\n",
    "    'l_clean_phone'\n",
    "\n",
    "]\n",
    "\n",
    "left_delta_token_columns = [\n",
    "    'l_acronym',\n",
    "    'l_org_name',\n",
    "    'l_alt_name',\n",
    "    #'l_address1',\n",
    "    #'l_address2',\n",
    "    'l_city',\n",
    "    'l_domain'\n",
    "]\n",
    "\n",
    "right_unique_token_columns = [\n",
    "    'r_state', \n",
    "    'r_postal_code',\n",
    "    'r_clean_phone'\n",
    "]\n",
    "\n",
    "right_delta_token_columns = [\n",
    "    #'r_acronym',\n",
    "    'r_org_name',\n",
    "    #'r_alt_name',\n",
    "    #'r_address1',\n",
    "    #'r_address2',\n",
    "    'r_city',\n",
    "    'r_domain'\n",
    "]\n",
    "\n",
    "# lowercase the name and split on spaces, remove non-alphanumeric chars\n",
    "def tokenize_name(name):\n",
    "    if isinstance(name, basestring) is True:\n",
    "        clean_name = ''.join(c if c.isalnum() else ' ' for c in name)\n",
    "        return clean_name.lower().split()\n",
    "    else:\n",
    "        return name\n",
    "    \n",
    "unique_tokens = []    \n",
    "for col in left_unique_token_columns:\n",
    "    for word in left_df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            unique_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "for col in right_unique_token_columns:\n",
    "    for word in right_df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            unique_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "unique_flat_list = [item for sublist in unique_tokens for item in sublist]\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "u_cnt = Counter()\n",
    "for token in unique_flat_list:\n",
    "    u_cnt[token] += 1\n",
    "\n",
    "u_cnt_dict = dict(u_cnt) #convert to dictionary\n",
    "\n",
    "unique_tokens_df = pd.DataFrame(u_cnt_dict.items(), columns=['token', 'count'])\n",
    "unique_tokens_df = unique_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "#consider waiting to do the count flag thing later, instead use some type of \"token type\" code\n",
    "unique_token_flag = []\n",
    "for index, value in enumerate(unique_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        unique_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    else:\n",
    "        unique_token_flag.append(1)\n",
    "\n",
    "unique_tokens_df['flag'] = unique_token_flag        \n",
    "\n",
    "all_other_words = []\n",
    "for col in left_delta_token_columns:\n",
    "    for word in left_df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            all_other_words.append(tokenize_name(str(word)))\n",
    "            \n",
    "for col in right_delta_token_columns:\n",
    "    for word in right_df[col]:\n",
    "        if isinstance(word, float) is False:\n",
    "            all_other_words.append(tokenize_name(str(word)))\n",
    "            \n",
    "flat_list = [item for sublist in all_other_words for item in sublist] #flatten list so it can be counted\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "cnt = Counter()\n",
    "for token in flat_list:\n",
    "    cnt[token] += 1\n",
    "\n",
    "cnt_dict = dict(cnt) #convert to dictionary\n",
    "\n",
    "main_tokens_df = pd.DataFrame(cnt_dict.items(), columns=['token', 'count'])\n",
    "main_tokens_df = main_tokens_df.sort_values(by='count')  #sorting by count so that we can take the first x% of tokens by rare frequency\n",
    "\n",
    "#wait to do count until joined with unique tokens?\n",
    "main_token_flag = []\n",
    "for index, value in enumerate(main_tokens_df['count']):\n",
    "    if value == 1:\n",
    "        main_token_flag.append(0)  #for any tokens occuring only once, we exclude\n",
    "    elif index < int(main_tokens_df.shape[0] * token_limiter): #important line, we are cutting the top x% of frequently occuring tokens\n",
    "        main_token_flag.append(1)\n",
    "    else:\n",
    "        main_token_flag.append(0)  #for the most common tokens, we exclude\n",
    "\n",
    "main_tokens_df['flag'] = main_token_flag\n",
    "\n",
    "all_tokens = pd.concat([unique_tokens_df, main_tokens_df])\n",
    "\n",
    "all_tokens.drop('count',axis=1,inplace=True)\n",
    "all_tokens['flag'] = all_tokens.flag.astype(int) #converting flags to int\n",
    "tokens_dct = all_tokens.to_dict('split') #converting tokens_df to dictionary\n",
    "tokens_dct=dict(tokens_dct['data']) #honestly can't remember why this works, something to do with conversion to dictionary\n",
    "\n",
    "#preparing token_ids which will be used for joining left and right dfs\n",
    "all_tokens.sort_values(by='flag',ascending=False,inplace=True)\n",
    "all_tokens.sort_values(by='token',inplace=True)\n",
    "all_tokens.drop_duplicates(subset='token',keep='first',inplace=True)\n",
    "token_ids = all_tokens.index.get_level_values(0)\n",
    "all_tokens['token_id'] = token_ids\n",
    "\n",
    "all_tokens.drop('flag',axis=1,inplace=True)\n",
    "all_tokens['token_id'] = all_tokens.token_id.astype(int)\n",
    "token_id_dct = all_tokens.to_dict('split')\n",
    "tokens_id_dct=dict(token_id_dct['data'])\n",
    "\n",
    "vocabulary = np.array([w for w, c in tokens_dct.items() if c ==1]) #this works even without the ==1 and I don't know why\n",
    "cv = CountVectorizer( vocabulary=vocabulary)\n",
    "\n",
    "#now we are ready to tokenize left and right dataframes\n",
    "all_left_cols = left_unique_token_columns + left_delta_token_columns\n",
    "\n",
    "left_frame_list = []\n",
    "for colname in all_left_cols:\n",
    "    tokenmapping = cv.fit_transform(left_df[colname])\n",
    "    df_row, token_id = tokenmapping.nonzero()\n",
    "\n",
    "    left_frame_list.append(pd.DataFrame(np.vstack([vocabulary[token_id], left_df['id'].values[df_row]]).T, columns = ['token', 'id_l']))\n",
    "\n",
    "left_keyed = pd.concat(left_frame_list)\n",
    "left_keyed.drop_duplicates()#inplace=True)\n",
    "#removing duplicates inplace was giving me a very strange issue where a small percentage of token_ids would be excluded from the left_keyed index\n",
    "\n",
    "#append token_id to token as this will be more efficient to join with\n",
    "left_token_ids = []\n",
    "for token in left_keyed.token:\n",
    "    left_token_ids.append(tokens_id_dct[token])\n",
    "\n",
    "left_keyed['token_id'] = left_token_ids\n",
    "left_keyed.sort_values(by='token_id',inplace=True)\n",
    "left_keyed.set_index('token_id',inplace=True)\n",
    "left_keyed.drop('token',axis=1,inplace=True)\n",
    "\n",
    "left_keyed.sort_values(by='id_l',inplace=True)\n",
    "\n",
    "all_right_cols = right_unique_token_columns + right_delta_token_columns\n",
    "\n",
    "right_frame_list = []\n",
    "for colname in all_right_cols:\n",
    "    tokenmapping = cv.fit_transform(right_df[colname])\n",
    "    df_row, token_id = tokenmapping.nonzero()\n",
    "\n",
    "    right_frame_list.append(pd.DataFrame(np.vstack([vocabulary[token_id], right_df['id'].values[df_row]]).T, columns = ['token', 'id_r']))\n",
    "\n",
    "right_keyed = pd.concat(right_frame_list)\n",
    "right_keyed.drop_duplicates()#inplace=True)\n",
    "\n",
    "#append token_id to token as this will be more efficient to join with\n",
    "right_token_ids = []\n",
    "for token in right_keyed.token:\n",
    "    right_token_ids.append(tokens_id_dct[token])\n",
    "\n",
    "right_keyed['token_id'] = right_token_ids\n",
    "right_keyed.sort_values(by='token_id',inplace=True)\n",
    "right_keyed.set_index('token_id',inplace=True)\n",
    "right_keyed.drop('token',axis=1,inplace=True)\n",
    "\n",
    "aggregations = {\n",
    "    'id_l': 'count'\n",
    "}\n",
    "\n",
    "left_keyed.to_csv('left_keyed.csv')\n",
    "bonus_point_tokens = []\n",
    "for token in main_tokens_df[(main_tokens_df['count'] > 1) & (main_tokens_df['count']<=unique_token_freq_max)].token:\n",
    "    bonus_point_tokens.append(tokens_id_dct[token])\n",
    "\n",
    "match_dfs = []\n",
    "for df in pd.read_csv('left_keyed.csv',keep_default_na=False,chunksize=50000,index_col='token_id'):\n",
    "    \n",
    "    joined = df.join(right_keyed, how='inner',lsuffix='_l',rsuffix='_r')\n",
    "    joined['id_l']=joined.id_l.astype('str')\n",
    "    #double-counting unique token matches\n",
    "\n",
    "    intersection_bonus_tokens = set(bonus_point_tokens).intersection(set(list(joined.index)))    \n",
    "\n",
    "    bonus_token_joins = []\n",
    "    for token_id in intersection_bonus_tokens:\n",
    "        bonus_token_joins.append(joined.loc[token_id])\n",
    "\n",
    "    bonus_joins = pd.concat(bonus_token_joins)\n",
    "    bonus_joins_cols = bonus_joins[['id_l','id_r']].copy()\n",
    "    bonus_joins_cols.dropna(inplace=True)\n",
    "    joined_bonus = pd.concat([joined,bonus_joins_cols])\n",
    "    \n",
    "    keys_grouped = joined_bonus.groupby(by=['id_l', 'id_r']).agg(aggregations)\n",
    "    keys_grouped.rename(columns={'id_l':'id_l count'}, inplace=True)\n",
    "    matched_records = keys_grouped[keys_grouped['id_l count'] >= token_match_min]\n",
    "    matched_records.reset_index(inplace=True)\n",
    "    match_dfs.append(matched_records)\n",
    "    \n",
    "all_match_candidates = pd.concat(match_dfs)\n",
    "all_match_candidates.reset_index(inplace=True)\n",
    "all_match_candidates.drop(labels='index',axis=1)\n",
    "all_match_candidates.id_l.astype('str')\n",
    "\n",
    "print(\"match candidates identified --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "left_df.rename(columns={'id':'id_l'},inplace=True)\n",
    "right_df.rename(columns={'id':'id_r'},inplace=True)\n",
    "\n",
    "left_match_data = left_df[['id_l','l_org_name','l_city','l_state','l_postal_code','l_domain','l_clean_phone']].copy()\n",
    "right_match_data = right_df[['id_r','r_org_name','r_city','r_state','r_postal_code','r_domain','r_clean_phone']].copy()\n",
    "\n",
    "#making sure keys are str, results in blank df otherwise\n",
    "left_match_data.id_l = left_match_data.id_l.astype('str')\n",
    "right_match_data.id_r = right_match_data.id_r.astype('str')\n",
    "matched_records.id_l = matched_records.id_l.astype('str')\n",
    "matched_records.id_r = matched_records.id_r.astype('str')\n",
    "\n",
    "#merging matched_records df with original record data for ease of review\n",
    "l_conc = pd.merge(all_match_candidates, left_match_data, on='id_l')\n",
    "full_conc = pd.merge(l_conc, right_match_data, on='id_r')\n",
    "\n",
    "print(\"original data concatenated with matches --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING ORG NAME SIMULARITY...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on edit distance of org names\n",
    "def jaro_simularity(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return jaro_winkler(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '')\n",
    "def fuzz_partial(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.partial_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_sort(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_sort_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "def fuzz_set(left_record, right_record):\n",
    "    if len(left_record) > 0 and len(right_record) > 0:\n",
    "        if isinstance(left_record, numbers.Integral) is False and isinstance(right_record, numbers.Integral) is False:\n",
    "            return fuzz.token_set_ratio(unicode(left_record, 'utf-8') or '', unicode(right_record, 'utf-8') or '') / float(100)\n",
    "\n",
    "full_conc['l_org_name'] = full_conc['l_org_name'].replace('', 'none', regex=True).astype('str')\n",
    "full_conc['r_org_name'] = full_conc['r_org_name'].replace('', 'none', regex=True).astype('str')\n",
    "\n",
    "jaro_time = time.time()\n",
    "full_conc['jaro_score'] = full_conc.apply(lambda x: jaro_simularity(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"jaro scores done --- %s seconds ---\" % (time.time() - jaro_time))\n",
    "#jaro_reduced = full_conc[full_conc.jaro_score > .25] #I don't feel comfortable reducing candidates on a single metric\n",
    "partial_time = time.time()\n",
    "full_conc['fuzz_partial_score'] = full_conc.apply(lambda x: fuzz_partial(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz partial scores done --- %s seconds ---\" % (time.time() - partial_time))\n",
    "sort_time = time.time()\n",
    "full_conc['fuzz_sort_score'] = full_conc.apply(lambda x: fuzz_sort(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz sort scores done --- %s seconds ---\" % (time.time() - sort_time))\n",
    "set_time = time.time()\n",
    "full_conc['fuzz_set_score'] = full_conc.apply(lambda x: fuzz_set(x.l_org_name, x.r_org_name), axis=1)\n",
    "print(\"fuzz set scores done --- %s seconds ---\" % (time.time() - set_time))\n",
    "print \"\"\n",
    "\n",
    "print(\"name simularity scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"SCORING NAME SEQUENCE UNIQUENESS...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "org_tokens = []    \n",
    "for word in left_df['l_org_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_tokens.append(tokenize_name(str(word)))\n",
    "\n",
    "for word in right_df['r_org_name']:\n",
    "    if isinstance(word, float) is False:\n",
    "        org_tokens.append(tokenize_name(str(word)))\n",
    "            \n",
    "org_flat_list = [item for sublist in org_tokens for item in sublist]\n",
    "\n",
    "#instantiate counter and use to count word frequencies in flat list\n",
    "org_cnt = Counter()\n",
    "for token in org_flat_list:\n",
    "    org_cnt[token] += 1\n",
    "\n",
    "org_cnt_dict = dict(org_cnt) #convert to dictionary\n",
    "\n",
    "def sequence_uniqueness(seq):\n",
    "    return sum(1/org_cnt_dict[str.lower(t)]**0.5 for t in seq)\n",
    "\n",
    "def name_similarity(a, b):\n",
    "    if isinstance(a,basestring) is True and isinstance(b,basestring) is True:\n",
    "        if len(a) > 0 and len(b) > 0:\n",
    "            a_tokens = set(tokenize_name(a))\n",
    "            b_tokens = set(tokenize_name(b))\n",
    "            a_uniq = sequence_uniqueness(a_tokens)\n",
    "            b_uniq = sequence_uniqueness(b_tokens)\n",
    "\n",
    "            return sequence_uniqueness(a_tokens.intersection(b_tokens))/(a_uniq * b_uniq) ** 0.5\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "full_conc['uniq'] = full_conc.apply(lambda x: name_similarity(x.l_org_name, x.r_org_name), axis=1)\n",
    "\n",
    "print(\"name uniqueness scored --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR STATE CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def sanitize_state(state):\n",
    "    if isinstance(state,basestring) is True:\n",
    "        return ''.join(c for c in (state or '') if c in 'abcdefghijklmnopqrstuvwxyz')\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def state_match(state_a, state_b):\n",
    "    sanitized_state_a = str(sanitize_state(state_a))\n",
    "    sanitized_state_b = str(sanitize_state(state_b))\n",
    "\n",
    "    # if the value is too short, means it's fubar\n",
    "    if len(sanitized_state_a) < 2 or len(sanitized_state_b) < 2:\n",
    "        return 0\n",
    "    if state_a == state_b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "full_conc['state_match'] = full_conc.apply(lambda x: state_match(x.l_state, x.r_state), axis=1)\n",
    "\n",
    "print(\"state codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR POSTAL CODE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching postal code\n",
    "\n",
    "def sanitize_postal(postal):\n",
    "    if isinstance(postal, basestring) is True:\n",
    "        return ''.join(c for c in (postal or '') if c in '1234567890')\n",
    "    if isinstance(postal, float) is False:\n",
    "        return postal\n",
    "\n",
    "def postal_simularity(postal_a, postal_b):\n",
    "    sanitized_postal_a = str(sanitize_postal(postal_a))\n",
    "    sanitized_postal_b = str(sanitize_postal(postal_b))\n",
    "\n",
    "    # if the number is too short, means it's fubar\n",
    "    if len(sanitized_postal_a) < 5 or len(sanitized_postal_b) < 5:\n",
    "        return 0\n",
    "    if float(max(len(sub) for sub in find_common_subsequences(sanitized_postal_a, sanitized_postal_b))) / 5 >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "full_conc['zip_match'] = full_conc.apply(lambda x: postal_simularity(x.l_postal_code, x.r_postal_code), axis=1)\n",
    "    \n",
    "print(\"postal codes checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR WEB DOMAIN MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def domain_match(domain_a, domain_b):\n",
    "    if isinstance(domain_a, basestring) is True and isinstance(domain_b, basestring) is True:\n",
    "        if len(domain_a) > 0 and len(domain_b) > 0:\n",
    "            if domain_a == domain_b:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "full_conc['domain_match'] = full_conc.apply(lambda x: domain_match(x.l_domain, x.r_domain), axis=1)\n",
    "\n",
    "print(\"web domains checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"CHECKING FOR PHONE MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#scoring match candidates based on matching phone\n",
    "def phone_simularity(phone_a, phone_b):\n",
    "\n",
    "    if len(phone_a) < 10 or len(phone_b) < 10:\n",
    "        return 0\n",
    "    elif float(max(len(sub) for sub in find_common_subsequences(phone_a, phone_b))) / 10 >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "full_conc['phone_match'] = full_conc.apply(lambda x: phone_simularity(x.l_clean_phone, x.r_clean_phone), axis=1)\n",
    "    \n",
    "print(\"phones checked --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "start_time = time.time()\n",
    "print \"COMPOSITE SCORING, PREDICTING MATCHES...\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#full_conc['overall_name_score'] = full_conc.jaro_score * name_weight \\\n",
    "#+ full_conc.fuzz_partial_score * name_weight \\\n",
    "#+ full_conc.fuzz_sort_score * name_weight \\\n",
    "#+ full_conc.fuzz_set_score * name_weight \\\n",
    "#+ full_conc.uniq * name_uniqueness_weight\n",
    "\n",
    "#calculate composite match score based on component scores and weights\n",
    "#full_conc['composite_match_score'] = full_conc.overall_name_score \\\n",
    "#+ full_conc.zip_match * zip_weight \\\n",
    "#+ full_conc.state_match * state_weight \\\n",
    "#+ full_conc.domain_match * domain_weight \\\n",
    "#+ full_conc.phone_match * phone_weight\n",
    "\n",
    "#use labeled matches to train logistic regression to predict matches\n",
    "training_data = pd.read_table('training_data_utf.txt')\n",
    "\n",
    "feature_cols = ['jaro_score',\n",
    "                'fuzz_partial_score',\n",
    "                'fuzz_sort_score',\n",
    "                'fuzz_set_score',\n",
    "                'uniq',\n",
    "                'state_match',\n",
    "                'zip_match']#,\n",
    "                #'domain_match',\n",
    "                #'phone_match']\n",
    "\n",
    "# define X and y\n",
    "X = training_data[feature_cols]\n",
    "y = training_data['is_match']\n",
    "\n",
    "# logistic regression\n",
    "log = LogisticRegression()\n",
    "log.fit(X, y)\n",
    "y_pred_class = log.predict(full_conc[feature_cols])\n",
    "y_pred_proba = log.predict_proba(full_conc[feature_cols])[:,1]\n",
    "\n",
    "full_conc['match_pred'] = y_pred_class\n",
    "full_conc['pred_proba'] = y_pred_proba\n",
    "\n",
    "#we take any matches meeting either name match threshold or composite match threshold as matches for review\n",
    "#org_matches = full_conc[(full_conc.overall_name_score >= name_score_min) | (full_conc.composite_match_score >= composite_score_min)]\n",
    "org_matches = full_conc[full_conc.match_pred == 1]\n",
    "failed_matches_for_review = full_conc[(full_conc.match_pred == 0) & (full_conc.pred_proba > .4)]\n",
    "\n",
    "print(\"final matches isolated --- %s seconds ---\" % (time.time() - start_time))\n",
    "print \"\"\n",
    "\n",
    "print(\"TOTAL COMPUTE TIME --- %s seconds ---\" % (time.time() - overall_time))\n",
    "\n",
    "org_matches.sort_values(by='pred_proba', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use kfolds to test the accuracy of model using our training data\n",
    "from sklearn import cross_validation\n",
    "\n",
    "training_data = pd.read_table('training_data3.txt')\n",
    "X = training_data[feature_cols]\n",
    "y = training_data['is_match']\n",
    "\n",
    "kf = cross_validation.KFold(len(X), n_folds=10, shuffle=True)\n",
    "\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "f1_scores = []\n",
    "n= 0\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf:\n",
    "    lm = LogisticRegression().fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    recall_scores.append(metrics.recall_score(y.iloc[test_index], lm.predict(X.iloc[test_index])))\n",
    "    precision_scores.append(metrics.precision_score(y.iloc[test_index], lm.predict(X.iloc[test_index])))\n",
    "    f1_scores.append(metrics.f1_score(y.iloc[test_index], lm.predict(X.iloc[test_index])))\n",
    "    n+=1\n",
    "    print('Model', n)\n",
    "    print('Recall:', recall_scores[n-1])\n",
    "    print('Precision:', precision_scores[n-1])\n",
    "    print('F1-Score:', f1_scores[n-1])\n",
    "\n",
    "print(\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print('Mean recall scores for all folds:', np.mean(recall_scores))\n",
    "print('Mean precision scores for all folds:', np.mean(precision_scores))\n",
    "print('Mean F1 scores for all folds:', np.mean(f1_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
